<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>To do list</title>
    <url>/1975/zhiding/</url>
    <content><![CDATA[<ul>
<li>完成大一所有作业</li>
<li>填补博客坑</li>
<li>完成数据结构思考题</li>
<li>完成概率导论</li>
<li>完成吴恩达机器学习</li>
<li>完成mma学习</li>
<li>完善github上的sysu项目</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>费用为k的生成树</title>
    <url>/2021/20210718/</url>
    <content><![CDATA[<p>问题：输入无向图G和一个整数k。G中有n个顶点。每条边的费用非1 即2。请在图G中寻找一棵费用为k 的生成树。<br>问题为数据结构与算法HW5的bouns题第一题<br>(待写)</p>
<span id="more"></span>

<p>参考资料：<a href="https://www.comp.nus.edu.sg/~gilbert/CS4234/2015/psets/pset1-solutions.pdf">新加坡国立大学CS4234课程Problem Set 1</a></p>
]]></content>
  </entry>
  <entry>
    <title>机器学习之推荐系统</title>
    <url>/2021/20210716/</url>
    <content><![CDATA[<p>推荐系统</p>
<span id="more"></span>

<p>待写。</p>
]]></content>
  </entry>
  <entry>
    <title>级数的若干反例</title>
    <url>/2021/20210710/</url>
    <content><![CDATA[<p>（一）$\sum _{n=1}^{\infty}a_n$收敛，但是$\sum _{n=1}^{\infty}{a_n}^3$发散</p>
<p>反例：<br>$$<br>\sum _{n=1}^{\infty}a_n=1-1+\frac{1}{\sqrt[3]{2}}-\frac{1}{2\sqrt[3]{2}}-\frac{1}{2\sqrt[3]{2}}\<br>+…+\frac{1}{\sqrt[3]{k}}-\frac{1}{k\sqrt[3]{k}}(减去k个)-…<br>$$</p>
<span id="more"></span>

<p>（二）$\sum _{n=1}^{\infty}a_n$收敛，但是$a_n \neq o(\frac{1}{n})$</p>
<p>反例一：<br>$$<br>\sum _{n=1}^{\infty}a_n=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4}+\<br>+\frac{1}{5^2}+\frac{1}{6^2}+\frac{1}{7^2}+\frac{1}{8^2}+\frac{1}{9}…<br>\<br>（即对k^2项改写，将\frac{1}{n^2}换成\frac{1}{n}）<br>$$<br>反例二：<br>$$<br>a_n=\frac{1}{nlnn}<br>$$</p>
<p>（三）$\sum _{n=1}^{\infty}a_n+b_n$收敛，但是$\sum _{n=1}^{\infty}a_n$、$\sum _{n=1}^{\infty}a_n$均不收敛。</p>
<p>反例：<br>$$<br>a_n=(-1)^n\ \ ,\ \  b_n=(-1)^{n+1}<br>$$<br>（四）$\sum _{n=1}^{\infty}a _{2n-1}+a _{2n}$收敛，但是$\sum _{n=1}^{\infty}u_n$不收敛。</p>
<p>反例：<br>$$<br>a_n=(-1)^n\ \ ,\ \  b_n=(-1)^{n+1}<br>$$<br>（五）$\lim _{n\ \rightarrow \ \infty \ }\ na_n=0$,但是$\sum _{n=1}^{\infty}a _{2n-1}+a _{2n}$收敛。</p>
<p>反例：<br>$$<br>a_n=\frac{1}{nlnn}<br>$$<br>（六）$\sum _{n=1}^{\infty}a_n$为发散的交错级数，但是$\lim _{n\ \rightarrow \ \infty \ }\ a_n=0$</p>
<p>反例：<br>$$<br>\sum _{n=1}^{\infty}a_n=1-\frac{1}{2}+\frac{1}{3^2}-\frac{1}{5^2}\<br>+…+\frac{1}{(2n-1)^2}-\frac{1}{2n}+…<br>$$<br>（七）$\sum _{n=1}^{\infty}a_n$为发散的交错级数，但是$|a_n|&gt;|a _n+1|$</p>
<p>反例：<br>$$<br>\sum _{n=1}^{\infty}a_n=(-1)^n(1+\frac{1}{n})<br>$$</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习之降维算法</title>
    <url>/2021/20210614/</url>
    <content><![CDATA[<p>降维可以压缩数据，因而使用较少的计算机内存或磁盘空间，加快我们的学习算法。</p>
<p>在数据可视化中，降维也是常用的手段。</p>
<span id="more"></span>

<h5 id="PCA算法（Principal-Component-Analysis）"><a href="#PCA算法（Principal-Component-Analysis）" class="headerlink" title="PCA算法（Principal Component Analysis）"></a>PCA算法（Principal Component Analysis）</h5><p>在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>
<p><img src="http://www.ai-start.com/ml2014/images/a93213474b35ce393320428996aeecd9.jpg" alt="img"></p>
<h5 id="主成分分析与线性回归的比较"><a href="#主成分分析与线性回归的比较" class="headerlink" title="主成分分析与线性回归的比较"></a>主成分分析与线性回归的比较</h5><p>尽管主成分分析与线性回归看上去很相似，但两者是两种不同的算法。主成分分析最小化的是投射误差（<strong>Projected Error</strong>），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p>
<p><img src="http://www.ai-start.com/ml2014/images/7e1389918ab9358d1432d20ed20f8142.png" alt="img"></p>
<p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。</p>
<h5 id="具体算法："><a href="#具体算法：" class="headerlink" title="具体算法："></a>具体算法：</h5><p><strong>PCA</strong> 减少$n$维到$k$维：</p>
<p>第一步是<strong>数据预处理</strong>。我们将进行均值归一化(mean normalization)/特征缩放(feature scaling)。普遍的做法是计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ$。</p>
<p>第二步是计算<strong>协方差矩阵</strong>（<strong>covariance matrix</strong>）<br>$$<br>Σ：<br>\sum=\dfrac {1}{m}\sum^{n}_{i=1}( x^{(i)}) ( x^{(i)}) ^{T}<br>$$</p>
<p>第三步是计算协方差矩阵$Σ$的<strong>特征向量</strong>（<strong>eigenvectors</strong>）:</p>
<p>在 <strong>Octave</strong> 里我们可以利用<strong>奇异值分解</strong>（<strong>singular value decomposition</strong>）来求解，<code>[U, S, V]= svd(sigma)</code>。</p>
<p>对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量<br>$$<br>z^{(i)}:<br>z^{(i)}=U^T_{reduce}*x^{(i)}<br>$$</p>
<p>其中$x$是$n×1$维的，因此结果为$k×1$维度。</p>
<h5 id="选择主成分的数量"><a href="#选择主成分的数量" class="headerlink" title="选择主成分的数量"></a>选择主成分的数量</h5><p>主要成分分析是减少投射的平均均方误差：</p>
<p>训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left| x^{( i) }\right| ^{2}$</p>
<p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。</p>
<p>如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。</p>
<p>我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。</p>
<p>还有一些更好的方式来选择$k$，当我们在<strong>Octave</strong>中调用“<strong>svd</strong>”函数的时候，我们获得三个参数：<code>[U, S, V] = svd(sigma)</code>。</p>
<p>其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：<br>$$<br>\dfrac {\dfrac {1}{m}\sum^{m}<em>{i=1}\left| x^{( i) }-x^{( i) }</em>{approx}\right| ^{2}}{\dfrac {1}{m}\sum^{m}<em>{i=1}\left| x^{(i)}\right| ^{2}}=1-\dfrac {\Sigma^{k}</em>{i=1}S_{ii}}{\Sigma^{m}<em>{i=1}S</em>{ii}}\leq 1%<br>$$<br>也就是：<br>$$<br>\frac {\Sigma^{k}<em>{i=1}s</em>{ii}}{\Sigma^{n}<em>{i=1}s</em>{ii}}\geq0.99<br>$$<br>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：<br>$$<br>x^{( i) }<em>{approx}=U</em>{reduce}z^{(i)}<br>$$</p>
<h5 id="压缩重现"><a href="#压缩重现" class="headerlink" title="压缩重现"></a>压缩重现</h5><p><strong>PCA</strong>算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，$z$为1维，$z=U^{T}<em>{reduce}x$，相反的方程为：<br>$$<br>x</em>{appox}=U_{reduce}\cdot z\ ,\ x_{appox}\approx x<br>$$</p>
<h5 id="几个错误使用"><a href="#几个错误使用" class="headerlink" title="几个错误使用"></a>几个错误使用</h5><p>（一）用来减少过拟合</p>
<p>这样做非常不好，效果也并不优于正则化处理。PCA只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p>
<p>(二)滥用PCA</p>
<p>我们经常不管三七二十一就将PCA算法归入我们的学习过程的一部分。吴恩达给出的建议是：当我们在使用PCA时要考虑不用PCA是否能达到我们的目标，只有做不到的时候采取考虑PCA算法。</p>
]]></content>
  </entry>
  <entry>
    <title>京都大学 2021 年入学試験問題（理系数学）部分解答</title>
    <url>/2021/20210612/</url>
    <content><![CDATA[<p>京都大学 2021 年入学试题略解：</p>
<span id="more"></span>

<p>1.略</p>
<p>2.性质：抛物线上的一点处的切线与该店的搅拌机的过相应焦点的垂线的交点的轨迹为抛物线准线。</p>
<p>3.解：根据欧拉公式：<br>$$<br>\sum_{n=0}^\infty \frac{1}{2^n}\cos (\frac{n\pi}{6})\<br>=Re(\sum_{n=0} \left(\frac{e^\frac{i\pi}{6}}{2}\right)^n)\<br>=Re(\lim_{n\to\infty}\frac{1-(\frac{e^{\frac{i\pi}{6}}}{2})^n}{1-\frac{e^{\frac{i\pi}{6}}}{2}})\<br>=Re(\lim_{n\to\infty}\frac{1}{1-\frac{e^{\frac{i\pi}{6}}}{2}})\<br>=\frac{1}{13}(14+3\sqrt{3})<br>$$<br>4.略。</p>
<p>5.</p>
<p>(1)略</p>
<p>(2)利用性质O为△ABC的外心，H为垂心，求证：$\overrightarrow{OH}=\overrightarrow{OA}+\overrightarrow{OB}+\overrightarrow{OC}$．(见《奥数教程》七（或者八）年级版)</p>
<p>6.</p>
<p>(1)法一：</p>
<p>若n不是质数，设$n=ab$，其中$a、b＞1$。则$3^a-2^a|3^n-2^n$.故$3^n-2^n$是合数，矛盾。</p>
<p>法二：</p>
<p>利用西格蒙德（Zsigmondy）定理可进一步证明:</p>
<p>若$a^n-b^n$为素数的幂，其中$n&gt;2，a、b&gt;1$，且$gcd(a,b)=1$，则n也为素数。</p>
<p>(2)Hint:对a求导</p>
]]></content>
  </entry>
  <entry>
    <title>世另我（一）</title>
    <url>/2021/anotherme/</url>
    <content><![CDATA[<p>来自问题：有没有一部电影让你在深夜中痛哭？</p>
<p>作者：乔维里</p>
<span id="more"></span>

<p>《爆裂鼓手》。<br>因为羡慕，因为我从来都没有那样炽烈的热爱，别说是爱，很强烈的恨，很绝望的悲伤，很无奈的沮丧，我都没有。</p>
<p>我喜欢踢足球，喜欢弹吉他，可球踢得一般，吉他弹得也不好，<br>但当别人问起“你喜欢什么”的时候，我还是会告诉他，“我喜欢踢球，喜欢弹吉他，对了，我还很喜欢生活。”</p>
<p>我很羡慕那些拥有炽烈的感情的人，<br>我曾经一度也想当这样的人。</p>
<p>高二那年，我很喜欢很喜欢的姑娘出了国，<br>中午的时候，我在宿舍午休，上课前，从校裤里偷偷翻出手机看到一条短信，内容我已经不记得了，大意是，要走了，却来不及再见，很抱歉。<br>我脑子里先是晕了一下，然后鼻子一酸，<br>我以为我要哭了，<br>然后外面忽然打了个雷，要下雷阵雨了。</p>
<p>我想，要不我出去吧，到操场上跑一圈，疯一圈，淋一圈，哭一圈，<br>那个时候我内心居然有一闪而过的喜悦，也不知道是为何，我觉得这场雨要造就了我，我一边冲出教室往下走，一边脑补着自己以后慢慢颓废的日子，慢慢变长的头发，慢慢变少的言语，和对姑娘慢慢增加的思念，<br>我就觉得很兴奋，或许是中二，或许是装逼，但我觉得我很快就要拥有一份炽烈的感情了。</p>
<p>我跑到楼下，外面已经哗啦哗啦雨下得很大了。<br>我停在屋檐下，准备冲出去，<br>我先蹲下挽了个裤脚，然后又把校服外套的帽子戴在头上，然后我又撸起了袖子，<br>接着我真的要出去了，<br>然后我打了个喷嚏。<br>阿嚏——正好被上楼经过的班主任看到了，他冲我大叫：“快上课了，你站在下面干嘛？！”</p>
<p>是啊，我干嘛啊？？</p>
<p>于是我灰溜溜的上楼。<br>同桌问我干嘛去了，我说，厕所蹲坑。</p>
<p>那天下午雷阵雨转成持续的大暴雨，我在教室里上完了语文，数学，物理，和自习，<br>偶尔听课，偶尔看着窗外的雨发呆，还在语文课上偷偷看了二十分钟的杂志。</p>
<p>我问自己，XXX走了，去美国了，移民了，你不难过吗？<br>然后我又自己回答自己，是啊，好像挺难过的吧，但好像，也没那么难过。</p>
<p>我看过好多青春片青春电影，<br>里面的分离，男女主角总是在哭，有那么难过吗？<br>里面的再会，女主角总是一副前世今生的怨恨面孔，男主角总是一副人面桃花相映红的感慨，有那么思念吗？<br>里面的结局，男女主角总是在拥吻（除非某一方死了的），有那么爱吗？</p>
<p>没有吧。<br>我觉得没有，<br>我觉得所有的青春片如果发生在我身上，<br>就好像那天的大雨，我缩着头，挽了裤脚，带了帽子，打了喷嚏，却忽然反应过来了，有那么强烈的感情吗？<br>没有吧，没有。</p>
<p>其实并不是所谓的“长得好看的人才有青春”，而是拥有炽烈感情的人才有青春。</p>
<p>但好像很奇怪，我没有，我身边的人也没有，而且我没见过多少人有，听过，听说过，在小说里，在电影里。</p>
<p>再回到《爆裂鼓手》这部电影，<br>第一次看的时候，我大三。<br>那天下午，我一个人坐公交车去市区逛大书店，因为不知道自己到底要看什么书，所以只能去书店看看。<br>很巧，<br>我又遇到了高中时候很喜欢很喜欢，但高二就离开了这里的那个姑娘，<br>大概有，四年多没见了吧，<br>我听声音辨认出她的，她站在我前面，我们站在同一个扶梯上，她只是在说，“妈，我待会就回家。”<br>我当时想，这个姑娘的声音好熟悉啊。</p>
<p>然后她就回头了，她愣了，我也愣了。</p>
<p>可是愣了之后，得有话说吧，说什么呢？当然是好久不见。<br>我说，好久不见，<br>她说，好久不见。<br>我说，你不夸夸我，长高了变帅了之类的？<br>她笑了，你也没长得很高。<br>我盯着她的眼睛，咖啡色的美瞳，我说，你倒是变漂亮了不少。</p>
<p>然后我还应该再说些什么的，<br>毕竟这么有缘，这大概算一种缘分吧。<br>我们站在扶梯的出口，挡住了后面人的道路，后面人说，麻烦让一让，我们就让一让，<br>姑娘在笑着，浅笑，不知道是在笑缘分，还是单纯的觉得尴尬，<br>我忽然指着她背后，书店二楼的天花板上，挂了一个巨大的海报，儿童读物，一个卡通的宇宙，<br>我说，“看，飞碟！”<br>————最烂的“再遇见对话”也不过如此，“看，飞碟。”</p>
<p>之后我们道别，连“再见”也没说，<br>她说，“我去三楼。”<br>我点头，“恩恩，我就在二楼看看。”</p>
<p>那天我还真的挑了好几本书，都很感兴趣，有讲历史的，有讲经济的，还有一本日本侦探悬疑小说，<br>我付了钱，走出门，抬头看天都黑了，“哪特么有飞碟啊！”我心里自己骂了自己一句，</p>
<p>其实我脑补过的，也只是脑补，<br>脑补我在飞奔，我甚至脑补过了细节，我急速转身时鞋子摩擦地板的声音，不小心撞翻的几本书，对挡路的人说着抱歉的“借过”，然后我疯了似的找她，找那个姑娘，<br>这个时候要是有个镜头就好了，摆在我头顶上，镜头在旋转，旋转旋转，我也在旋转，四周全是书，<br>终于，我又见到了那个姑娘，<br>这个时候应该有《独自等待》里夏雨的能耐，“嘿！你忘了一样东西！”“什么？”“我啊！”<br>然后我抱她！说，“我太特么想你了！”</p>
<p>脑补归脑补，现实归现实，<br>况且，我有那么想吗？</p>
<p>那晚我吃了麦当劳，吃了新口味的圣代，喝了新口味的气泡果汁，<br>然后再坐车回学校回寝室，洗衣服洗澡躺床上看书。</p>
<p>什么都没想，<br>真的，什么都没想，连飞碟也没想。</p>
<p>那晚我习惯性的失眠，没什么特别的原因，大学狗，晚上精神焕发是再正常不过的了。<br>我随手翻到了《爆裂鼓手》这部电影，<br>“好像最近挺火的吧，”我心里这样想，然后点开看。</p>
<p>看完我真的哭得跟傻逼一样，<br>起床翻箱倒柜找烟，<br>翻到了几个安全套翻到了两张挂科的成绩单翻到了上海世博会的纪念章，就是找不到烟，一根都没有。<br>我蹲在厕所哭，<br>厕所特臭，<br>我不想张嘴哭，就低着头，声音特别像奇怪的笑。</p>
<p>跟姑娘无关，<br>我在想，我什么时候该有这样热烈的，炽烈的感情啊？<br>大概这辈子都不会有。<br>我喜欢看球，喜欢AC米兰，但我熬夜顶多熬到2点，而他们的比赛大部分在2点45开始；<br>我喜欢吉他，感觉弹吉他会放松身心更重要的是可以耍帅，但我顶多拨十分钟，哼半首歌；<br>我曾经也有很宏大的目标，我发誓要怎样怎样，我买了新本子，买了新笔，用新笔在新本子上写下自己的宏大目标，然后本子和笔都找不到了，甚至看完《爆裂鼓手》的那天晚上，我翻箱倒柜找烟的时候都没有把它们找到；<br>我喜欢爱，喜欢我对别人的爱，也喜欢别人对我的爱，更喜欢互相的爱，可是没了爱会怎样呢？会伤心吧——1秒，会失落吧——3秒，会空虚吧——5秒，<br>之后呢，之后也不会忽然地cheer up起来，也不会全然地抛在脑后，<br>但怎样的，好像也就这样吧。</p>
<p>好像也就这样吧。</p>
<p>就好像《爆裂鼓手》里的主角一样，<br>我后来思考过自己，<br>我甚至从来没有，从来没有为了一件事情，开心得也好，愤怒得也好，怎样都好，从来没有发自内心地说一句“f u c k”，从来没有。</p>
<p>或许是性格使然，或许我还没有遇到，<br>但炽烈的感情真的不属于我，<br>我羡慕，<br>但我似乎也并不选择，<br>就好像即使那天晚上我莫名其妙地哭完之后，擦擦眼泪，还顺便对着镜子照了一下有没有多长痘痘，还顺便打开了厕所的排风扇，<br>然后我问自己，因为什么哭啊？真的有必要哭吗？<br>好像，也没有吧。<br>好像，怎么样，也都可以过下去吧。<br>就这样过下去吧。</p>
<p>. .<br> .<br>1/12<br>留个句号<br>。</p>
]]></content>
  </entry>
  <entry>
    <title>2021新高考一卷压轴题</title>
    <url>/2021/20210607/</url>
    <content><![CDATA[<p>题目：<br>$$<br>f(x)=x(1-lnx) \<br>(1)求单调性\<br>(2)a不等于b，且blna-alnb=a-b \证明：2&lt;\frac{1}{a}+\frac{1}{b}&lt;e<br>$$</p>
<span id="more"></span>

<p>（1）略</p>
<p>（2）</p>
<p>易证$\frac{1}{a}$和$\frac{1}{b}$是$f(x)$与$y=n$的两个两个公共点的横坐标，记作$x_1、x_2$,易知,$0&lt;x_1&lt;1&lt;x_2&lt;e$.</p>
<p>左式解法一：（对称化构造）</p>
<p>令$h(x)=f(2-x)-f(x)$，求导得其在$0&lt;x&lt;1$时单调递减，$f(2-x)&gt;f(x)$</p>
<p>化简可得所证。<br>左式解法二：（放缩）</p>
<p>设<br>$$<br>x_1+a=x_1lnx_1&gt;x_1*(\frac{1}{2}*(x_1-\frac{1}{x_1}))\</p>
<p>x_2+a=x_2lnx_2&lt;x_2*(\frac{1}{2}*(x_2-\frac{1}{x_2}))<br>$$<br>记$h(x)=\frac{x^2}{2}-x-\frac{1}{2}$，则由上两式子得：$h(x_2)&gt;h(x_1)$.</p>
<p>注意到，$h(x)$是对称轴为x=1的二次函数，故得证。</p>
<p>右式解法一：</p>
<p>设$h(x)=\frac{1-lnx}{e-x}.$<br>故$h’(x)=-\frac{-2x+xlnx+e}{x(e-x)^2}&lt;0$<br>$\frac{1-lnx_1}{e-x_1}&gt;\frac{1-lnx_2}{e-x_2}$<br>$\frac{x_1(1-lnx_1)}{x_1(e-x_1)}&gt;\frac{x_2(1-lnx_2)}{x_2(e-x_2)}$<br>$\frac{1}{x_1(e-x_1)}&gt;\frac{1}{x_2(e-x_2)}$<br>$(x_1-x_2)(x_1+x_2-e)&gt;0$</p>
<p>右式解法二：<br>$$<br>x_1&lt;x_1(1-lnx_1)=x_2(1-lnx_2)=x_2ln\frac{e}{x_2}\leq x_2(\frac{e}{x_2}-1)=e-x_2<br>$$<br>得证。</p>
<p>右式解法三：(长风数学)</p>
<p>$x_1&lt;x_1(1-lnx_1)=x_2(1-lnx_2)$</p>
<p>$x_1+x_2&lt;x_2(1-lnx_2)+x_2&lt;e$</p>
<p>右式解法四：（对称化构造）</p>
<p>略。</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习之聚类算法</title>
    <url>/2021/20210606/</url>
    <content><![CDATA[<p>在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签。聚类算法(<strong>Clustering</strong>）是一种非监督学习算法。</p>
<span id="more"></span>

<h5 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h5><p><strong>K-means</strong>是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p>
<p><strong>K-means</strong>是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:</p>
<p>首先选择$K$个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）；</p>
<p>对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</p>
<p>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</p>
<p>重复步骤2-4直至中心点不再变化。</p>
<p><strong>K-均值</strong>算法的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Repeat &#123;</span><br><span class="line"></span><br><span class="line">for i = 1 to m</span><br><span class="line"></span><br><span class="line">c(i) := index (form 1 to K) of cluster centroid closest to x(i)</span><br><span class="line"></span><br><span class="line">for k = 1 to K</span><br><span class="line"></span><br><span class="line">μk := average (mean) of points assigned to cluster k</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>算法分为两个步骤，第一个<strong>for</strong>循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个<strong>for</strong>循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。</p>
<h5 id="K-means-的代价函数"><a href="#K-means-的代价函数" class="headerlink" title="K-means 的代价函数"></a>K-means 的代价函数</h5><p>K-means的代价函数（又称<strong>畸变函数</strong> <strong>Distortion function</strong>）为：<br>$$<br>J(c^{(1)},…,c^{(m)},μ_1,…,μ_K)=\frac {1}{m}\sum^{m}<em>{i=1}\left| X^{\left( i\right) }-\mu</em>{c^{(i)}}\right| ^2<br>$$</p>
<p>其中$\mu _c^{(i)}$代表与$x^{(i)}$最近的聚类中心点。</p>
<p>回顾刚才给出的:<br><strong>K-均值</strong>迭代算法，我们知道，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$\mu _i$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p>
<h5 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h5><p>为了避免使用K-means时出现陷入局部最优，我们通常需要多次（一般为50~1000次）运行<strong>K-均值</strong>算法，每一次都重新进行随机初始化，最后再比较多次运行<strong>K-均值</strong>的结果，选择代价函数最小的结果。这种方法在$K$较小的时候（2–10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。</p>
<h5 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h5><p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。</p>
<p>以下是一个可能会谈及的方法。</p>
<blockquote>
<p>肘部法则：</p>
<p>我们所需要做的是改变$K$值，也就是聚类类别数目的总数。我们用一个聚类来运行<strong>K均值</strong>聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数$J$。$K$代表聚类数字。</p>
<p><img src="https://i0.hdslb.com/bfs/album/ebba8111b6d86e6e594e5f78aa5faf5677c12ba1.png"></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，$K=3$之后就下降得很慢，那么我们就选$K=3$。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
<p>但是通常我们得到的是右图，很难看出”肘部“的位置。所以“肘部法则”并不是一个非常常用的法则。</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>机器学习之支持向量机</title>
    <url>/2021/20210530/</url>
    <content><![CDATA[<p>支持向量机（support vector machines,SVM)是一种二类分类模型。</p>
<span id="more"></span>

<h5 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h5><p>我们回忆一下逻辑回归。其代价函数如下：</p>
<p><img src="http://www.ai-start.com/ml2014/images/66facb7fa8eddc3a860e420588c981d5.png" alt="img"></p>
<p>我们要实现我们的目标，就要把$-\log(1-\frac{1}{1+e^{-z}})$一点一点修改，修改为下两图的紫色直线函数，左边的函数，称之为${\cos}t_1{(z)}$，同时，右边函数称它为${\cos}t_0{(z)}$。</p>
<p>最后SVM代价函数变为：</p>
<p><img src="http://www.ai-start.com/ml2014/images/5a63e35db410fdb57c76de97ea888278.png" alt="img"></p>
<p>最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数$\theta $时，支持向量机所做的是它来直接预测$y$的值等于1，还是等于0。因此，这个假设函数会预测1。当$\theta^Tx$大于或者等于0时，或者等于0时，所以学习参数$\theta $就是支持向量机假设函数的形式。</p>
<h5 id="大间距分类器"><a href="#大间距分类器" class="headerlink" title="大间距分类器:"></a>大间距分类器:</h5><p>支持向量机的目标是找到一个能正确划分数据集且间隔最大的超平面。</p>
<p><img src="http://www.ai-start.com/ml2014/images/e68e6ca3275f433330a7981971eb4f16.png" alt="img"></p>
<p>如该图所示，黑线比红线有着更大的距离。</p>
<h5 id="核函数："><a href="#核函数：" class="headerlink" title="核函数："></a>核函数：</h5><p>核函数可以看成是判断近似程度的函数。</p>
<p><img src="http://www.ai-start.com/ml2014/images/529b6dbc07c9f39f5266bd0b3f628545.png" alt="img"></p>
<p>为了获得上图所示的判定边界，我们的模型可能是<br>$$<br>\theta _0+\theta _1x_1+\theta _2x_2+\theta _3x_1x_2+\theta _4x_1^2+\theta _5x_2^2+\cdots<br>$$<br>的形式。</p>
<p>我们可以用一系列的新的特征$f$来替换模型中的每一项。例如令：<br>$$<br>f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2<br>$$</p>
<p>…得到:<br>$$<br>h_θ(x)=\theta _1f_1+\theta _2f_2+…+\theta _nf_n<br>$$<br>。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3$？我们可以利用核函数来计算出新的特征。</p>
<p>给定一个训练样本$x$，我们利用$x$的各个特征与我们预先选定的<strong>地标</strong>(<strong>landmarks</strong>)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$</p>
<p><img src="http://www.ai-start.com/ml2014/images/2516821097bda5dfaf0b94e55de851e0.png" alt="img"></p>
<p>例如：<br>$$<br>f_1=similarity(x,l^{(1)})=e(-\frac{\left| x-l^{(1)} \right|^2}{2\sigma^2})<br>\<br>其中:\left| x-l^{(1)}\right|^2=\sum{_{j=1}^{n}}{(x_j-l_j^{(1)})^2}<br>$$<br>上例中的$similarity(x,l^{(1)})$就是核函数，具体而言，这里是一个<strong>高斯核函数</strong>(<strong>Gaussian Kernel</strong>)。 <strong>注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</strong></p>
<p>这些地标的作用是什么？如果一个训练样本$x$与地标$l$之间的距离近似于0，则新特征 $f$近似于$e^{-0}=1$，如果训练样本$x$与地标$l$之间距离较远，则$f$近似于$e^{-(一个较大的数)}=0$。</p>
<h5 id="支持向量机参数的影响："><a href="#支持向量机参数的影响：" class="headerlink" title="支持向量机参数的影响："></a>支持向量机参数的影响：</h5><p>两个参数$C$和$\sigma$的影响：</p>
<blockquote>
<p>$C=1/\lambda$</p>
<p>$C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；</p>
<p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差；</p>
<p>$\sigma$较大时，可能会导致低方差，高偏差；</p>
<p>$\sigma$较小时，可能会导致低偏差，高方差。</p>
</blockquote>
<p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<strong>线性核函数</strong>(<strong>linear kernel</strong>)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。</p>
<h5 id="一些核函数"><a href="#一些核函数" class="headerlink" title="一些核函数"></a>一些核函数</h5><p>除了上文提到的高斯核函数外还有：</p>
<p>多项式核函数（<strong>Polynomial Kerne</strong>l）</p>
<p>字符串核函数（<strong>String kernel</strong>）</p>
<p>卡方核函数（ <strong>chi-square kernel</strong>）</p>
<p>直方图交集核函数（<strong>histogram intersection kernel</strong>）</p>
<h5 id="一些软件库和好的软件包"><a href="#一些软件库和好的软件包" class="headerlink" title="一些软件库和好的软件包"></a>一些软件库和好的软件包</h5><p>吴恩达推荐：<strong>liblinear</strong>和<strong>libsvm</strong></p>
<h5 id="多类分类问题："><a href="#多类分类问题：" class="headerlink" title="多类分类问题："></a>多类分类问题：</h5><p>我们同样也可以训练个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p>
<p>尽管你不去写你自己的<strong>SVM</strong>的优化软件，但是你也需要做几件事：</p>
<p>1、是提出参数的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。</p>
<p>2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的<strong>SVM</strong>（支持向量机），这就意味这他使用了不带有核函数的<strong>SVM</strong>（支持向量机）</p>
<p><strong>一些普遍使用的准则：</strong></p>
<p>$n$为特征数，$m$为训练样本数。</p>
<p>(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</p>
<p>(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。</p>
<p>(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</p>
<p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>
]]></content>
  </entry>
  <entry>
    <title>辨析依概率收敛和以概率一收敛</title>
    <url>/2021/20210514/</url>
    <content><![CDATA[<p>依概率收敛和以概率一收敛是常见的收敛定义。但不易分清。</p>
<span id="more"></span>

<p>往往其常见的定义比较含糊。</p>
<p>可以转换一下，变得更加清晰。</p>
<p>定义如下：</p>
<p>Xn是一个随机变量序列，X是随机变量，对于任意自然数ε&gt;0</p>
<p>都有：</p>
<p><strong>依概率收敛：</strong></p>
<p>$$<br>\lim_{n\to\infty}P({\omega:|X_n(\omega)-X_0(\omega)|\geqε})=0<br>$$<br><strong>以概率一收敛（几乎处处收敛）：</strong></p>
<p>$$<br>\lim_{n\to\infty}P({\omega:\sup_{m&gt;n}|X_m(\omega)-X_0(\omega)|\geqε})=0<br>$$</p>
<hr>
<p>我们以一个图示来更加清晰地说明，我们把差值$|x_n-x|$绘成n的函数。为简单起见，把序列画成了曲线。于是曲线表示一特定的序列$|x_n(\xi )-x(\xi )|$。依概率收敛表示对于特定的$n&gt;n_o$，仅有一小部分曲线的坐标超过ε(图(a))。当然，对于每一个$n&gt;n_0$，可能甚至没有一条曲线始终小于ε,另一方面，几乎处处收敛则要求大多数曲线对每个$n&gt;n_0$都低于ε(图(b))。</p>
<p><img src="https://ae01.alicdn.com/kf/U06ed9e75205341088b128eac35fd404bt.jpg" alt="1"></p>
<p>阅读材料：</p>
<p>1.<a href="http://www.themattsimpson.com/2013/08/22/im-almost-sure-its-probably-converging-wait-what/">Matt Simpson-I’m Almost Sure it’s Probably Converging… Wait, What?</a></p>
<p>2.Papoulis A,Pillai S.U.《概率、随机变量与随机过程》</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习之神经网络</title>
    <url>/2021/20210509/</url>
    <content><![CDATA[<p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>所以我们引入神经网络。</p>
<span id="more"></span>

<h5 id="一些神经网络术语："><a href="#一些神经网络术语：" class="headerlink" title="一些神经网络术语："></a>一些神经网络术语：</h5><p>下图为一个3层的神经网络，第一层成为输入层（<strong>Input Layer</strong>），最后一层称为输出层（<strong>Output Layer</strong>），中间一层成为隐藏层（<strong>Hidden Layers</strong>）。我们为每一层都增加一个偏差单位（<strong>bias unit</strong>）：</p>
<p><img src="http://www.ai-start.com/ml2014/images/8293711e1d23414d0a03f6878f5a2d91.jpg" alt="img"></p>
<h5 id="逻辑函数的一些例子："><a href="#逻辑函数的一些例子：" class="headerlink" title="逻辑函数的一些例子："></a>逻辑函数的一些例子：</h5><p>下图的神经元（三个权重分别为-30，20，20）可以被视为作用同于逻辑与（<strong>AND</strong>）：</p>
<p><img src="http://www.ai-start.com/ml2014/images/57480b04956f1dc54ecfc64d68a6b357.jpg" alt="img"></p>
<p>下图的神经元（三个权重分别为-10，20，20）可以被视为作用等同于逻辑或（<strong>OR</strong>）：</p>
<p><img src="http://www.ai-start.com/ml2014/images/7527e61b1612dcf84dadbcf7a26a22fb.jpg" alt="img"></p>
<p>下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（<strong>NOT</strong>）：</p>
<p><img src="http://www.ai-start.com/ml2014/images/1fd3017dfa554642a5e1805d6d2b1fa6.jpg" alt="img"></p>
<p>**$\text{XNOR}$**：</p>
<p>我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现<strong>XNOR</strong> 功能（输入的两个值必须一样，均为1或均为0），即<br>$$<br>\text{XNOR}=( \text{x}_1, \text{AND}, \text{x}_2 ), \text{OR} \left( \left( \text{NOT}, \text{x}_1 \right) \text{AND} \left( \text{NOT}, \text{x}_2 \right) \right)<br>$$<br>首先构造一个能表达$( \text{NOT}, \text{x}_1 ) \text{AND} ( \text{NOT}, \text{x}_2 )$部分的神经元：</p>
<p><img src="http://www.ai-start.com/ml2014/images/4c44e69a12b48efdff2fe92a0a698768.jpg" alt="img"></p>
<p>然后将表示 <strong>AND</strong> 的神经元和表示$( \text{NOT}, \text{x}_1 ) \text{AND} ( \text{NOT}, \text{x}_2 )$的神经元以及表示 OR 的神经元进行组合：</p>
<p><img src="http://www.ai-start.com/ml2014/images/432c906875baca78031bd337fe0c8682.jpg" alt="img"></p>
<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的<strong>neuron</strong>个数($S_l$表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。</p>
<p>将神经网络的分类定义为两种情况：二类分类和多类分类，</p>
<p>二类分类：$S_L=0, y=0, or, 1$表示哪一类；</p>
<p>$K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k&gt;2)$</p>
<p><img src="http://www.ai-start.com/ml2014/images/8f7c28297fc9ed297f42942018441850.jpg" alt="img"></p>
<h5 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h5><p>$$<br>h_\theta(x)\in \mathbb{R}^{K}{({h_\theta}(x))}<em>{i}={i}^{th} \text{output}<br>$$<br>$$<br>J(\Theta) = -\frac{1}{m} \left[ \sum\limits</em>{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log {(h_\Theta(x^{(i)}))}<em>k + \left( 1 - y_k^{(i)} \right) \log \left( 1- {\left( h_\Theta \left( x^{(i)} \right) \right)<em>k} \right) \right] + \frac{\lambda}{2m} \sum\limits</em>{l=1}^{L-1} \sum\limits</em>{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2<br>$$</p>
<p>$h_\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行<strong>regularization</strong>的<strong>bias</strong>项处理所有参数的平方和。</p>
]]></content>
  </entry>
  <entry>
    <title>Wolfram Mathematica识别猫种</title>
    <url>/2021/20210415/</url>
    <content><![CDATA[<p>自Wolfram Mathematica 11.3起逐渐开始支持神经网络等功能，</p>
<span id="more"></span>

<p>相关函数有<a href="https://reference.wolfram.com/language/ref/NetModel.html">NetModel</a>等。</p>
<p>也提供了一系列强大的<a href="https://resources.wolframcloud.com/NeuralNetRepository/">神经网络库</a>，有音频分析、分类、数据生成、特征提取、图像处理、语言建模、</p>
<p>对象检测、回归、语义分段、语音识别等107个库，还是比较全的。</p>
<p>我们检测猫的种类的图片用的是：</p>
<img src="http://www.hereinuk.com/wp-content/uploads/2016/06/14654596596466.jpg" alt="1" style="zoom:25%;" />





<p>分别使用了：</p>
<p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/8d890763-4d74-4f89-90bf-88ef7c60f439/">Inception V3 Trained on ImageNet Competition Data</a>:</p>
<p>该模型由Google Inc.（也称为GoogLeNet）于2015年发布，这个模型建立在之前的Inception V1之上，使用不到100 MB的参数将性能提高了15%。它比它的前辈有一些架构上的改进，例如大滤波器卷积和非对称卷积级的分解。</p>
<p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/EfficientNet-Trained-on-ImageNet-with-AdvProp-and-AutoAugment/">EfficientNet Trained on ImageNet with AdvProp and AutoAugment</a>:</p>
<p>该模型于2019年发布，利用EfficientNet架构上的AdvProp和AutoAugment数据增强技术有效地执行图像分类。</p>
<p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/">ResNet-101 Trained on ImageNet Competition Data</a>:</p>
<p>微软亚洲研究院于2015年发布的ResNet架构(包括ResNet-50，ResNet-101和ResNet-152),在ImageNet和MS-COCO竞赛中获得了非常成功的成绩。这些模型中所利用的核心思想残余连接(residual connections)极大地改善了梯度流动，从而允许训练更深入的模型，包括数十层甚至数百层。</p>
<blockquote>
<p>In[1]=NetModel[“Inception V3 Trained on ImageNet Competition Data”][a, “TopProbabilities”]</p>
<p>Out[1]={tabbycat-&gt;0.211156,Egyptiancat-&gt;0.154156,tigercat-&gt;0.135589,bathtub-&gt;0.040887,vat-&gt;0.0263624}</p>
</blockquote>
<blockquote>
<p>In[2]= NetModel[“EfficientNet Trained on ImageNet with AdvProp and AutoAugment”][a, “TopProbabilities”]</p>
<p>Out[2]={Egyptiancat-&gt;0.419587,tabbycat-&gt;0.101588}</p>
</blockquote>
<blockquote>
<p>In[3]=NetModel[“ResNet-101 Trained on ImageNet Competition Data”][a, “TopProbabilities”]</p>
<p>Out[3]={Egyptiancat-&gt;0.735536,tabbycat-&gt;0.0787424}</p>
</blockquote>
<p>根据以上结果，这只猫大概率是埃及猫（Egyptiancat），也有可能是虎斑猫（tabbycat）。其中Inception V3给出了最多的可能答案，虽然两种不是猫（？）。而EfficientNet和ResNet-101给出Egyptiancat的概率均远大于其他。【虽然我觉得猫的种类结果都不太对（？）】</p>
<p>PS:没有这次识别，我也不清楚猫竟然有这么多种种类，有点孤陋寡闻。</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习之回归</title>
    <url>/2021/20210411/</url>
    <content><![CDATA[<h5 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h5><p>（<strong>Arthur Samuel</strong>）在进行特定编程的情况下，给予计算机学习能力的领域。</p>
<p>（<strong>Tom Mitchell</strong>）一个程序被认为能从经验（Experience，<strong>E</strong>)中学习，解决任务(Task,<strong>T</strong>)，达到性能度量值(preference，<strong>P</strong>），有了经验<strong>E</strong>后，经过<strong>P</strong>评判，程序在处理T时的性能有所提升。</p>
<p>监督学习：监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。</p>
<p>无监督学习：数据集中没有任何的标签，没有提前告知算法一些信息。</p>
<span id="more"></span>

<h5 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h5><p>相关符号:</p>
<blockquote>
<p>m代表训练集中实例的数量</p>
<p>x代表特征/输入变量</p>
<p>y代表目标变量/输出变量</p>
<p>(x,y)代表训练集中的实例</p>
<p>$(x^{(i)},y^{(i)})$代表第$i$个观察实例</p>
<p>h为<strong>hypothesis</strong>（假设）,假设函数，在这一节记为$h_{\theta \ }(x)=\theta \ _0+\theta \ _1x$</p>
</blockquote>
<p>为了找到最好的回归直线，我们要使得代价函数<br>$$<br>J(\theta \ _0,\theta \ _1)=\frac{1}{2m}\sum <em>{i=1}^{\ m}(h</em>{\theta \ }(x^{(i)})-y^{(i)}\ )^2<br>$$<br>最小。（这是回归中常用的代价函数，故又被称为平方误差代价函数，square error  function）</p>
<p>梯度下降法是一个比较常用的方法。</p>
<h5 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h5><p>我们引入一系列新的注释：</p>
<p>$n$ 代表特征的数量</p>
<p>$x^{( i )}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p>
<p>在多变量线性回归中，<br>$$<br>h_{\theta}( x )=\theta^TX={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+…+{\theta_{n}}{x_{n}}<br>$$</p>
<p>代价函数：<br>$$<br>J( {\theta_{0}},{\theta_{1}}…{\theta_{n}} )=\frac{1}{2m}\sum\limits_{i=1}^{m}{( h_\theta (X^{( i )})-{y}^{( i )} )}^{2}<br>$$</p>
<p>当使用梯度下降法时：</p>
<blockquote>
<p> 在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p>
</blockquote>
<p><img src="http://www.ai-start.com/ml2014/images/b8167ff0926046e112acf789dba98057.png" alt="img"></p>
<blockquote>
<p> 最简单的方法是令：${x_n}=\frac{x_n-\mu_n}{s_n}$，其中 ${\mu_n}$是平均值，${s_n}$是标准差(极差也行)。</p>
</blockquote>
<p>使用正规方程：</p>
<blockquote>
<p>假设我们的训练集特征矩阵为 $X$（包含了 ${X_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量:<br>$$<br>\theta =( X^T X )^{-1}{X^T}y<br>$$</p>
</blockquote>
<p>梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody><tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算$(X^TX )^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O( n^{3} )$，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型梯度下降与正规方程的比较：</td>
</tr>
</tbody></table>
<p>若$(X^T X)^-1$不可逆的情况:<br>(1)首先，看特征值里是否有一些多余的特征，像这些${x_1}$和${x_2}$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。<br>(2)正规化方法<br>(3)直接使用伪逆(pseudoinverse）,有时被称为广义逆（Generalized inverse）。定义：A* B* A=A，则B是A的广义逆矩阵。所有的矩阵都存在逆矩阵。若一矩阵存在逆矩阵，则其逆矩阵即为其唯一的广义逆矩阵。Matlab的伪逆函数是pinv(逆矩阵函数为inv），可以证明：使用伪逆，以下式子任成立<br>$$<br>\theta =( X^T X )^{-1}{X^T}y<br>$$</p>
<h5 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归 (Logistic Regression)"></a>逻辑回归 (<strong>Logistic Regression</strong>)</h5><p>简单而言，即将数据分类划分为离散的值{0,1}。</p>
<p>我们将因变量(<strong>dependent variable</strong>)可能属于的两个类分别称为负向类（<strong>negative class</strong>）和正向类（<strong>positive class</strong>），则因变量$y\in { 0,1 \}$ ，其中 0 表示负向类，1 表示正向类。</p>
<p>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。<br>逻辑回归模型的假设是： $h_\theta ( x )=g(\theta^{T}X )$<br>其中：<br>$X$ 代表特征向量<br>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为： $g( z )=\frac{1}{1+e^{-z}}$。</p>
<p>该函数的图像为：</p>
<p><img src="http://www.ai-start.com/ml2014/images/1073efb17b0d053b4f9218d4393246cc.jpg" alt="img"></p>
<p>$h_\theta ( x )$的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1的可能性（<strong>estimated probablity</strong>）即$h_\theta ( x )=P( y=1|x;\theta )$<br>例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta ( x )=0.7$，则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7=0.3。</p>
<p><strong>代价函数：</strong></p>
<p>若按照原来的公式，我们得到的代价函数将是一个非凸函数（<strong>non-convexfunction</strong>）。</p>
<p><img src="http://www.ai-start.com/ml2014/images/8b94e47b7630ac2b0bcb10d204513810.jpg" alt="img"></p>
<p>这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p>
<p>故我们重新定义逻辑回归的代价函数为：$J( \theta  )=\frac{1}{m}\sum\limits_{i=1}^{m}Cost( {h_\theta}( {x}^{( i )} ),{y}^{( i )} )}$，其中<img src="http://www.ai-start.com/ml2014/images/54249cb51f0086fa6a805291bf2639f1.png" alt="img"></p>
<p>${h_\theta}( x )$与 $Cost( {h_\theta}( x ),y )$之间的关系如下图所示：</p>
<p><img src="http://www.ai-start.com/ml2014/images/ffa56adcc217800d71afdc3e0df88378.jpg" alt="img"></p>
<p><em>\theta}( x )$的变大而变大。<br>将构建的 $Cost( {h_\theta}( x ),y )$简化如下：<br>$Cost( {h_\theta}( x ),y )=-y\times log( {h_\theta}( x ) )-(1-y)\times log( 1-{h_\theta}( x ) )$<br>带入代价函数得到：<br>$J( \theta  )=\frac{1}{m}\sum\limits</em>{i=1}^{m}{[-y^{(i)}\log ( {h_\theta}( x^{(i)} ) )-( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)} ) )]}$<br>即：$J( \theta  )=-\frac{1}{m}\sum\limits_{i=1}^{m}{[y^{(i)}\log ( {h_\theta}( x^{(i)} ) )+( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)} ) )]}$</p>
<p><strong>一对多的情况：</strong></p>
<p><img src="http://www.ai-start.com/ml2014/images/b72863ce7f85cd491e5b940924ef5a5f.png" alt="img"></p>
<p>我们只需分别多次如右上角的操作，可化归为分成两类的逻辑回归。</p>
<h5 id="正规化"><a href="#正规化" class="headerlink" title="正规化"></a>正规化</h5><p>我们已经学了几种算法，如最小二乘法（Ordinary Least Squares,OLS)。我们所学的算法不足之处是随着特征维度的增加会出现线性模型的过度拟合。</p>
<p><img src="http://www.ai-start.com/ml2014/images/72f84165fbf1753cd516e65d5e91c0d3.jpg" alt="img"></p>
<p>过拟合解决方案：</p>
<ol>
<li><p>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</p>
</li>
<li><p>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p>
</li>
</ol>
<p>正则化：</p>
<p>我们可以修改代价函数，增加几个惩罚项。比如将代价函数写成：<br>$$<br>J\left( \theta  \right)=\frac{1}{2m}[\sum\limits_{i=1}^{m}({h_\theta}(x^{(i)})-y^{(i)})^{2}+\lambda \sum\limits_{j=1}^{n}{\theta_j^2}]<br>$$</p>
<p>其中$\lambda $又称为正则化参数（<strong>Regularization Parameter</strong>）。 注：根据惯例，我们不对${\theta_{0}}$ 进行惩罚。事实上其也被称为岭回归：（Ridge Regression），由俄罗斯科学家Tikhonov 提出的对OLS的改进。$\lambda \sum\limits_{j=1}^{n}{\theta_j^2}$被称为L2惩罚项（L2 Penalty）。</p>
<blockquote>
<p> 附：Lasso回归：</p>
<p>与岭回归类似：</p>
</blockquote>
<p>$$<br>\lambda \sum\limits_{j=1}^{n}{\theta_j^2}改为\lambda \sum\limits_{j=1}^{n}{|\theta_j|}<br>$$</p>
<blockquote>
<p>相应地，$\lambda \sum\limits_{j=1}^{n}{|\theta_j|}$被称为L1惩罚项。</p>
</blockquote>
<p>在这里，我们展示只讨论岭回归。</p>
<p>我们也利用正规方程来求解正则化线性回归模型：<img src="http://www.ai-start.com/ml2014/images/71d723ddb5863c943fcd4e6951114ee3.png" alt="img"></p>
<p>图中的矩阵尺寸为 （n+1）*(n+1)。</p>
<p>同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：</p>
<p>$$<br>J(\theta)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-y^{(i)}\log (h_\theta( x^{(i)}) )-( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)}) )]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}\theta _j ^2<br>$$</p>
]]></content>
  </entry>
  <entry>
    <title>机器学习教程中文笔记——斯坦福大学2014（吴恩达）</title>
    <url>/2021/20210410/</url>
    <content><![CDATA[<p>吴恩达老师的机器学习课程，可以说是机器学习入门的第一课和最热门最经典的课程，在吴老师自己创办的coursera上可以观看。</p>
<p>以下是笔记部分：</p>
<span id="more"></span>

<p>基本来自：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p>
<p>目录:</p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week1.html">第一周</a></li>
</ul>
<p>一、 引言(<strong>Introduction</strong>) </p>
<p>1.1 欢迎 </p>
<p>1.2 机器学习是什么？ </p>
<p>1.3 监督学习 </p>
<p>1.4 无监督学习 </p>
<p>二、单变量线性回归**(Linear Regression with One Variable**) </p>
<p>2.1 模型表示 </p>
<p>2.2 代价函数 </p>
<p>2.3 代价函数的直观理解I </p>
<p>2.4 代价函数的直观理解II </p>
<p>2.5 梯度下降 </p>
<p>2.6 梯度下降的直观理解 </p>
<p>2.7 梯度下降的线性回归 </p>
<p>2.8 接下来的内容 </p>
<p>三、线性代数回顾(<strong>Linear Algebra Review</strong>) </p>
<p>3.1 矩阵和向量 </p>
<p>3.2 加法和标量乘法 </p>
<p>3.3 矩阵向量乘法 </p>
<p>3.4 矩阵乘法 </p>
<p>3.5 矩阵乘法的性质 </p>
<p>3.6 逆、转置</p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week2.html">第二周</a></li>
</ul>
<p>四、多变量线性回归(<strong>Linear Regression with Multiple Variables</strong>) </p>
<p>4.1 多维特征 </p>
<p>4.2 多变量梯度下降 </p>
<p>4.3 梯度下降法实践1-特征缩放 </p>
<p>4.4 梯度下降法实践2-学习率 </p>
<p>4.5 特征和多项式回归 </p>
<p>4.6 正规方程 </p>
<p>4.7 正规方程及不可逆性（选修） </p>
<p>五、Octave教程(<strong>Octave Tutorial</strong>) </p>
<p>5.1 基本操作 </p>
<p>5.2 移动数据 </p>
<p>5.3 计算数据 </p>
<p>5.4 绘图数据 </p>
<p>5.5 控制语句：<strong>for</strong>，<strong>while</strong>，<strong>if</strong>语句 </p>
<p>5.6 向量化 88</p>
<p>5.7 工作和提交的编程练习 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week3.html">第三周</a></li>
</ul>
<p>六、逻辑回归(<strong>Logistic Regression</strong>) </p>
<p>6.1 分类问题 </p>
<p>6.2 假说表示 </p>
<p>6.3 判定边界 </p>
<p>6.4 代价函数 </p>
<p>6.5 简化的成本函数和梯度下降 </p>
<p>6.6 高级优化 </p>
<p>6.7 多类别分类：一对多 </p>
<p>七、正则化(<strong>Regularization</strong>) </p>
<p>7.1 过拟合的问题 </p>
<p>7.2 代价函数 </p>
<p>7.3 正则化线性回归 </p>
<p>7.4 正则化的逻辑回归模型 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week4.html">第四周</a></li>
</ul>
<p>第八、神经网络：表述(<strong>Neural Networks: Representation</strong>) </p>
<p>8.1 非线性假设 </p>
<p>8.2 神经元和大脑 </p>
<p>8.3 模型表示1 </p>
<p>8.4 模型表示2 </p>
<p>8.5 样本和直观理解1 </p>
<p>8.6 样本和直观理解II </p>
<p>8.7 多类分类 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week5.html">第五周</a></li>
</ul>
<p>九、神经网络的学习(<strong>Neural Networks: Learning</strong>) </p>
<p>9.1 代价函数 </p>
<p>9.2 反向传播算法 </p>
<p>9.3 反向传播算法的直观理解 </p>
<p>9.4 实现注意：展开参数 </p>
<p>9.5 梯度检验 </p>
<p>9.6 随机初始化 </p>
<p>9.7 综合起来 </p>
<p>9.8 自主驾驶 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week6.html">第六周</a></li>
</ul>
<p>十、应用机器学习的建议(<strong>Advice for Applying Machine Learning</strong>) </p>
<p>10.1 决定下一步做什么 </p>
<p>10.2 评估一个假设 </p>
<p>10.3 模型选择和交叉验证集 </p>
<p>10.4 诊断偏差和方差 </p>
<p>10.5 正则化和偏差/方差 </p>
<p>10.6 学习曲线 </p>
<p>10.7 决定下一步做什么 </p>
<p>十一、机器学习系统的设计(<strong>Machine Learning System Design</strong>) </p>
<p>11.1 首先要做什么 </p>
<p>11.2 误差分析 </p>
<p>11.3 类偏斜的误差度量 </p>
<p>11.4 查准率和查全率之间的权衡 </p>
<p>11.5 机器学习的数据 </p>
<p><a href="http://www.ai-start.com/ml2014/html/week7.html">第7周</a></p>
<p>十二、支持向量机(<strong>Support Vector Machines</strong>) </p>
<p>12.1 优化目标 </p>
<p>12.2 大边界的直观理解 </p>
<p>12.3 大边界分类背后的数学（选修） </p>
<p>12.4 核函数1 </p>
<p>12.5 核函数2 </p>
<p>12.6 使用支持向量机 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week8.html">第八周</a></li>
</ul>
<p>十三、聚类(<strong>Clustering</strong>) </p>
<p>13.1 无监督学习：简介 </p>
<p>13.2 K-均值算法 </p>
<p>13.3 优化目标 </p>
<p>13.4 随机初始化</p>
<p>13.5 选择聚类数 </p>
<p>十四、降维(<strong>Dimensionality Reduction</strong>) </p>
<p>14.1 动机一：数据压缩 </p>
<p>14.2 动机二：数据可视化 </p>
<p>14.3 主成分分析问题 </p>
<p>14.4 主成分分析算法 </p>
<p>14.5 选择主成分的数量 </p>
<p>14.6 重建的压缩表示 </p>
<p>14.7 主成分分析法的应用建议 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week9.html">第九周</a></li>
</ul>
<p>十五、异常检测(<strong>Anomaly Detection</strong>) </p>
<p>15.1 问题的动机 </p>
<p>15.2 高斯分布 </p>
<p>15.3 算法 </p>
<p>15.4 开发和评价一个异常检测系统 </p>
<p>15.5 异常检测与监督学习对比 </p>
<p>15.6 选择特征 </p>
<p>15.7 多元高斯分布（选修） </p>
<p>15.8 使用多元高斯分布进行异常检测（选修） </p>
<p>十六、推荐系统(<strong>Recommender Systems</strong>) </p>
<p>16.1 问题形式化 </p>
<p>16.2 基于内容的推荐系统 </p>
<p>16.3 协同过滤 </p>
<p>16.4 协同过滤算法 </p>
<p>16.5 向量化：低秩矩阵分解 </p>
<p>16.6 推行工作上的细节：均值归一化 </p>
<ul>
<li><a href="http://www.ai-start.com/ml2014/html/week10.html">第十周</a></li>
</ul>
<p>十七、大规模机器学习(<strong>Large Scale Machine Learning</strong>) </p>
<p>17.1 大型数据集的学习 </p>
<p>17.2 随机梯度下降法 </p>
<p>17.3 小批量梯度下降 </p>
<p>17.4 随机梯度下降收敛 </p>
<p>17.5 在线学习 </p>
<p>17.6 映射化简和数据并行 </p>
<p>十八、应用实例：图片文字识别(<strong>Application Example: Photo OCR</strong>) </p>
<p>18.1 问题描述和流程图</p>
<p>18.2 滑动窗口 </p>
<p>18.3 获取大量数据和人工数据 </p>
<p>18.4 上限分析：哪部分管道的接下去做 </p>
<p>十九、总结(<strong>Conclusion</strong>) </p>
<p>19.1 总结和致谢 </p>
<hr>
]]></content>
  </entry>
  <entry>
    <title>有趣的几道数院每日一题</title>
    <url>/2021/20210405/</url>
    <content><![CDATA[<p>今天的每日一题虽然难度不大，但比较有趣，事实上也无需大学的知识就能解决。</p>
<span id="more"></span>

<hr>
<p>数分：</p>
<blockquote>
<p>求极限：<br>$$<br>lim_{n\to \infty}sinsin….x.\ \ \ \ (n个sin)<br>$$</p>
</blockquote>
<p>几代：</p>
<blockquote>
<p>设f(x)是整系数多项式，若f(0)与f(1)均为奇数.证明：f(x)无整根。</p>
</blockquote>
<p>第一题：</p>
<p>事实上这个极限与$\frac{1}{\sqrt{n}}$同阶，更直接地说，这个极限与$\frac{\sqrt{3}}{\sqrt{n}}$等价。</p>
<p>详细可见论文： F. W. Hartmann.E2451(The Iterated Sine).The American Mathematical Monthly, Vol. 82, No. 1 (Jan., 1975), pp. 82-83</p>
<p>另解：</p>
<p>还可以利用：<br>$$<br>当x&gt;0时，sinx&lt;\frac{x}{\sqrt{1+\frac{x^2}{3}}}<br>$$</p>
<p>这个式子非常适合迭代。</p>
<p>第二题：</p>
<p>解法一：（反证法）</p>
<p>假设f(x)有整根n,则f(x)=(x-n)g(x),g(x）也为整系数多项式,<br>因为f(0)=-ng(0)为奇数,所以n为奇数,<br>又f(1)=-(n-1)g(1)为奇数,所以n-1为奇数；所以,n-1、n都为奇数,矛盾.<br>所以,假设不成立,<br>所以,f(x)无整根.</p>
<p>解法二：</p>
<p>f(x)是整系数多项式，故f(n)只有整数之间加、减、乘运算。整数可分为奇数和偶数，且满足以下运算法则：</p>
<p>​    奇数+奇数=偶数</p>
<p>​    奇数+偶数=偶数</p>
<p>​    偶数+偶数=偶数</p>
<p>​    奇数*奇数=奇数</p>
<p>​    奇数*偶数=偶数</p>
<p>​    偶数*偶数=偶数</p>
<p>而且运算是封闭的。</p>
<p>f(0)为奇代表着f(n)为奇（n为偶数）</p>
<p>f(1)为奇代表着f(n)为奇（n为奇数）</p>
<p>故f(n)都为奇数，故不存在整根。</p>
]]></content>
  </entry>
  <entry>
    <title>一些简单的串的模式匹配算法汇总</title>
    <url>/2021/20210331/</url>
    <content><![CDATA[<p>串的模式匹配算法在许多领域，包括爬虫、机器学习等都有重要应用。以下列了一些串的模式匹配算法：</p>
<span id="more"></span>

<hr>
<h3 id="1-Brute-Force算法（暴力算法）"><a href="#1-Brute-Force算法（暴力算法）" class="headerlink" title="1.Brute-Force算法（暴力算法）"></a><strong>1.Brute-Force算法（暴力算法）</strong></h3><p>正如其名，非常直接且简单。</p>
<p>首先将匹配串和模式串左对齐，然后从左向右一个一个进行比较，如果不成功则模式串向右移动一个单位。</p>
<p>例：</p>
<p>设置两个指针，一个匹配串指针i，一个模式串指针j</p>
<p>一开始i=1,j=1</p>
<p>第一个不符合，i++</p>
<p>…当i=3时，</p>
<p>匹配串 ：ab<strong>cbc</strong>sdxzcxx</p>
<p>模式串：<strong>cbc</strong>ac</p>
<p>符合i++,j++，依旧符合i++,j++，直到i=6,j=4，不符合，i回退到3+1，j回退到1</p>
<p>…</p>
<p>该算法的缺陷是匹配串指针i会不断地回退，使时间复杂度较高。每次进行，对模式串的错误没有记忆，相当于每次模式串都是全新。接下来的算法基本都利用了模式串匹配的错误信息，从而避免了匹配串指针i不会一直回退。</p>
<h3 id="2-KMP算法"><a href="#2-KMP算法" class="headerlink" title="2.KMP算法"></a><strong>2.KMP算法</strong></h3><blockquote>
<p>相关论文：<br>Knuth D.E., Morris J.H., and Pratt V.R., Fast pattern matching in strings, SIAM Journal on Computing, 6(2), 323-350, 1977.</p>
</blockquote>
<p>以下来自：<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html">http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html</a></p>
<p>1.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050103.png" alt="img"></p>
<p>首先，字符串”BBC ABCDAB ABCDABCDABDE”的第一个字符与搜索词”ABCDABD”的第一个字符，进行比较。因为B与A不匹配，所以搜索词后移一位。</p>
<p>2.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050104.png" alt="img"></p>
<p>因为B与A不匹配，搜索词再往后移。</p>
<p>3.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050105.png" alt="img"></p>
<p>就这样，直到字符串有一个字符，与搜索词的第一个字符相同为止。</p>
<p>4.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050106.png" alt="img"></p>
<p>接着比较字符串和搜索词的下一个字符，还是相同。</p>
<p>5.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p>
<p>直到字符串有一个字符，与搜索词对应的字符不相同为止。</p>
<p>6.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050108.png" alt="img"></p>
<p>这时，最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把”搜索位置”移到已经比较过的位置，重比一遍。</p>
<p>7.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p>
<p>一个基本事实是，当空格与D不匹配时，你其实知道前面六个字符是”ABCDAB”。KMP算法的想法是，设法利用这个已知信息，不要把”搜索位置”移回已经比较过的位置，继续把它向后移，这样就提高了效率。</p>
<p>8.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png" alt="img"></p>
<p>怎么做到这一点呢？可以针对搜索词，算出一张《部分匹配表》（Partial Match Table）。这张表是如何产生的，后面再介绍，这里只要会用就可以了。</p>
<p>9.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p>
<p>已知空格与D不匹配时，前面六个字符”ABCDAB”是匹配的。查表可知，最后一个匹配字符B对应的”部分匹配值”为2，因此按照下面的公式算出向后移动的位数：</p>
<blockquote>
<p>　　移动位数 = 已匹配的字符数 - 对应的部分匹配值</p>
</blockquote>
<p>因为 6 - 2 等于4，所以将搜索词向后移动4位。</p>
<p>10.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050110.png" alt="img"></p>
<p>因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（”AB”），对应的”部分匹配值”为0。所以，移动位数 = 2 - 0，结果为 2，于是将搜索词向后移2位。</p>
<p>11.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050111.png" alt="img"></p>
<p>因为空格与A不匹配，继续后移一位。</p>
<p>12.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png" alt="img"></p>
<p>逐位比较，直到发现C与D不匹配。于是，移动位数 = 6 - 2，继续将搜索词向后移动4位。</p>
<p>13.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050113.png" alt="img"></p>
<p>逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 = 7 - 0，再将搜索词向后移动7位，这里就不再重复了。</p>
<p>14.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050114.png" alt="img"></p>
<p>下面介绍《部分匹配表》是如何产生的。</p>
<p>首先，要了解两个概念：”前缀”和”后缀”。 “前缀”指除了最后一个字符以外，一个字符串的全部头部组合；”后缀”指除了第一个字符以外，一个字符串的全部尾部组合。</p>
<p>15.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png" alt="img"></p>
<p>“部分匹配值”就是”前缀”和”后缀”的最长的共有元素的长度。以”ABCDABD”为例，</p>
<blockquote>
<p>　　－　“A”的前缀和后缀都为空集，共有元素的长度为0；</p>
<p>　　－　“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；</p>
<p>　　－　“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；</p>
<p>　　－　“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；</p>
<p>　　－　“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；</p>
<p>　　－　“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；</p>
<p>　　－　“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。</p>
</blockquote>
<p>16.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png" alt="img"></p>
<p>“部分匹配”的实质是，有时候，字符串头部和尾部会有重复。比如，”ABCDAB”之中有两个”AB”，那么它的”部分匹配值”就是2（”AB”的长度）。搜索词移动的时候，第一个”AB”向后移动4位（字符串长度-部分匹配值），就可以来到第二个”AB”的位置。</p>
<p>（完）</p>
<h3 id="3-Boyer-Moore算法"><a href="#3-Boyer-Moore算法" class="headerlink" title="3. Boyer-Moore算法"></a><strong>3. Boyer-Moore算法</strong></h3><blockquote>
<p>相关论文：<br>R.S.Boyer, J.S.Moore, A fast string searching algorithm , Communications of the ACM,20(10):762-772 ,1977<br>来自<a href="http://www.ruanyifeng.com/blog/2013/05/boyer-moore_string_search_algorithm.html">http://www.ruanyifeng.com/blog/2013/05/boyer-moore_string_search_algorithm.html</a></p>
</blockquote>
<p>1.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050302.png" alt="img"></p>
<p>假定字符串为”HERE IS A SIMPLE EXAMPLE”，搜索词为”EXAMPLE”。</p>
<p>2.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050303.png" alt="img"></p>
<p>首先，”字符串”与”搜索词”头部对齐，从尾部开始比较。</p>
<p>这是一个很聪明的想法，因为如果尾部字符不匹配，那么只要一次比较，就可以知道前7个字符（整体上）肯定不是要找的结果。</p>
<p>我们看到，”S”与”E”不匹配。这时，**”S”就被称为”坏字符”（bad character），即不匹配的字符。**我们还发现，”S”不包含在搜索词”EXAMPLE”之中，这意味着可以把搜索词直接移到”S”的后一位。</p>
<p>3.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050304.png" alt="img"></p>
<p>依然从尾部开始比较，发现”P”与”E”不匹配，所以”P”是”坏字符”。但是，”P”包含在搜索词”EXAMPLE”之中。所以，将搜索词后移两位，两个”P”对齐。</p>
<p>4.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050305.png" alt="img"></p>
<p>我们由此总结出**”坏字符规则”**：</p>
<blockquote>
<p>　　后移位数 = 坏字符的位置 - 搜索词中的上一次出现位置</p>
</blockquote>
<p>如果”坏字符”不包含在搜索词之中，则上一次出现位置为 -1。</p>
<p>以”P”为例，它作为”坏字符”，出现在搜索词的第6位（从0开始编号），在搜索词中的上一次出现位置为4，所以后移 6 - 4 = 2位。再以前面第二步的”S”为例，它出现在第6位，上一次出现位置是 -1（即未出现），则整个搜索词后移 6 - (-1) = 7位。</p>
<p>5.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050306.png" alt="img"></p>
<p>依然从尾部开始比较，”E”与”E”匹配。</p>
<p>6.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050307.png" alt="img"></p>
<p>比较前面一位，”LE”与”LE”匹配。</p>
<p>7.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050308.png" alt="img"></p>
<p>比较前面一位，”PLE”与”PLE”匹配。</p>
<p>8.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png" alt="img"></p>
<p>比较前面一位，”MPLE”与”MPLE”匹配。<strong>我们把这种情况称为”好后缀”（good suffix），即所有尾部匹配的字符串。</strong>注意，”MPLE”、”PLE”、”LE”、”E”都是好后缀。</p>
<p>9.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050310.png" alt="img"></p>
<p>比较前一位，发现”I”与”A”不匹配。所以，”I”是”坏字符”。</p>
<p>10.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050311.png" alt="img"></p>
<p>根据”坏字符规则”，此时搜索词应该后移 2 - （-1）= 3 位。问题是，此时有没有更好的移法？</p>
<p>11.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png" alt="img"></p>
<p>我们知道，此时存在”好后缀”。所以，可以采用**”好后缀规则”**：</p>
<blockquote>
<p>　　后移位数 = 好后缀的位置 - 搜索词中的上一次出现位置</p>
</blockquote>
<p>举例来说，如果字符串”ABCDAB”的后一个”AB”是”好后缀”。那么它的位置是5（从0开始计算，取最后的”B”的值），在”搜索词中的上一次出现位置”是1（第一个”B”的位置），所以后移 5 - 1 = 4位，前一个”AB”移到后一个”AB”的位置。</p>
<p>再举一个例子，如果字符串”ABCDEF”的”EF”是好后缀，则”EF”的位置是5 ，上一次出现的位置是 -1（即未出现），所以后移 5 - (-1) = 6位，即整个字符串移到”F”的后一位。</p>
<p>这个规则有三个注意点：</p>
<blockquote>
<p>　　（1）”好后缀”的位置以最后一个字符为准。假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5（从0开始计算）。</p>
<p>　　（2）如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。</p>
<p>　　（3）如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，请问这时”好后缀”的上一次出现位置是什么？回答是，此时采用的好后缀是”B”，它的上一次出现位置是头部，即第0位。这个规则也可以这样表达：如果最长的那个”好后缀”只出现一次，则可以把搜索词改写成如下形式进行位置计算”(DA)BABCDAB”，即虚拟加入最前面的”DA”。</p>
</blockquote>
<p>回到上文的这个例子。此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以后移 6 - 0 = 6位。</p>
<p>12.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050312.png" alt="img"></p>
<p>可以看到，”坏字符规则”只能移3位，”好后缀规则”可以移6位。所以，<strong>Boyer-Moore算法的基本思想是，每次后移这两个规则之中的较大值。</strong></p>
<p>更巧妙的是，这两个规则的移动位数，只与搜索词有关，与原字符串无关。因此，可以预先计算生成《坏字符规则表》和《好后缀规则表》。使用时，只要查表比较一下就可以了。</p>
<p>13.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050313.png" alt="img"></p>
<p>继续从尾部开始比较，”P”与”E”不匹配，因此”P”是”坏字符”。根据”坏字符规则”，后移 6 - 4 = 2位。</p>
<p>14.</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050314.png" alt="img"></p>
<p>从尾部开始逐位比较，发现全部匹配，于是搜索结束。如果还要继续查找（即找出全部匹配），则根据”好后缀规则”，后移 6 - 0 = 6位，即头部的”E”移到尾部的”E”的位置。</p>
<p>（完）</p>
<h3 id="4-Horspool算法"><a href="#4-Horspool算法" class="headerlink" title="4.Horspool算法"></a><strong>4.Horspool算法</strong></h3><blockquote>
<p>相关论文：</p>
<p>Horspool R.N., 1980, Practical fast searching in strings, Software - Practice &amp; Experience, 10(6):501-506</p>
</blockquote>
<p>Horspool算法的一个独特之处是它的模式串是尾匹配的（即从右到左）</p>
<p>例：</p>
<p>我们将在匹配串寻找字符使得模式串能从右往左匹配。（1）</p>
<p>模式串的尾是e，匹配到第一个e，这已经是匹配串的第11个字符。</p>
<p>匹配串 ：Here <strong>is a e</strong>xcellent example</p>
<p>模式串：      <strong>example</strong></p>
<p>这是我们从右往左匹配：</p>
<p>空格与l不匹配,模式串将从不匹配的那个字符开始从右向左寻找匹配串中不匹配的字符的位置，模式串里没有空格，所以移动一个模式串长度的单位</p>
<p>匹配串 ：Here is a e<strong>xcellen</strong>t example</p>
<p>模式串：      <strong>example</strong></p>
<p>不匹配，按（1）操作。</p>
<p>逐次类推。</p>
<p>（完）</p>
<h3 id="5-Sunday算法"><a href="#5-Sunday算法" class="headerlink" title="5.Sunday算法"></a>5.Sunday算法</h3><blockquote>
<p>相关论文：</p>
<p>Daniel M. Sunday, A very fast substring search algorithm, Communications of the ACM, v.33 n.8, p.132-142, Aug. 1990</p>
</blockquote>
<p>Sunday算法和Horspool算法类似，都是尾匹配，不过匹配方式略有不同，不是找匹配串中不匹配的字符在模式串的位置，而是直接找最右边对齐的右一位的那个字符在模式串的位置</p>
<p>例：</p>
<p>我们将在匹配串寻找字符使得模式串能从右往左匹配。</p>
<p>模式串的尾是e，匹配到第一个e，这已经是匹配串的第11个字符。</p>
<p>匹配串 ：Here <strong>is a e</strong>xcellent example</p>
<p>模式串：      <strong>example</strong></p>
<p>空格和l没有匹配上，寻找匹配串右一个字符在模式串的位置。</p>
<p>匹配串 ：Here is a <strong>excelle</strong>nt example</p>
<p>模式串：          <strong>example</strong></p>
<p>继续匹配，发现匹配串右一个字符在模式串里找不到，跳过去</p>
<p>匹配串 ：Here is a excellen<strong>t examp</strong>le</p>
<p>模式串：          <strong>example</strong></p>
<p>继续匹配，匹配串右一个字符为l</p>
<p>匹配串 ：Here is a excellent <strong>example</strong></p>
<p>模式串：          <strong>example</strong></p>
<p>（完）</p>
<h3 id="更高级的串模式匹配算法还有以下几种："><a href="#更高级的串模式匹配算法还有以下几种：" class="headerlink" title="更高级的串模式匹配算法还有以下几种："></a>更高级的串模式匹配算法还有以下几种：</h3><h4 id="6-有限自动机算法"><a href="#6-有限自动机算法" class="headerlink" title="6.有限自动机算法"></a><strong>6.有限自动机算法</strong></h4><h4 id="7-Rabin-Karp算法"><a href="#7-Rabin-Karp算法" class="headerlink" title="7.Rabin-Karp算法"></a>7.Rabin-Karp算法</h4><blockquote>
<p> 相关论文：</p>
<p> Rachard M.Karp and Michael O.Rabin.Efficient randomized pattern-matching algorithms.IBM Journal of Research and Development,31(2):249-260,1987.</p>
</blockquote>
<p>利用哈希值。</p>
<h4 id="8-Galil-Seiferas-算法"><a href="#8-Galil-Seiferas-算法" class="headerlink" title="8.Galil-Seiferas 算法"></a>8.Galil-Seiferas 算法</h4><blockquote>
<p>相关论文：</p>
<p>Zvi Galil and.Joel Seiferas. Time-Space-Optimal String Matching. Journal of Computer And System Sciences, 26(3),:280-294 (1983) </p>
</blockquote>
<p>该算法基于重复因子的字符串匹配。该算法是一个奇妙的线性时间字符串算法，除了匹配串和模式串所需的存储空间外，只需要O(1)的额外存储空间。</p>
<h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p>[1]<a href="https://blog.csdn.net/xiaodu93/article/details/39209421">https://blog.csdn.net/xiaodu93/article/details/39209421</a>.</p>
<p>[2]算法导论（Introduction to Algorithms）.</p>
]]></content>
  </entry>
  <entry>
    <title>我没有为你伤春悲秋不配有憾事</title>
    <url>/2021/20210326/</url>
    <content><![CDATA[<p>听了张敬轩的《春秋》，本想提笔写写自己的感受，奈何在知乎上看到了一个写得非常非常好的评论，有种”眼前有景道不得,崔颢提诗在上头“的感觉。</p>
<span id="more"></span>

<hr>
<p>我其实已经不太记得你了。不记得你跟我说过的第一句话，也不记得我们说过的最后一句话。不记得你的生日，不记得那些曾珍而重之的细节。不记得你跟我熟识起来到底是哪一天，也不记得我们渐渐疏离是什么时候。但我记得我曾经坐在空房间整天整天的想你，我记得花了很长时间为你准备的礼物，我记得你遗失的背包，我记得你课本上的字迹。也记得我们在一起时总是笑，偶尔也会怄气，好长时间不说话。  我其实已经记不清你的样子，却还记得喜欢的感觉。就像我早已经不了解你，却因为曾经的亲密而沾沾自喜。  我把你当成你是我情感世界里最重要的一个。但其实，在世人眼里，你和我的关系恐怕还不如我和我的貌合神离的朋友。哪怕是向你表白也是借着酒意，以如此隐秘的方式，不留一丝痕迹。而你也是这样淡淡的拒绝，甚至扔像最好的朋友一样和我见面，跟我谈天说地。我的心不可能拒绝跟你的会面啊。明明你对我还是那么好，我没有理由逃避你，哪怕我如此爱你，而你只当我是朋友。  爱多数时候是自欺欺人，每个人都像小说家，替自己造梦。现实和梦的反差总要被当成是痛苦的根源。但说真的，能扮弱者玩失意，也是一种运气。至少能用自卑换一点你的关注，换一些两人相处的时光。但时机一过，真的只剩心酸罢了。“ 想心底 留根刺， 至少要见面上万次”，而我与你见过几面？到现在，你有了伴侣，我们或许也将天各一方。可能再见面只能是在梦里了。我这个曾经的“朋友”，凭什么在你心里留下一个坎，让你像留恋自己的情人一样留恋我呢？</p>
<p>你曾经害过我吗？你曾经背叛了我吗？都没有。既然都没有，我又有什么理由发泄自己的悲伤痛苦？我没有为你痛哭流涕过，也没有陪你走过你儿时走的那条小巷，我从未拥有你，也没多少厚重的回忆可以延续我们的故事。</p>
<p>如果你因为我的失意而感到不安，觉得是你害了我，那是我的大错！你找到了命中注定的爱人，我这个“朋友”凭什么有怨恨？我突然觉得即便是你的某某前任也比我幸运得多，至少有名目嫉妒。而我只能像天使一样，快活自在的跟你说祝福，没权让你多一丝负担。 </p>
<p> 时过境迁才感到后悔，原来我与你的联系，连一件实证都找不到。如果非要说的话，那年夏天被剪断的长发其实是为了你。但我不敢任何人说起，就像我不敢跟任何人说起我曾爱你一样。就这样飘渺到甚至连自己都已经不明朗的记忆，这样单薄的记忆，值得写成文字记录吗？ 后人如果来读，大概只觉得是一场杜撰。即便真实存在又如何呢？人生的最后如果要写一部回忆录的话，有多少东西可写啊。初恋的憧憬，新婚的期待，甚至中年的婚外情。而这渺渺两页没有痛点的过场呢？应该会被当做废纸撕去吧。  我们的故事其实是这么短，这么浅。不要说去媲美《春秋》的宏大，在平凡短暂的一生里都算不了是什么值得纪念的片段。那么我的眼泪有意义吗？我的想念有意义吗？其实真的极幼稚，只有孩子才有资格为一颗丢失的糖哭泣吧。我何以有幸为这样微不足道的记忆伤怀呢？我怎么能将自己的失意归咎于你呢？爱若是伟大到像罗密欧与朱丽叶，再怎么痛苦煎熬，也足够让人敞开胸襟轰轰烈烈。但是我对你的这份感情的实质和它看上去的样子其实是没有区别的。于青书史册也好，于你的人生也罢。都是那样轻，轻得不值一提。</p>
]]></content>
  </entry>
  <entry>
    <title>怎么去拯救Ta们？</title>
    <url>/2021/20210313/</url>
    <content><![CDATA[<p>她低头看着湖水，黑暗中仿佛空无一物，只有黑幽幽的颜色，一片巨大的虚无在她脚下铺展开来。没关系的，她告诉自己，然后，她就跨出小船，走进水中。</p>
<p>——《无声告白》</p>
<span id="more"></span>

<p>当初看完了这一段，叹息了许久。莉迪亚最终还是走向了另一种”新生“。父亲作为外来者的自卑和软弱，母亲作为失意者的不甘与歇斯底里，给莉迪亚施加了病态的爱，使莉迪亚陷入了无止境的痛苦。为了对父母的爱，莉迪亚“献祭”了自己，自愿成为牵线木偶，随着内斯的离开，最后的慰藉不复存在，成为了压垮莉迪亚的最后一根稻草，自杀成了莉迪亚离开人世的无声告白。</p>
<p>而现实中济南大学女生自杀更加令人扼腕长叹，太痛心了。在这样的花样年华，人生还很漫长，还有很多精彩，却选择了结束美好的生命。</p>
<p>这位父亲捅妻子一剪刀，把妻子打得鼻子骨折，无疑已经是家庭暴力的行为。《民法典》第一千零四十二条、《刑法》第二百六十条、《中华人民共和国治安管理处罚条例》第二十二条、《中华人民共和国反家庭暴力法》都制定了应对家庭暴力的相关法律法规。显然，这还不够，仅仅是惩戒还不够做到防范于未然。预防也必不可少，《中华人民共和国反家庭暴力法》第六条 提出了</p>
<p>“国家开展家庭美德宣传教育，普及反家庭暴力知识，增强公民反家庭暴力意识。</p>
<p>工会、共产主义青年团、妇女联合会、残疾人联合会应当在各自工作范围内，组织开展家庭美德和反家庭暴力宣传教育。</p>
<p>广播、电视、报刊、网络等应当开展家庭美德和反家庭暴力宣传。</p>
<p>学校、幼儿园应当开展家庭美德和反家庭暴力教育。”</p>
<p>可是是不是宣传做的还不够，从小到大，亲身经历过的反家庭暴力教育有几次？少之又少。</p>
<p>法律的制裁和组织的宣传就足够了吗？能否实施对父母的“义务教育”，达到一定标准才能为人父母？父母是世界上最容易的职业，成为父母不需要考试，也不需要经过孩子的同意。正如《罗生门》中所说：“‘憎恨他犯下的罪，但不憎恨他本人。‘这做起来并不一定很难。大多数孩子对大部分父母都很好地实现了这个格言。”利用父母的身份来情感绑架无疑是最好的控制工具，是许多不幸者因原生家庭走向悲剧的根源。“我觉得我活的像狗一样”道出了这位女生的心声。这种畸形，窒息的”爱“让人坍塌成了深渊，陷入了无边无际的黑暗。这是一种令人无法呼吸的束缚，令人深陷其中逃不出来的桎梏。懂事的孩子想让人开心，自己却没有让自己开心的能力。</p>
<p>这位女生何罪？无罪。无它，出生在了一个悲惨的家庭，碰上了一个糟糕的父亲罢了。出生从来没有选择权，出生是一种不公平，倘若她出生在另一个家庭，是不是她就能避免今日的惨剧呢？有多少人能被爱得“有恃无恐”，能被温柔对待？有多少人能出生在一个温馨氛围的家？在影片《何以为家》中，Zain发出来对父母的控诉：“我要起诉我的父母，因为他们生了我。” “我以为我们能活得体面，能被所有人爱。但上帝不希望我们这样，他宁愿我们做洗碗工。”法律只是对人们最低的道德要求，成为父母需要更高的道德。除了这位自杀的女生，还有多少人证备受煎熬，深陷深渊？</p>
<p>该怎么去拯救他们？该怎么去根绝这种现象？该怎么治愈在原生家庭中承受暴力中受到的创伤？这种家庭模式是不是不甚好？如何在现有家庭模式寻回确实的配位的正常的爱？能否采取社会化抚养的方式？是否采取另一种甚至更激进的方式更适合？该怎么办呢？该怎么办呢？</p>
<hr>
<p>愿逝者安息。</p>
]]></content>
  </entry>
  <entry>
    <title>梯度下降法</title>
    <url>/2021/20210309/</url>
    <content><![CDATA[<p>梯度下降法是一种常见的无约束优化方法,其问题定义为<br>$$<br>min f(x),x\in R^n,f在R^n上连续可微<br>$$</p>
<span id="more"></span>

<p>其以负梯度为搜索方向，这是一种非常自然，正确的选择。</p>
<p>简述步骤如下：</p>
<p>给定初始点$x\in dom f$</p>
<p>重复进行：</p>
<p>1.$\Delta x:=-\nabla f(x)$</p>
<p>2.<strong>直线搜索。</strong>确定步长t（通过精确或回溯直线）。</p>
<blockquote>
<p>通常t取<br>$$<br>t=-\frac{\nabla f(x)^T\Delta x}{(\Delta x)^TA\Delta x}，此处A=\nabla ^2 f(x)(即Hessian矩阵)<br>$$</p>
</blockquote>
<p>3.<strong>修改</strong>。$x:=x+t\Delta x$</p>
<p>直到满足停止准则（大部分情况下，步骤1完成后就检验停止条件，，而不是在修改后才检验。</p>
<ul>
<li>可能会发现梯度下降法和最速下降法步骤相同，但事实上，两者并不完全一样，简单来说采用Euclid 范数的最速下降法就是梯度下降法。</li>
</ul>
<p>更多可见：</p>
<p>3Blue1Brown的《深度学习之梯度下降法》</p>
<p>Stephen Boyd ，Lieven Vandenberghe的《凸优化》</p>
]]></content>
  </entry>
  <entry>
    <title>《他者的消失》读书笔记</title>
    <url>/2021/20210224/</url>
    <content><![CDATA[<p>《他者的消失》由“德国哲学界的一颗新星” 韩炳哲所作，有多篇小短文构成，对“他者”进行了剖析，有些晦涩，但也不乏好句。</p>
<span id="more"></span>

<hr>
<h3 id="摘抄："><a href="#摘抄：" class="headerlink" title="摘抄："></a>摘抄：</h3><h4 id="同质化的恐怖"><a href="#同质化的恐怖" class="headerlink" title="同质化的恐怖"></a>同质化的恐怖</h4><p>如今，感知（die Wahrnehmung）本身呈现出一种“狂看”（Binge Watching）的形式，即“毫无节制的呆视”（Komaglotzen）。它指的是无时间限制地消费视频和电影。人们持续不断地为消费者提供完全符合他们欣赏品位的、讨他们喜欢的电影和连续剧。消费者像牲畜一样，被饲以看似花样翻新实则完全相同的东西。如今社会的感知模式完全可以用这种“毫无节制的呆视”来概括。同质化的扩散不是癌症性质的，而是昏睡性质的。它并未遭遇免疫系统的抵抗。人们就这样呆视着，直至失去意识。</p>
<p>数字化的全联网（Totalvernetzung）和全交际（Totalkommunikation）并未使人们更容易遇见他者。相反，它恰恰更便于人们从陌生者和他者身边经过，无视他们的存在，寻找到同者、志同道合者，从而导致我们的经验视野日渐狭窄。它使我们陷入无尽的自我循环之中，并最终导致我们“被自我想象洗脑”[插图]。</p>
<p>然而如今的网络已变成一个特殊的共振空间，一个回音室，任何不同与陌生都被消除了。真正的共鸣以他者的切近为前提。如今，他者的切近让位于同者的无差别性（Abstandslosigkeit）。全球化交际只允许相同的他者（gleiche Andere）或其他的同者（andere Gleiche）存在。</p>
<p>我们就是被不知名的力量操纵的牵线木偶，没有一丝一毫是我们自己！”</p>
<h4 id="全球化与恐怖主义的暴力"><a href="#全球化与恐怖主义的暴力" class="headerlink" title="全球化与恐怖主义的暴力"></a>全球化与恐怖主义的暴力</h4><p>金钱是一个很糟糕的身份授予者，虽然它能代替身份，让拥有金钱的人至少获得安全感和平静。然而，那些一文不名的人是真的一无所有，既无身份也无安全。因此，没钱的人就只好走进虚幻之境，比如成为民族主义者，这会很快给他一个身份。与此同时，他也为自己创造了一个敌人。人们通过假想来构建免疫力，以获得有意义的身份。挥之不散的恐惧不知不觉地唤醒一种对敌人的渴求。敌人能快速给人以身份，哪怕是幻想中的敌人：“敌人勾勒出我们自身问题的形象，因此我必须与之横眉冷对，以获得自身的尺度、界线和轮廓。”[插图]想象弥补了现实的缺失。就连恐怖主义者也栖身于他们自己的想象之中。全球化让想象空间诞生，想象的空间却带来真实的暴力。</p>
<h4 id="真实性的恐怖"><a href="#真实性的恐怖" class="headerlink" title="真实性的恐怖"></a>真实性的恐怖</h4><p>自残不仅是自我惩罚的仪式，痛恨自己在面对如今功利的、追求完美的社会时和多数人一样力不从心，同时也是对爱的呼唤。</p>
<p>空虚感是抑郁和边缘性人格障碍的基本症状。边缘人通常无法感受到自己的存在。只有在自残的时候他们才终于有所感觉。抑郁的功能主体视自身为沉重负担。他厌倦自己，又沉湎于自己，完全无力从自身当中走出来，这一切都矛盾地导致自身的虚无和空洞。自我封闭、自我关押，失去一切与他者的关联。我触摸自己，却只能通过他者的触摸而感受到自己。他者是塑造稳定自我的根本途径。</p>
<p>自拍瘾（Selfie-Sucht）实际上跟虚荣心关系也不大，它无非就是孤独、自恋的自我在瞎忙。面对内心的空虚，人们徒劳地尝试着卖弄自己，博人眼球。唯有空虚在自我复制。自拍照是自身的空虚形态。自拍瘾加剧了空虚感。导致这一结果的不是虚荣心，而是自恋的自我关涉。自拍照是自身的美丽平面，而这自身空洞、不安。为了逃避空虚感的折磨，人们要么拿起刀片，要么拿起智能手机。自拍照是让空虚的自我短暂退隐的扁平表面。倘若把照片翻过来，人们就会撞见那伤痕累累的背面，汩汩流着血。自拍照的背后是伤口。</p>
<h4 id="恐惧"><a href="#恐惧" class="headerlink" title="恐惧"></a>恐惧</h4><p>[插图]。今天，我们都竭力逃离否定者，从不在其身边栖居。黏在肯定者身畔只能复制更多的同者。</p>
<p>自我对他人亦步亦趋，当自我觉得无法跟上他人步伐之时，便会慌乱无措。……他人如何看待我，他人觉得我又是如何看待他们，这些想象成为社会恐惧的来源。使个人不胜其烦、心力交瘁的并非客观的情况，而是感受到在与多数派的他者相比较时所产生的竞争。</p>
<p>如今，很多人饱受诸多恐惧的折磨，害怕拒绝，害怕失败，害怕被落下，害怕犯错误或者做错决定，害怕达不到自己的要求。在不停与他人比较的过程中，恐惧也不断加剧。这是与纵向恐惧背道而驰的横向恐惧。前者是面向全然他者、面向茫然失所和面向“无”之时才生出的恐惧。</p>
<h4 id="门槛"><a href="#门槛" class="headerlink" title="门槛"></a>门槛</h4><p>作为新兴生产方式，数字化交际彻底打破所有距离，以加速自身运行，所有保护性的距离也就此消失了。在超交际中万事万物皆熔于一炉，内在和外在之间的壁垒也愈发脆弱。如今，我们完全被外化为一个“纯粹的平面”[插图]，暴露在无孔不入的网络中。这种强制性的透明化克服了一切视觉与信息缺口，世间万物清晰可见。它不给人任何退路，令所有安全空间消失不见。万事万物汹涌而来，而我们却无遮无挡，亦无处可藏。我们本身也只是全球网络中的通道而已。透明化和超交际夺走了保护着我们的内心世界。是的，我们是自愿放弃了内心世界，甘于受数字化网络的奴役，任由它们穿透、照透、刺透我们。数字化的过度曝光与毫无遮掩带来一种潜在的恐惧，它并非源于他者的否定性，而是源于过多的肯定性。同质化的透明地狱并没有驱离恐惧，令人胆寒的正是同质化愈发震耳欲聋的轰鸣。</p>
<h4 id="异化"><a href="#异化" class="headerlink" title="异化"></a>异化</h4><p>如今我们生活在后马克思主义时代。在新自由主义的政制下，剥削不再以异化和自我现实化剥夺的面貌出现，而是披上了自由、自我实现和自我完善的外衣。这里并没有强迫我劳动、使我发生异化的剥削者。相反，我心甘情愿地剥削着我自己，还天真地以为是在自我实现。这是新自由主义的奸险逻辑。所以，过劳症（Burn-out）的初期表现恰恰是亢奋。干劲十足地投身于劳动之中，直至精疲力竭为止。自我实现，实现至死。自我完善，完善而亡。新自由主义的统治藏身于幻想中的自由背后。它与自由携手并立于我们面前之际，正是它大功告成之时。这种感觉上的自由消弭了任何反抗、革命的可能性，这才是它的致命之处。有什么可反抗的呢？已经没有人再压迫你了啊！珍妮·霍尔泽所说的“保护我免受我所欲之害”将这一范式转变表达得十分贴切。</p>
<h4 id="目光"><a href="#目光" class="headerlink" title="目光"></a>目光</h4><p>由于缺乏起到镇压作用的目光（这与纪律社会的监督策略有着本质区别），便产生了一种具有欺骗性的自由感。数字化全景监狱里的犯人并未觉得被凝视，也就是并未觉得被监控。因此，他们感到很自由，且自愿地去暴露自己。数字化全景监狱并非限制了自由，而是将其极尽利用。</p>
<h4 id="他者之语言"><a href="#他者之语言" class="headerlink" title="他者之语言"></a>他者之语言</h4><p>如今，我们为了成为被关注的焦点而无所不用其极。为了博取关注，我们彼此都成了橱窗。</p>
<h4 id="倾听"><a href="#倾听" class="headerlink" title="倾听"></a>倾听</h4><p>如今，我们越来越丧失倾听的能力。妨碍倾听的罪魁祸首便是日渐严重的自我聚焦，是社会的自恋化倾向。</p>
<p>我不必求助于某个“个人”对象，从网络上就能找到信息。我也不必前往公共领域去获取信息或购买商品，而是让它们被送上门来。数字化交际将我联入网中，但同时也使我孤立于他人。它虽然消灭了距离，然而，“无距离”却产生不了人与人的切近。</p>
<p>人们在脸书（Facebook）上不会提及关乎我们大家的、可以讨论的问题，而主要是发布一些不需要讨论的、只为凸显发布者形象的广告。在那里，人们不会想到，他者可能有着烦恼和痛苦。在“点赞”的共同体中人们只会遇到自己，或者和自己相同的人。那里也不可能形成讨论。</p>
]]></content>
  </entry>
  <entry>
    <title>《概率导论》补充答案</title>
    <url>/2021/20210122(1~5)/</url>
    <content><![CDATA[<p>《概率导论》第二版答案补充，书上已给解答的不再赘述。</p>
<span id="more"></span>

<p>部分来自：</p>
<p> <a href="https://github.com/NeilKleistGao/answer-to-introduction-to-probability">answer-to-introduction-to-probability</a> （NeilKleistGao/雪岛/HumphreyYang）</p>
<p><a href="http://athenasc.com/prob-solved_2ndedition.pdf">http://athenasc.com/</a> （Athena Scientific, Belmont, Massachusetts）-</p>
<hr>
<h3 id="第一章-样本空间与概率"><a href="#第一章-样本空间与概率" class="headerlink" title="第一章 样本空间与概率"></a>第一章 样本空间与概率</h3><h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><p>令事件A为掷出偶数，即$A = {2, 4, 6}$</p>
<p>令B表示点数大于3的事件，即$B = {4, 5, 6}$</p>
<p>那么：</p>
<p>$A^c = {1, 3, 5}, B^c = {1, 2, 3}$</p>
<p>$A \cup B = {2, 4, 5, 6}, A \cap B = {4, 6}$</p>
<p>所以$(A \cup B)^c = {1, 3} = A^c \cap B^c$</p>
<p>$(A \cap B)^c = {1, 2, 3, 5} = A^c \cup B^c$</p>
<h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><h5 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h5><p>设全集为S，由代数性质：</p>
<p>$\begin{array}{lcr}<br>                A^c = \<br>            A^c \cap S = \<br>            A^c \cap (B \cup B^c) = \<br>            (A^c \cap B) \cup (A^c \cap B^c)<br>            \end{array}$</p>
<p>对$B^c$的结论同理，证毕。</p>
<p>也可以通过韦恩图进行证明：任意一个集合中的元素，要么属于$B$，要么属于$B^c$。上式就是全概率定理的特殊情况。</p>
<h5 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h5><p>设全集为S，由德摩根律：</p>
<p>$\begin{array}{lcr}<br>                (A \cap B)^c = \<br>                A^c \cup B^c = \<br>                (A^c \cap S) \cup (B^c \cap S) = \<br>                (A^c \cap B) \cup  (A^c \cap B^c) \cup (A^c \cap B^c) \cup (A \cap B^c) = \<br>                (A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c)<br>            \end{array}$</p>
<p>证毕。</p>
<p>也可以通过韦恩图进行证明：A交B的补集中，只有三种可能：属于A但不属于B、属于B但不属于A、既不属于A也不属于B，整理即可得到上式。</p>
<h5 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h5><p>令事件A为掷出偶数，即$A = {2, 4, 6}$</p>
<p>令B表示点数大于3的事件，即$B = {1, 2, 3}$</p>
<p>则$(A \cap B)^c = {1, 3, 4, 5, 6}$</p>
<p>$(A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c) = {1, 3} \cup {5} \cup {4, 6} = {1, 3, 4, 5, 6}$</p>
<p>所以$(A \cap B)^c = (A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c)$。</p>
<h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><p>略。</p>
<h4 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h4><p>证明略，这里给出一个实例。假设[0, 1]区间中的数可数，那么我们可以列出：<br>$$\begin{array}{rcl}<br>            x_1 &amp;= 0.3154… \<br>            x_2 &amp;= 0.114514… \<br>            x_3 &amp;= 0.2333… \<br>            …<br>        \end{array}$$<br>我们可以找到一个y，使得$y = 0.121…$。由于y的第n位与$x_n$不同，于是这个y不可能在上面的数中。</p>
<p>当然找到y的方式并不止书上一种。</p>
<h4 id="5"><a href="#5" class="headerlink" title="5."></a>5.</h4><p>设挑选一个学生，这个学生是天才为事件A，这个学生喜欢巧克力为事件B，那么：</p>
<p>$P(A) = 0.6, P(B) = 0.7, P(A \cap B) = 0.4$</p>
<p>所以$P(\bar A \cap \bar B) = 1 - P(A) - P(B) + P(A \cap B) = 0.1$。</p>
<p>这题是12题容斥原理的应用。</p>
<h4 id="6"><a href="#6" class="headerlink" title="6."></a>6.</h4><p>因为偶数的概率是奇数的两倍，又因为$P(even) + P(odd) = 1$，所以$P(even) = \frac{2}{3}, P(odd) = \frac{1}{3}$。</p>
<p>因为不同偶数/奇数面出现的概率相同，于是：</p>
<p>$$\begin{array}{rcl}<br>            P(1) &amp;= \frac{1}{9} \<br>            P(2) &amp;= \frac{2}{9} \<br>            P(3) &amp;= \frac{1}{9} \<br>            P(4) &amp;= \frac{2}{9} \<br>            P(5) &amp;= \frac{1}{9} \<br>            P(6) &amp;= \frac{2}{9}<br>        \end{array}$$</p>
<p>所以点数小于4的概率为$P(1) + P(2) + P(3) = \frac{4}{9}$。</p>
<h4 id="7"><a href="#7" class="headerlink" title="7."></a>7.</h4><p>样本空间为：第一次时停止，第二次时停止，第三次时停止……</p>
<h4 id="8"><a href="#8" class="headerlink" title="8."></a>8.</h4><p>不妨设第n局获胜的概率为$P_n$，那么最终获胜的概率：</p>
<p>$P = P_1(1 - P_2)P_3 + P_1P_2 + (1 - P_1)P_2P_3$</p>
<p>化简上式得到$P = P_1P_3 + P_2(P_1 + P_3 - 2P_1P_3)$。</p>
<p>对于函数$f(x, y) = x + y - 2xy$在x和y都属于[0,<br>1]区间内时，$x \geq x^2, y \geq y^2, (x - y)^2 \geq 0$，所以$x + y - 2xy \geq x^2 + y^2 -2xy = (x - y)^2 \geq 0$</p>
<p>所以对于$(P_1 + P_3 - 2P_1P_3)$，这个值永远非负，故$P_2$最大时整个式子最大，且与$P_1,P_3$的顺序无关。</p>
<p>证毕。</p>
<h4 id="9"><a href="#9" class="headerlink" title="9."></a>9.</h4><h5 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h5><p>$P(A) = P(A \cap \Omega) = P(A \cap \bigcup_{i = 1}^nS_i) = P(\bigcup_{i = 1}^n(A \cap S_i))$</p>
<p>由于${S_1, S_2, …, S_n}$互不相容，故$P(A) = \sum_{i = 1}^nP(A \cap S_i)$</p>
<p>证毕。</p>
<h5 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h5><p>不难发现，$B^c \cap C^c, B \cap C^c, B^c \cap C, B \cap C$互不相容</p>
<p>所以$P(A) = P(A \cap B^c \cap C^c) + P(A \cap B \cap C^c) + P(A \cap B^c \cap C) + P(A \cap B \cap C)$</p>
<p>由容斥原理，$P(A) = P(A \cap B^c \cap C^c) + P(A \cap B) + P(A \cap C) - P(A \cap B \cap C)$</p>
<p>证毕。</p>
<h4 id="10"><a href="#10" class="headerlink" title="10."></a>10.</h4><p>$P(A) + P(B) - 2P(A \cap B) = P(A \cup B) - P(A \cap B) = P((A \cap B^c) \cup (A^c \cap B))$</p>
<p>证毕。</p>
<h4 id="11"><a href="#11" class="headerlink" title="11."></a>11.</h4><h5 id="a-2"><a href="#a-2" class="headerlink" title="(a)"></a>(a)</h5><p>原文给出的等式应为$P(A \cup B) = P(A) + P(B) - P(A \cap B)$，似乎存在笔误。</p>
<h5 id="b-2"><a href="#b-2" class="headerlink" title="(b)"></a>(b)</h5><p>略。</p>
<h4 id="12"><a href="#12" class="headerlink" title="12."></a>12.</h4><h5 id="a-3"><a href="#a-3" class="headerlink" title="(a)"></a>(a)</h5><p>略。</p>
<h5 id="b-3"><a href="#b-3" class="headerlink" title="(b)"></a>(b)</h5><p>当n = 3时，由(a)可知等式成立。</p>
<p>假定n = m时等式依然成立，那么当n = m + 1时：</p>
<p>$P(\bigcup_{k = 1}^{m + 1}) = P(A_{m + 1} \cup \bigcup_{k = 1}^m A_k) =<br>            P(A_{m + 1}) + \sum_{i \in S_1}P(A_i)<br>            - \sum_{(i_1, i_2) \in S_2}P(A_{i_1} \cap A_{i_2}) + … + (-1)^{n - 1}P(\bigcap_{k = 1}^{n}A_k) -P(A_{m + 1} \cap \bigcup_{k = 1}^m A_k)$</p>
<p>整理上式可得：</p>
<p>$P(\bigcup_{k = 1}^{m + 1}) = \sum_{i \in S_1}P(A_i) - \sum_{(i_1, i_2) \in S_2}P(A_{i_1} \cap A_{i_2}) + … + (-1)^nP(\bigcap_{k = 1}^{m + 1}A_k)$</p>
<p>归纳完毕。</p>
<h4 id="13"><a href="#13" class="headerlink" title="13."></a>13.</h4><p>略。</p>
<h4 id="14"><a href="#14" class="headerlink" title="14."></a>14.</h4><h5 id="a-4"><a href="#a-4" class="headerlink" title="(a)"></a>(a)</h5><p>仅有(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6,<br>6)满足情况，故$P = \frac{1}{6}$</p>
<h5 id="b-4"><a href="#b-4" class="headerlink" title="(b)"></a>(b)</h5><p>若总和小于等于4，则有(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3,<br>1)六种情况，其中一对的事件有两个，故$P = \frac{1}{3}$。</p>
<h5 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h5><p>设两个骰子都是6为事件A，恰有一个骰子为6为事件B，则$P(A) + P(B) = \frac{1}{36} + \frac{10}{36} = \frac{11}{36}$即为所求。</p>
<h5 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h5><p>不同的抛掷结果一共有30种，而至少一个骰子为6（也只能有一个骰子为6）有10种情况，所以$P = \frac{1}{3}$</p>
<h4 id="15"><a href="#15" class="headerlink" title="15."></a>15.</h4><p>在已知第一次为正面的情况下，两次均正面朝上的概率只取决于第二次抛掷的情况，所以此时的概率为$\frac{1}{2}$。</p>
<p>而两次中至少有一次正面朝上的概率为$\frac{3}{4}$，在已知该前提下，两次正面的概率为$\frac{1}{3}$，所以爱丽丝的结论是正确的。</p>
<p>假定硬币不对称，即$p \ne 1 - p$，那么第一种情况的概率变为$p$；而第二种情况下至少一次正面朝上的概率变为了$1 - (1 - p)^2 = 2p - p^2$，<br>在该前提下两次正面朝上的概率就应该是$\frac{p}{2 - p}$。因为$p \leq 1$，所以仅当$p = 1$或$p = 0$，即正面始终朝上或朝下时，两个条件概率相等，<br>其他条件下，前者的概率都要比后者大。</p>
<p>可以将这个结论推广到任意n$(n \geq 2)$次游戏：将前n -<br>1次游戏抛得正面条件下得到n次正面的概率与至少n-1次得到正面条件下的概率相差，<br>得到$\frac{(n - 1)p + (1 - n)p^2}{p + n - np}$，由于$p \geq p^2$，可以得到和上面一样的结论。</p>
<h4 id="16"><a href="#16" class="headerlink" title="16."></a>16.</h4><p>所有抛掷可以得到正面的概率为$\frac{1}{3} + \frac{1}{6} = \frac{1}{2}$，而选中正常硬币并抛掷得到正面的概率为$\frac{1}{3} \times \frac{1}{2} = \frac{1}{6}$，<br>于是由条件概率可得$\frac{\frac{1}{6}}{\frac{1}{2}} = \frac{1}{3}$。</p>
<p>也可以考虑所有抛掷的结果：一共是三正三反，而三个正面朝上的情况中只有其中一个是来自于正常硬币，结果也是$\frac{1}{3}$。</p>
<h4 id="17"><a href="#17" class="headerlink" title="17."></a>17.</h4><p>设这批产品被接受为事件A，则$P(A) = \frac{C_{96}^{5}}{C_{100}^5}$，那么被拒绝的概率就为$1 - P(A) \approx 0.19$</p>
<h4 id="18"><a href="#18" class="headerlink" title="18."></a>18.</h4><p>$P(A \cap B|B) = \frac{P(A \cap B \cap B)}{P(B)} = \frac{P(A \cap B)}{P(B)} = P(A|B)$</p>
<p>证毕。</p>
<h4 id="19"><a href="#19" class="headerlink" title="19."></a>19.</h4><p>若$j \ne i$，则在第i个抽屉内找但没有找到的概率为$1 - p_id_i$（要么不在这个抽屉，要么在但是没有找到），而报告在第j个抽屉的概率为$p_j$，<br>故结果为$\frac{p_j}{1 - p_id_i}$。</p>
<p>若$j = i$，在第i个抽屉内找但没有找到的概率同上，而报告恰好就在第i个抽屉并且没有被找到的概率为$P_i(1 - d_i)$，<br>故结果为$\frac{p_i(1 - d_i)}{1 - p_id_i}$</p>
<p>证毕。</p>
<h4 id="20"><a href="#20" class="headerlink" title="20."></a>20.</h4><h5 id="a-5"><a href="#a-5" class="headerlink" title="(a)"></a>(a)</h5><h5 id="i"><a href="#i" class="headerlink" title="(i)"></a>(i)</h5><p>进攻风格下，和局的概率为0，所以我们只需要考虑两种情况：<br>前两局鲍里斯均获胜，前两局平局，第三局鲍里斯获胜。</p>
<p>所以获胜概率为：$p_w^2 + p_w^2(1 - p_w)$。</p>
<h5 id="ii"><a href="#ii" class="headerlink" title="(ii)"></a>(ii)</h5><p>保守风格下，获胜的概率为0，所以获胜情况只有一种：<br>前两局和局，第三局获胜。</p>
<p>所以获胜概率为：$p_d^2p_w$。</p>
<h5 id="iii"><a href="#iii" class="headerlink" title="(iii)"></a>(iii)</h5><p>由全概率定理，整理出所有获胜情况：</p>
<ul>
<li><p>  第一局获胜，第二局平局</p>
</li>
<li><p>  第一局获胜，第二局失败，第三局获胜</p>
</li>
<li><p>  第一局失败，第二局获胜，第三局获胜</p>
</li>
</ul>
<p>所以获胜概率为：$p_wp_d + p_w^2(1 - p_d) + p_w^2(1 - p_w)$</p>
<h5 id="b-5"><a href="#b-5" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)(iii)可知，策略三的获胜概率为$p_w(p_d + 2p_w - p_w^2 - p_wp_d)$。</p>
<p>不难发现若$p_w = 0.49$时，$\exists p_d \approx 0.56$，使得上式可以大于0.5。</p>
<h4 id="21"><a href="#21" class="headerlink" title="21."></a>21.</h4><ul>
<li><p>  若在第一回合获胜，概率为$\frac{m}{m + n}$</p>
</li>
<li><p>  若在第二回合获胜，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{m}{m + n - 2}$</p>
</li>
<li><p>  若在第三回合获胜，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{n - 2}{m + n - 2}\frac{n - 3}{m + n - 3}\frac{m}{m + n - 4}$</p>
</li>
<li><p>  ...</p>
</li>
</ul>
<p>从上面的式子并不容易看出递推式来，但如果稍作一些改动：</p>
<ul>
<li><p>  若第一回合前无法决出胜负，概率为$1$</p>
</li>
<li><p>  若第二回合前无法决出胜负，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}$</p>
</li>
<li><p>  若第三回合前无法决出胜负，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{n - 2}{m + n - 2}\frac{n - 3}{m + n - 3}$</p>
</li>
<li><p>  ...</p>
</li>
</ul>
<p>设第i回合前无法决出胜负的概率为$p_i$，那么我们可以得到$p_1 = 1$，递推式$p_i = p_{i - 1}\frac{n - 2i + 4}{m + n - 2i + 4}\frac{n - 2i + 3}{m + n -2i + 3}$</p>
<p>所以获胜概率为：$\sum_{i = 1}^{1 + \lfloor \frac{n}{2} \rfloor}p_i \frac{m}{m + n - 2i + 2}$</p>
<h4 id="22"><a href="#22" class="headerlink" title="22."></a>22.</h4><p>当$k = 1$时，取出白球的概率为$\frac{m}{m + n}$显然成立。</p>
<p>不妨假设当$k = p$时，在第k个罐子里取出白球的概率依然是$\frac{m}{m + n}$，<br>则对于$k = p + 1$时，由全概率定理， 在第k个罐子里取出白球的概率为</p>
<p>$\frac{m}{m + n}\frac{m + 1}{m + n + 1} + \frac{n}{m + n}\frac{n}{m + n + 1}$</p>
<p>整理后得到$\frac{m}{m + n}$，归纳完毕。</p>
<p>证毕。</p>
<h4 id="23"><a href="#23" class="headerlink" title="23."></a>23.</h4><p>不妨设罐子里的球的个数为n，分两种情况讨论：</p>
<ul>
<li><p>  第一次交换两个球，第二次再交换回来，第三第四次亦如此，这样做的概率为$(\frac{1}{n})^4$</p>
</li>
<li><p>  第一次第二次一共交换了两组球，第三第四次再交换回来，这样做的概率为$\frac{4(n - 1)^2}{n^6}$</p>
</li>
</ul>
<p>总的概率为二者之和。</p>
<h4 id="24"><a href="#24" class="headerlink" title="24."></a>24.</h4><p>$\frac{2}{3}$是先验概率，而$\frac{1}{2}$是后验概率。</p>
<h4 id="25"><a href="#25" class="headerlink" title="25."></a>25.</h4><p>不妨设两个信封内的钱数分别为S和L，且$L &gt; S$。</p>
<p>对于S，选中的概率为$\frac{1}{2}$，且只有抛硬币的次数不少于S次时才会换信封。<br>若第一次选中了该信封，则最后得到的钱数为L的概率为</p>
<p>$\frac{1}{2}(1 - \sum_{i = 1}^{S - 1}(\frac{1}{2})^i) = (\frac{1}{2})^S$</p>
<p>对于L，选中的概率也是$\frac{1}{2}$，且只有抛硬币的次数小于L次时才会选择不换信封。<br>若第一次选中了该信封，则最后得到的钱数为L的概率为</p>
<p>$\frac{1}{2}\sum_{i = 1}^{L - 1}(\frac{1}{2})^i = \frac{1}{2} - (\frac{1}{2})^L$</p>
<p>所以最后的概率为$\frac{1}{2} - (\frac{1}{2})^L + (\frac{1}{2})^S$。因为$L &gt; S$，所以这个概率大于$\frac{1}{2}$。</p>
<h4 id="26"><a href="#26" class="headerlink" title="26."></a>26.</h4><h5 id="a-6"><a href="#a-6" class="headerlink" title="(a)"></a>(a)</h5><p>由于A和B独立，由条件概率$P(A|B) = \frac{p(1 - q)}{1 -q} = p$。</p>
<h5 id="b-6"><a href="#b-6" class="headerlink" title="(b)"></a>(b)</h5><p>$P(C) = q(p + \frac{1 - p}{2}) = \frac{q(1 + p)}{2}$</p>
<p>由贝叶斯准则：$P(A|C) = \frac{P(A)P(C|A)}{P(C)} = \frac{2p}{p + 1}$</p>
<h4 id="27"><a href="#27" class="headerlink" title="27."></a>27.</h4><ul>
<li><p>  当鲍勃的所有硬币均正面朝上时，爱丽丝无论怎么抛都是失败，一共有$2^n$种情况</p>
</li>
<li><p>  当鲍勃有一枚硬币朝下时，对于鲍勃来说有$C^1_{n+1}$种可能，而对于爱丽丝来说，除了全正以外都是鲍勃获胜，故有$2^n - C^0_n$种情况</p>
</li>
<li><p>  当鲍勃有两枚硬币朝下时，对于鲍勃来说有$C^2_{n+1}$种可能，对于爱丽丝来说，有$2^n - C^0_n - C^1_n$种情况</p>
</li>
<li><p>  …</p>
</li>
</ul>
<p>综上，所有鲍勃获胜的情况的可能数一共是<br>$2^n + C_{n + 1}^1(2^n - C_n^0) + C_{n + 1}^2(2^n - C_n^0 - C_n^1) + \dots + C_{n + 1}^n(2^n - C_n^0 - \dots - C_n^{n - 1})$</p>
<p>上式化简后得到$2^{2n + 1} - 2^n - \sum_{i = 1}^nC_{n + 1}^i(\sum_{j = 0}^{i - 1}C_n^j)$。</p>
<p>注意到$2^{2n + 1} = 2^{n + 1} \times 2^n = (\sum_{i = 0}^{n + 1}C_{n + 1}^i)(\sum_{i = 0}^{n}C_n^i)$</p>
<p>所以$(\sum_{i = 0}^{n + 1}C_{n + 1}^i)(\sum_{i = 1}^{n}C_n^i) = 2^{2n + 1} - 2^{n + 1}$</p>
<p>因为$\sum_{i = 1}^{n}C_{n + 1}^i(\sum_{j = 0}^{i - 1}C_n^j) = \sum_{i = 1}^{n}C_{n + 1}^i(\sum_{j = i}^{n}C_n^j)$</p>
<p>所以<br>$2\sum_{i = 1}^{n}C_{n + 1}^i(\sum_{j = 0}^{i - 1}C_n^j) = \sum_{i = 1}^{n}C_{n + 1}^i(\sum_{j = 0}^{i - 1}C_n^j) + \sum_{i = 1}^{n}C_{n + 1}^i(\sum_{j = i}^{n}C_n^j)$</p>
<p>上式就等于$(\sum_{i = 1}^{n}C_{n + 1}^i)(\sum_{i = 0}^{n}C_n^i)$，即$2^{2n + 1} - 2^{n + 1}$</p>
<p>带回原式可知鲍勃获胜的情况总共有$2^{2n}$种，所有情况一共有$2^{2n + 1}$种，故获胜的概率为$\frac{1}{2}$。</p>
<p>证毕。</p>
<h4 id="28"><a href="#28" class="headerlink" title="28."></a>28.</h4><p>略。</p>
<h4 id="29"><a href="#29" class="headerlink" title="29."></a>29.</h4><p>略。</p>
<h4 id="30"><a href="#30" class="headerlink" title="30."></a>30.</h4><p>如果两头猎犬均选择了正确方向，概率为$p^2$；<br>如果只有一头选择了正确方向，概率为$2p(1 - p)$，此时随机选择到正确方向的概率是$p(1 - p)$，<br>故在这个策略下选择正确方向的概率为$p^2 + p - p^2 = p$，并不会比只让一条猎犬选择更优。</p>
<h4 id="31"><a href="#31" class="headerlink" title="31."></a>31.</h4><h5 id="a-7"><a href="#a-7" class="headerlink" title="(a)"></a>(a)</h5><p>由于不同信号之间独立，第k个信号正确传输的概率为$p(1 - \epsilon_0) + (1 - p)(1 - \epsilon_1)$</p>
<h5 id="b-7"><a href="#b-7" class="headerlink" title="(b)"></a>(b)</h5><p>1011一共含有三个1和一个0，且彼此独立，所以正确传输的概率为$(1 - \epsilon_0)(1 - \epsilon_1)^3$</p>
<h5 id="c-2"><a href="#c-2" class="headerlink" title="(c)"></a>(c)</h5><p>只有以下的情况数据可以被正常传输：</p>
<ul>
<li><p>  3个0均没有错误，概率为$(1 - \epsilon_0)^3$</p>
</li>
<li><p>  只有一个0发生错误，概率为$C^1_3\epsilon_0(1 - \epsilon_0)^2$</p>
</li>
</ul>
<p>综上，0被正确传输的概率为$(1 - \epsilon_0)^3 + 3\epsilon_0(1 - \epsilon_0)^2$</p>
<h5 id="d-1"><a href="#d-1" class="headerlink" title="(d)"></a>(d)</h5><p>将(c)中的式子变形为$1^3 - 3\epsilon_0^2(1 - \epsilon_0) - \epsilon_0^3 = 1 - 3\epsilon_0^2 + 2\epsilon_0^3$</p>
<p>对上式求导得到$-6\epsilon_0 + 6\epsilon_0^2$,由于$0 \leq \epsilon_0 \leq 1$，所以函数在$\epsilon_0 = 0$时取到最大值。<br>即：整个传输过程完全不要出错是坠吼的。</p>
<h5 id="e"><a href="#e" class="headerlink" title="(e)"></a>(e)</h5><p>由贝叶斯准则，对方发出的数据为0的概率为$\frac{P(0)}{P(1) + P(0)} = \frac{\epsilon_0^2}{\epsilon_1 + \epsilon_0^2}$</p>
<h4 id="32"><a href="#32" class="headerlink" title="32."></a>32.</h4><ul>
<li><p>  因为各次生育是独立的，所以国王的性别并不会影响他的兄弟姐妹，而生男的概率为$\frac{1}{2}$，故国王有一个兄弟的概率为$\frac{1}{2}$</p>
</li>
<li><p>  国王的母亲一共有两个孩子，并且两个都是男性，所以国王有一个兄弟的概率为$\frac{1}{4}$</p>
</li>
</ul>
<h4 id="33"><a href="#33" class="headerlink" title="33."></a>33.</h4><p>投一次硬币已经没办法解决这个问题了，那我们考虑投两次。投两次硬币的全部概率为$1^2 = (1 - p + p)^2 = (1 - p)^2 + 2p(1 - p) + p^2$。注意到里面有一个2，<br>那么我们不妨投两次硬币，如果两次结果相同，则重新开始；剩余的两种情况每个人获胜的概率均为$\frac{1}{2}$。</p>
<h4 id="34"><a href="#34" class="headerlink" title="34."></a>34.</h4><ul>
<li><p>  第一个子系统有效的概率为$p$</p>
</li>
<li><p>  第二个子系统有效的概率为$p + 3p^2 - 5P^3 + 2p^4$</p>
</li>
<li><p>  第三个子系统有效的概率为$2p - p^2$</p>
</li>
</ul>
<p>由于三个子系统独立，整个系统的有效概率为$p(p + 3p^2 - 5p^3 + 2p^4)(2p - p^2) = 2p^3 + 5p^4 -13p^5 + 9p^6 -2p^7$</p>
<h4 id="35"><a href="#35" class="headerlink" title="35."></a>35.</h4><p>由独立性： $\sum_{i = k}^nC^i_np^i(1 - p)^{n - i}$</p>
<h4 id="36"><a href="#36" class="headerlink" title="36."></a>36.</h4><h5 id="a-8"><a href="#a-8" class="headerlink" title="(a)"></a>(a)</h5><p>只有所有电厂均中断的时候全市才会停电，故由独立性可得：<br>$\Pi_{i = 1}^np_i$</p>
<h5 id="b-8"><a href="#b-8" class="headerlink" title="(b)"></a>(b)</h5><p>如果所有电厂中断，或者只有不到三个电厂在供电，全市会处在停电状态。<br>设$P(i), P(i, j)$分别为仅第i个电厂工作的概率和仅第i，j个电厂在工作的概率，<br>即$P(i) = \frac{(1 - p_i)\Pi_{j = 1}^np_j}{p_i}, P(i, j) = \frac{(1 - p_i)(1 - p_j)\Pi_{k = 1}^np_k}{p_ip_j}$，<br>全市停电的概率为$\Pi_{i = 1}^n + \sum_{i = 1}^nP(i) + \sum_{1 \leq i &lt; j \leq n}P(i, j)$。</p>
<h4 id="37"><a href="#37" class="headerlink" title="37."></a>37.</h4><p>$\sum_{i = 0}^{n_1}\sum_{j = 0}^{n_2}\left[r_1i + r_2j &gt; c\right]C_{n_1}^ip_1^i(1 - p_1)^{n_1 - i}C_{n_2}^jp_2^j(1 - p_2)^{n_2 - j}$</p>
<h4 id="38"><a href="#38" class="headerlink" title="38."></a>38.</h4><p>考虑泰里思领先的概率$p_T$：只有在剩下的比赛中得到至少6分，才能保证泰里思领先。<br>所以$p_T = \sum_{i = 6}^8C_8^ip^i(1 - p)^{8 - i}$。</p>
<p>同理对于温迪来说，$P_W = \sum_{i = 4}^8C_8^i(1 - p)^ip^{8 - i} = \sum_{i = 0}^4C_8^ip^i(1 - p)^{8 - i}$。</p>
<p>所以泰里思可以分到的钱为$\frac{10p_T}{p_T + p_W} = \frac{10p_T}{1 - p^5(1 - p)^3}$</p>
<h4 id="39"><a href="#39" class="headerlink" title="39."></a>39.</h4><p>设这天是好天气的概率为$p_w$，若当天每个学生的出勤概率为$t$，<br>则当天出勤人数不少于k的概率$P_k(t) = \sum_{i = k}^nC_n^it^i(1 - t)^{n - i}$，<br>那么教授可以讲课的概率为$p_wP_k(p_g) + (1 - p_w)P_k(p_b)$。</p>
<h4 id="40"><a href="#40" class="headerlink" title="40."></a>40.</h4><p>当$n = 0$时，$p_n = 1$。</p>
<p>当$n &gt; 0$时，$p_n = p(1 - p_{n - 1}) + p_{n - 1}(1 - p)$。</p>
<p>所以$p_n = p_{n - 1}(1 - 2p) + p$。令$p_n’ = p_n - \frac{1}{2}$，则$p_n’ = (1 - 2p)p_{n - 1}’$，<br>由等比数列通项公式：$p_n’ = \frac{(1 - 2p)^n}{2}$，所以$p_n = \frac{1 + (1 - 2p)^n}{2}$。</p>
<h4 id="41"><a href="#41" class="headerlink" title="41."></a>41.</h4><p>假设第一个人转出来的数字为$p$，则在某一回合内，这个人被淘汰的概率为$p$。<br>所以第一个人在第n个回合被淘汰的概率等于前n-1回合没被淘汰的概率乘上这个回合被淘汰的概率，<br>即$P(N = n) = (1 - p)^{n - 1}p$。</p>
<h4 id="42"><a href="#42" class="headerlink" title="42."></a>42.</h4><p>略。这个问题在第七章马尔可夫链中还会再遇到。</p>
<h4 id="43"><a href="#43" class="headerlink" title="43."></a>43.</h4><p>略。</p>
<h4 id="44"><a href="#44" class="headerlink" title="44."></a>44.</h4><h5 id="a-9"><a href="#a-9" class="headerlink" title="(a)"></a>(a)</h5><p>略。</p>
<h5 id="b-9"><a href="#b-9" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)可知：若A和B独立，则$A$和$B^c$独立，<br>即$P(A \cap B^c) = P(A)P(B^c)$。</p>
<p>对于事件$B^c$，由全概率公式：<br>$P(B^c) = P(A \cap B^c) + P(A^c \cap B^c) = P(A)P(B^c) + P(A^c \cap B^c)$，<br>所以$P(A^c \cap B^c) = (1 - P(A))P(B^c) = P(A^c)P(B^c)$，所以$A^c$和$B^c$独立。</p>
<p>证毕。</p>
<h4 id="45"><a href="#45" class="headerlink" title="45."></a>45.</h4><p>略。</p>
<h4 id="46"><a href="#46" class="headerlink" title="46."></a>46.</h4><p>略。</p>
<h4 id="47"><a href="#47" class="headerlink" title="47."></a>47.</h4><p>略。</p>
<h4 id="48"><a href="#48" class="headerlink" title="48."></a>48.</h4><p>略。</p>
<h4 id="49"><a href="#49" class="headerlink" title="49."></a>49.</h4><p>第一次投掷：</p>
<hr>
<p>  点数   1               2               3               4               5               6<br>  概率   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$</p>
<hr>
<p>第二次投掷：</p>
<hr>
<p>  点数和   2                3                4                5                6                7                8                9                10               11               12<br>  概率     $\frac{1}{36}$   $\frac{2}{36}$   $\frac{3}{36}$   $\frac{4}{36}$   $\frac{5}{36}$   $\frac{6}{36}$   $\frac{5}{36}$   $\frac{4}{36}$   $\frac{3}{36}$   $\frac{2}{36}$   $\frac{1}{36}$</p>
<hr>
<p>第三次投掷：</p>
<hr>
<p>  点数和   3                 4                 5                 6                  7                  8                 9                  10                 11                 12                 13                 14                 15                 16                17                18<br>  概率     $\frac{1}{216}$   $\frac{3}{216}$   $\frac{6}{216}$   $\frac{10}{216}$   $\frac{15}{216}$   $\frac{21}{36}$   $\frac{25}{216}$   $\frac{27}{216}$   $\frac{27}{216}$   $\frac{25}{216}$   $\frac{21}{216}$   $\frac{15}{216}$   $\frac{10}{216}$   $\frac{6}{216}$   $\frac{3}{216}$   $\frac{1}{216}$</p>
<hr>
<p>从上表来看和为11的概率要大于和为12。</p>
<h4 id="50"><a href="#50" class="headerlink" title="50."></a>50.</h4><p>如果$n &gt; 365$，那么根据鸽巢原理，一定有两个人的生日在同一天，所以这个概率是0。</p>
<p>如果$1 &lt; n \leq 365$，那么没有任何两个人在同一天生日的概率为$\frac{C_{365}^n}{365^n}$（每个人选其中的某一天作为自己的生日，可选的所有可能是$365^n$种）。</p>
<h4 id="51"><a href="#51" class="headerlink" title="51."></a>51.</h4><h5 id="a-10"><a href="#a-10" class="headerlink" title="(a)"></a>(a)</h5><p>样本空间：抽走两个红球（设为事件A），抽走两个白球（设为事件B），抽走一个红球和一个白球（设为事件C）。</p>
<p>基于离散均匀分布律的计数方法，我们有：<br>$$P(A) = \frac{C_m^2}{C_{m + n}^2}, P(B) = \frac{C_n^2}{C_{m + n}^2},<br>             P(C) = \frac{C_m^1C_n^1}{C_{m + n}^2}$$</p>
<p>设第一次抽到红球为事件D，第一次抽到白球为事件E，基于乘积规则，我们也可以得到：<br>$$P(D) = \frac{m}{m + n}, P(E) = \frac{n}{m + n}$$</p>
<p>那么：<br>$$P(A) = \frac{m - 1}{m + n - 1}P(D) = \frac{C_m^2}{C_{m + n}^2}$$<br>$$P(B) = \frac{n - 1}{m + n - 1}P(E) = \frac{C_n^2}{C_{m + n}^2}$$<br>$$P(C) = \frac{m}{m + n - 1}P(E) + \frac{n}{m + n - 1}P(D) = \frac{C_m^1C_n^1}{C_{m + n}^2}$$</p>
<h5 id="b-10"><a href="#b-10" class="headerlink" title="(b)"></a>(b)</h5><ul>
<li><p>  出现1的时候，全是红球的概率为$\frac{C_m^1}{C_{m + n}^1}$</p>
</li>
<li><p>  出现2的时候，全是红球的概率为$\frac{C_m^2}{C_{m + n}^2}$</p>
</li>
<li><p>  出现3的时候，全是红球的概率为$\frac{C_m^3}{C_{m + n}^3}$</p>
</li>
</ul>
<p>由全概率公式，取出的球全是红色的概率为$\frac{1}{3}(\frac{C_m^1}{C_{m + n}^1} + \frac{C_m^2}{C_{m + n}^2} + \frac{C_m^3}{C_{m + n}^3})$。</p>
<h4 id="52"><a href="#52" class="headerlink" title="52."></a>52.</h4><p>第13张牌正好是老K，意味着前12张牌均不能是老K，故概率为$\frac{C_{48}^{12}}{C_{52}^{12}}\frac{C_4^1}{C_{40}^{1}}$。</p>
<h4 id="53"><a href="#53" class="headerlink" title="53."></a>53.</h4><p>我们把90个学生排成一排，前30个学生在一个班，中间30个学生在一个班，最后30个学生在一个班。这样所有的可能有$90!$种。<br>对于乔和简，他们两个人可以先确定在三个班中的某一个，然后在这个班的30个位置中任意占据两个，其他人再随机排列。<br>这样满足条件的排列数是$C_3^1C_{30}^298!$。所以分到一个班的概率为$\frac{C_3^1C_{30}^298!}{90!}$。</p>
<h4 id="54"><a href="#54" class="headerlink" title="54."></a>54.</h4><h5 id="a-11"><a href="#a-11" class="headerlink" title="(a)"></a>(a)</h5><p>$\frac{20!}{10!10!} = 184756‬$</p>
<h5 id="b-11"><a href="#b-11" class="headerlink" title="(b)"></a>(b)</h5><p>不难发现只有两种排列方式满足要求（”美国车，外国车，美国车，…，外国车”和”外国车，美国车，外国车，…，美国车”）。<br>所以这个概率为$\frac{2}{184756}$。</p>
<h4 id="55"><a href="#55" class="headerlink" title="55."></a>55.</h4><p>总共可以摆放的方案一共是$C_{64}^8$种。考虑合法的情况，每一个车一定单独占据一行，第一行的车有8种选择，第二行的车只有7种选择……<br>所以合法的概率为$\frac{8!}{C_{64}^8}$。</p>
<h4 id="56"><a href="#56" class="headerlink" title="56."></a>56.</h4><h5 id="a-12"><a href="#a-12" class="headerlink" title="(a)"></a>(a)</h5><p>$C_8^4C_{10}^3 = 1680$</p>
<h5 id="b-12"><a href="#b-12" class="headerlink" title="(b)"></a>(b)</h5><ul>
<li><p>  假定所选的所有高水平课程均在$H_1, \dots, H_5$中，那么所有的可能有$C_7^3C_5^3 = 350$种；</p>
</li>
<li><p>  假定所选的所有高水平课程均在$H_6, \dots, H_10$中，那么所有的可能有$C_6^2C_5^3 = 150$种；</p>
</li>
<li><p>  假设二者均有涉及，那么三门先修课程均需要开设，所有的可能有$C_5^1\sum_{i = 1}^2C_5^iC_5^{3 - i} = 500$种</p>
</li>
</ul>
<p>所以一共有1000种。</p>
<h4 id="57"><a href="#57" class="headerlink" title="57."></a>57.</h4><p>将问题换一个方式陈述：将一个26个不同字母随机排列组成的字符串划分为6个部分，有多少种划分方式？</p>
<p>26个字母的总排列数是$26!$，将长度为26的字符串划分为6个部分，只需要在其中的25个间隔处插入空格即可，且不能有连续的空格。<br>所以最后的结果为$26!C_{25}^5$。</p>
<h4 id="58"><a href="#58" class="headerlink" title="58."></a>58.</h4><p>接下来的问题中我们假设一共有52张牌。</p>
<h5 id="a-13"><a href="#a-13" class="headerlink" title="(a)"></a>(a)</h5><p>$\frac{C_4^3C_{48}^4}{C_{52}^7}$</p>
<h5 id="b-13"><a href="#b-13" class="headerlink" title="(b)"></a>(b)</h5><p>$\frac{C_4^2C_{48}^5}{C_{52}^7}$</p>
<h5 id="c-3"><a href="#c-3" class="headerlink" title="(c)"></a>(c)</h5><p>$\frac{C_4^3C_{48}^4}{C_{52}^7} + \frac{C_4^2C_{48}^5}{C_{52}^7} - \frac{C_4^2C_4^3C_{44}^2}{C_{52}^7}$</p>
<h4 id="59"><a href="#59" class="headerlink" title="59."></a>59.</h4><p>$\frac{C_k^nC_{100 - k}^{m - n}}{C_{100}^m}$</p>
<p>柠檬法案：柠檬法（Lemon<br>Laws）是一种美国的消费者保护法，主要是在保障汽车买主的权益。柠檬法的名称起源于美国经济学家乔治·阿克罗夫（George<br>A.<br>Akerlof）所发表的一篇经济学论文，因为这缘故，对于出厂后有瑕疵问题的汽车，通常也会称呼其为柠檬车（Lemon<br>Car）或直接就称为柠檬。</p>
<h4 id="60"><a href="#60" class="headerlink" title="60."></a>60.</h4><p>我们可以使用类似于53题的思路，得到$\frac{13^4 \times \frac{48!}{(4!)^{12}}}{\frac{52!}{(4!)^{13}}} \approx 0.105$</p>
<h4 id="61"><a href="#61" class="headerlink" title="61."></a>61.</h4><p>略。</p>
<h4 id="62"><a href="#62" class="headerlink" title="62."></a>62.</h4><p>略。</p>
<h3 id="第二章-离散随机变量"><a href="#第二章-离散随机变量" class="headerlink" title="第二章 离散随机变量"></a>第二章 离散随机变量</h3><h4 id="1-1"><a href="#1-1" class="headerlink" title="1."></a>1.</h4><hr>
<p>  得分   0      1      2      3      4<br>  概率   0.18   0.27   0.34   0.14   0.07</p>
<hr>
<h4 id="2-1"><a href="#2-1" class="headerlink" title="2."></a>2.</h4><p>假设有k个人与你的生日相同，由二项分布：<br>$$P_X(k) = C_{500}^kp^k(1 - p)^{500 - k}$$<br>其中$p = \frac{1}{365}$，则有人生日与你相同的概率为$1 - P_X(0) = 1 - (1 - \frac{1}{365})^{500} \approx 0.75$。</p>
<p>若使用泊松分布逼近，$\lambda = np = \frac{500}{365}$，<br>那么$1 - P_X(0) = 1 - e^{-\lambda}\frac{\lambda^0}{0!} \approx 0.75$。</p>
<h4 id="3-1"><a href="#3-1" class="headerlink" title="3."></a>3.</h4><h5 id="a-14"><a href="#a-14" class="headerlink" title="(a)"></a>(a)</h5><p>设k为比赛连续平局的局数，则$P(k) = 0.3^k$，那么赢得比赛的概率为$0.4\sum_{i = 0}^9P(i) = 0.5714251972‬$。</p>
<h5 id="b-14"><a href="#b-14" class="headerlink" title="(b)"></a>(b)</h5><p>下棋的前9局数满足几何分布，所以$P_X(k) = (1 - p)^{k - 1}p = 0.3^{k - 1}0.7$，<br>第10局后比赛一定结束， 所以分布列为：</p>
<hr>
<p>  局数   1     2      3       4         5<br>  概率   0.7   0.21   0.063   0.0189‬   0.00567‬</p>
<hr>
<hr>
<p>  局数   6           7            8             9              10<br>  概率   0.001701‬   0.0005103‬   0.00015309‬   0.000045927‬   0.000019683‬</p>
<hr>
<h4 id="4-1"><a href="#4-1" class="headerlink" title="4."></a>4.</h4><h5 id="a-15"><a href="#a-15" class="headerlink" title="(a)"></a>(a)</h5><p>在使用者的数量小于等于50的时候，调制解调器的数量分布满足二项分布；<br>使用者数量大于50的时候，由于调制解调器数量不足，故一定是50个。</p>
<p>所以分布列为：</p>
<hr>
<p>  人数   $k \leq 50$                         $k &gt; 50$<br>  概率   $C_{1000}^k0.01^k0.99^{1000 - k}$   $1 - \sum_{i = 0}^{50}C_{1000}^i0.01^i0.99^{1000 - i}$</p>
<hr>
<h5 id="b-15"><a href="#b-15" class="headerlink" title="(b)"></a>(b)</h5><p>设使用者人数为k，由泊松分布有$P_X(k) = e^{-\lambda}\frac{\lambda^k}{k!} = e^{-10}\frac{10^k}{k!}$</p>
<h5 id="c-4"><a href="#c-4" class="headerlink" title="(c)"></a>(c)</h5><p>利用精确分布，$P = 1 - \sum_{i = 0}^{50}C_{1000}^i0.01^i0.99^{1000 - i}$，而利用泊松分布，$P = 1 - \sum_{i = 0}^{50}e^{-10}\frac{10^i}{i!}$</p>
<h4 id="5-1"><a href="#5-1" class="headerlink" title="5."></a>5.</h4><h5 id="a-16"><a href="#a-16" class="headerlink" title="(a)"></a>(a)</h5><p>第一时段结束时：</p>
<hr>
<p>  信息包数量k   $k &lt; b$                              $k = b$<br>  概率          $e^{-\lambda}\frac{\lambda^k}{k!}$   $1 - \sum_{i = 0}^{b - 1}e^{-\lambda}\frac{\lambda^i}{i!}$</p>
<hr>
<p>第二时段结束时：</p>
<hr>
<p>  信息包数量k   $k = 0$                                             $k &gt; 0$<br>  概率          $\sum_{i = 0}^c e^{-\lambda}\frac{\lambda^i}{i!}$   $\sum_{i = c + 1}^{b - 1} e^{-\lambda}\frac{\lambda^i}{i!} + 1 - \sum_{i = 0}^{b - 1}e^{-\lambda}\frac{\lambda^i}{i!}$</p>
<hr>
<h5 id="b-16"><a href="#b-16" class="headerlink" title="(b)"></a>(b)</h5><p>如果到达的信息包的数量不超过b，是不会发生丢包的，<br>所以丢包的概率为$1 - \sum_{i = 0}^{b}e^{-\lambda}\frac{\lambda^i}{i!}$。</p>
<h4 id="6-1"><a href="#6-1" class="headerlink" title="6."></a>6.</h4><h5 id="a-17"><a href="#a-17" class="headerlink" title="(a)"></a>(a)</h5><p>对于$n = 5$，凯尔特人队获胜的概率为$p^3 + C_3^1p^3(1 - p) + C_4^2p^3(1 - p)^2 = 10p^3 - 15p^4 + 6p^5$；</p>
<p>对于$n = 3$，凯尔特人队获胜的概率为$p^2 + C_2^1p^2(1 - p) = 3p^2 - 2p^3$</p>
<p>所以当$10p^3 - 15p^4 + 6p^5 &gt; 3p^2 - 2p^3$，时，$n = 5$比$n = 3$合算。<br>将所有式子移到左侧，消去$p^2(p &gt; 0)$并求导得到$18p^2 - 30p + 12$，不难发现当$p = \frac{1}{2}$或$p = 1$时，原式为0，<br>且该函数在$[0.5, 1]$上先增后减，所以当$p \in (0.5, 1)$时$n = 5$比$n = 3$合算。</p>
<h5 id="b-17"><a href="#b-17" class="headerlink" title="(b)"></a>(b)</h5><p>不难得到$P_N(n = 2k + 1) = p^{k + 1} + C_{k + 1}^1P^{k+1}(1 - p) + \dots + C_{2k}^kp^{k + 1}(1 - p)^k$，<br>并且对于不同的k，$P_N(0) = 0, P_N(1) = 1, P_N(\frac{1}{2}) = \frac{1}{2}$恒成立（0和1十分显然，对于0.5的证明我们额外放在(c)部分讲解）。</p>
<p>不妨做差$P_N(2k + 3) - P_N(2k + 1)$并对这个式子求导，不难验证导数在$\frac{1}{2}$处大于0。<br>由罗尔定理导函数在$[0.5, 1]$之间一定存在值为0的点，且仅有一个（这是一个$2k + 1$重多项式，其中0为k重根，1为k重根，$\frac{1}{2}$为1重根）。<br>所以导函数在区间上先增后减，原来的差值函数保证大于0，故$p \in (0.5, 1)$。</p>
<h5 id="c-5"><a href="#c-5" class="headerlink" title="(c)"></a>(c)</h5><p>原题不存在，这部分是(b)中$P_N(n)$在$p = \frac{1}{2}$为常数的证明。</p>
<p>因为$P_N(n = 2k + 1) = p^{k + 1} + C_{k + 1}^1p^{k+1}(1 - p) + \dots + C_{2k}^kp^{k + 1}(1 - p)^k$,<br>将$\frac{1}{2}$带入后$P_N(2k + 1) = (\frac{1}{2})^{k + 1} + C_k^1(\frac{1}{2})^{k + 2} + \dots + C_{2k}^k(\frac{1}{2})^{2k + 1}$。</p>
<p>当$k = 0$时，原式显然成立。假定$k = m$时原式也成立，则$k = m + 1$时，<br>我们不妨计算$2^{k+2}P_N(2m + 3), 2^{k + 1}P_N(2m + 1)$，将两个式子逐项做差，得到$2^{k + 1}P_N(2m + 1)$，<br>则$2^{k+2}P_N(2m + 3) = 2^{k + 2}P_N(2m + 1)$，故原式也为$\frac{1}{2}$，归纳完毕。</p>
<p>证毕。</p>
<h4 id="7-1"><a href="#7-1" class="headerlink" title="7."></a>7.</h4><h5 id="a-18"><a href="#a-18" class="headerlink" title="(a)"></a>(a)</h5><h5 id="1-2"><a href="#1-2" class="headerlink" title="(1)"></a>(1)</h5><p>由于失败后会做记号，所以尝试的次数最多只有四次（第四次即便失败，剩下的钥匙也没必要再试一次了）。</p>
<p>分布列：</p>
<hr>
<p>  尝试次数   1               2               3               4<br>  概率       $\frac{1}{5}$   $\frac{1}{5}$   $\frac{1}{5}$   $\frac{2}{5}$</p>
<hr>
<h5 id="2-2"><a href="#2-2" class="headerlink" title="(2)"></a>(2)</h5><p>随机尝试的情况下，尝试次数满足几何分布，所以分布律为：$P(k) = (\frac{4}{5})^{k - 1}(\frac{1}{5})^k$。</p>
<h5 id="b-18"><a href="#b-18" class="headerlink" title="(b)"></a>(b)</h5><h5 id="1-3"><a href="#1-3" class="headerlink" title="(1)"></a>(1)</h5><hr>
<p>  尝试次数   1               2                3                4                5               6                7                8<br>  概率       $\frac{1}{5}$   $\frac{8}{45}$   $\frac{7}{45}$   $\frac{2}{15}$   $\frac{1}{9}$   $\frac{4}{45}$   $\frac{1}{15}$   $\frac{1}{15}$</p>
<hr>
<h5 id="2-3"><a href="#2-3" class="headerlink" title="(2)"></a>(2)</h5><p>$P(k) = (\frac{4}{5})^{k - 1}(\frac{1}{5})^k$</p>
<h4 id="8-1"><a href="#8-1" class="headerlink" title="8."></a>8.</h4><p>二项随机变量满足$P_X(k) = C_n^kp^k(1 - p)^{n - k}$。</p>
<p>所以 $$P_X(k + 1) = C_n^{k + 1}p^{k + 1}(1 - p)^{n - k - 1} =$$<br>$$\frac{n!}{(k + 1)!(n - k -1)!}p^{k + 1}(1 - p)^{n - k - 1} =$$<br>$$\frac{n - k}{k + 1}\frac{n!}{k!(n - k)!}\frac{p}{1 - p}p^k(1 - p)^{n - k} = \frac{p}{1 - p}\frac{n - k}{k + 1}P_X(k)$$</p>
<p>证毕。</p>
<h4 id="9-1"><a href="#9-1" class="headerlink" title="9."></a>9.</h4><p>由第8题，我们可以得到：<br>$P_X(k + 1) - P_X(k) = (\frac{p}{1 - p}\frac{n - k}{k + 1} - 1)P_X(k)$。</p>
<p>由于$P_X(k) \geq 0$，我们不妨令$f(k) = \frac{p}{1 - p}\frac{n - k}{k + 1} - 1$，<br>则$f’(k) = \frac{-p(n + 1)}{(1 - p)(k + 1)^2}$。</p>
<p>不难发现导函数恒为负，且当$k = p(n + 1) - 1$时，原函数为0。所以当$k &lt;= (n + 1)p$时，$P_X(k)$是非降的，而在$k &gt; (n + 1)p$时是单调递减的。</p>
<p>证毕。</p>
<h4 id="10-1"><a href="#10-1" class="headerlink" title="10."></a>10.</h4><p>因为泊松分布$P_X(k) = e^{-\lambda}\frac{\lambda^k}{k!}$，<br>所以$P_X’(k) = \frac{\lambda^ke^{-\lambda}}{(k!)^2}(ln\lambda\Gamma(k + 1) - \Gamma’(k + 1))$。</p>
<p>由于括号外的部分恒正，我们解括号内的部分等于0，即$\frac{\Gamma’(k + 1)}{\Gamma(k + 1)} = \Psi(k + 1) = ln\lambda$。</p>
<p>解得近似解为$k = \lambda$。由于$\Psi(x)$是单调递增的，故导函数在$[0, \lambda]$上大于0，在$(\lambda, \infty)$上小于0。</p>
<p>也可以简单地通过$\frac{P_X(k + 1)}{P_X(k)} = \frac{\lambda}{k + 1}$看出单调性。</p>
<p>证毕。</p>
<h4 id="11-1"><a href="#11-1" class="headerlink" title="11."></a>11.</h4><p>略。</p>
<h4 id="12-1"><a href="#12-1" class="headerlink" title="12."></a>12.</h4><p>略。</p>
<h4 id="13-1"><a href="#13-1" class="headerlink" title="13."></a>13.</h4><hr>
<p>  数量   2                3                4                 5                 6                7<br>  概率   $\frac{1}{32}$   $\frac{5}{32}$   $\frac{10}{32}$   $\frac{10}{32}$   $\frac{5}{32}$   $\frac{1}{32}$</p>
<hr>
<h4 id="14-1"><a href="#14-1" class="headerlink" title="14."></a>14.</h4><h5 id="a-19"><a href="#a-19" class="headerlink" title="(a)"></a>(a)</h5><hr>
<p>  Y   0               1                2<br>  P   $\frac{2}{5}$   $\frac{3}{10}$   $\frac{3}{10}$</p>
<hr>
<h5 id="b-19"><a href="#b-19" class="headerlink" title="(b)"></a>(b)</h5><hr>
<p>  Y   0               1               2                5<br>  P   $\frac{1}{5}$   $\frac{1}{5}$   $\frac{1}{10}$   $\frac{1}{2}$</p>
<hr>
<h4 id="15-1"><a href="#15-1" class="headerlink" title="15."></a>15.</h4><p>因为$Y = ln(X), X = a^{|k|}$， 所以$Y = |K|lna$。<br>又因为$K$是$[-n, n]$上的均匀分布，<br>所以$P_Y(y = |k|lna) = {\begin{array}{lcr}<br>            \frac{2}{2n + 1} &amp; &amp; k \ne 0\<br>            \frac{1}{2n + 1} &amp; &amp; k = 0<br>        \end{array}$</p>
<h4 id="16-1"><a href="#16-1" class="headerlink" title="16."></a>16.</h4><h5 id="a-20"><a href="#a-20" class="headerlink" title="(a)"></a>(a)</h5><p>由归一性可知$2(P_X(3) + P_X(2) +P_X(1)) + P_X(0) = 1$，<br>解之得$a = \frac{1}{28}$。</p>
<p>由于$P_X$及其定义域关于$x = 0$对称，所以$E[x] = 0$。</p>
<h5 id="b-20"><a href="#b-20" class="headerlink" title="(b)"></a>(b)</h5><p>因为$E[x] = 0$，所以$Z = X^2$， 故$P_Z(z) = {\begin{array}{lcr}<br>                \frac{2z}{28} &amp; &amp; z = 0, 1, 4, 9 \<br>                0 &amp; &amp; otherwise<br>            \end{array}$</p>
<h5 id="c-6"><a href="#c-6" class="headerlink" title="(c)"></a>(c)</h5><p>由(b)得，$var(X) = E[Z] = \frac{2}{28} + \frac{32}{28} + \frac{162}{28} = 7$。</p>
<h5 id="d-2"><a href="#d-2" class="headerlink" title="(d)"></a>(d)</h5><p>$var(X) = \sum_{x}(x - E[X])^2p_X(x) = 2(\frac{1}{28} + \frac{16}{28} + \frac{81}{28}) = 7$</p>
<h4 id="17-1"><a href="#17-1" class="headerlink" title="17."></a>17.</h4><p>因为$F = 1.8C + 32$，所以$E[F] = 1.8E[C] + 32 = 40$，$var(F) = 1.8^2var(C) = 324$，所以$\sigma_F = 18$。</p>
<h4 id="18-1"><a href="#18-1" class="headerlink" title="18."></a>18.</h4><p>$E[X] = \int_a^b\frac{2^x}{b - a}dx = \frac{2^b - 2^a}{(b - a)ln2}$</p>
<p>$var(X) = \int_a^b(2^x - E[X])^2 = \frac{2^{2b} - 2^{2a}}{2ln2} - \frac{b - a}{ln2} + (2^b - 2^a)^2(b - a)$</p>
<h4 id="19-1"><a href="#19-1" class="headerlink" title="19."></a>19.</h4><p>略。</p>
<h4 id="20-1"><a href="#20-1" class="headerlink" title="20."></a>20.</h4><p>设需要购买的数量为随机变量X，则X满足几何分布。所以$P_X(x) = (1 - p)^{x - 1}p$，<br>故期望$E[X] = \sum_{x = 1}^{\infty}x(1 - p)^{x - 1}p$，计算$E[x] - (1 - p)E[x]$后求得期望为$\frac{1}{p}$。</p>
<p>首先计算$E[X^2]$，然后使用$var(X) = E[X^2] - E[X]^2$计算方差。<br>因为$\E[X^2] = \sum_{x = 1}^{\infty}x^2(1 - p)^{x - 1}p$，<br>不难发现可以利用积分得到类似$E[X]$的形式。最后求得$E[X^2] = \frac{2 - p}{p^2}$，<br>故$var(X) = \frac{1 - p}{p^2}$。</p>
<p>几何分布的期望也可以通过第6节的全期望定理完成。</p>
<h4 id="21-1"><a href="#21-1" class="headerlink" title="21."></a>21.</h4><p>由题意得，这是一个几何分布，所以$P_N(n) = (\frac{1}{2})^n$。<br>于是我们可以求得期望$E[X] = \sum_{x = 1}^{\infty}(\frac{1}{2})^x2^x = \infty$。</p>
<p>这个级数并不收敛，所以理论上你可以得到无穷多的钱，但是事实上这并不可能发生。</p>
<h4 id="22-1"><a href="#22-1" class="headerlink" title="22."></a>22.</h4><h5 id="a-21"><a href="#a-21" class="headerlink" title="(a)"></a>(a)</h5><p>$$P_X(x) = (1 - p - q + 2pq)^{x - 1}(p + q - 2pq)$$<br>$$E[X] = \frac{1}{p + q -2pq}$$<br>$$var(X) = \frac{1 - p - q + 2pq}{(p + q - 2pq)^2}$$</p>
<h5 id="b-21"><a href="#b-21" class="headerlink" title="(b)"></a>(b)</h5><p>由条件概率得： $\frac{p(1 - q)}{p + q - 2pq}$</p>
<h4 id="23-1"><a href="#23-1" class="headerlink" title="23."></a>23.</h4><h5 id="a-22"><a href="#a-22" class="headerlink" title="(a)"></a>(a)</h5><p>抛掷k次，前$k - 1$次必定是正反交替，最后一次和倒数第二次相同。抛掷k次结束可以有两种情况（最后一次为正面和最后一次为反面），<br>所以分布列为$P_X(k) = (\frac{1}{2})^{k - 1}$。</p>
<p>期望值$E[X] = \sum_{x = 2}^{\infty}x(\frac{1}{2})^{x - 1}$，利用积分可得$E[X] = 3$。</p>
<p>同理方差$var(X) = E[X^2] - E[X]^2 = 12 - 9 = 3$。</p>
<p>这个问题在条件一节的习题33中有对任意p值下期望值公式的推广。</p>
<h5 id="b-22"><a href="#b-22" class="headerlink" title="(b)"></a>(b)</h5><p>抛掷k次，一定先出现若干次（可以为0）反面朝上，接下来若干次正面朝上I（至少有一次），最后一次反面朝上。<br>所以分布列为$P_X(k) = (k - 1)(\frac{1}{2})^k$。</p>
<p>期望值$E[X] = \sum_{x = 2}^{\infty}x(x - 1)(\frac{1}{2})^x = 4$。</p>
<p>方差$var(X) = E[X^2] - E[X]^2 = 80 - 16 = 64$</p>
<h4 id="24-1"><a href="#24-1" class="headerlink" title="24."></a>24.</h4><h5 id="a-23"><a href="#a-23" class="headerlink" title="(a)"></a>(a)</h5><p>由题意得：X，Y是$-2 \leq x \leq 4, x - 1 \leq y \leq x + 1$上的均匀分布，所以：</p>
<hr>
<p>  Y/X   -2               -1               0                1                2                3                4<br>  -3    $\frac{1}{21}$   0                0                0                0                0                0<br>  -2    $\frac{1}{21}$   $\frac{1}{21}$   0                0                0                0                0<br>  -1    $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0                0                0<br>  0     0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0                0<br>  1     0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0<br>  2     0                0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0<br>  3     0                0                0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$<br>  4     0                0                0                0                0                $\frac{1}{21}$   $\frac{1}{21}$<br>  5     0                0                0                0                0                0                $\frac{1}{21}$</p>
<hr>
<p>所以边缘分布列$$P_X(x) = \left{\begin{array}{lcr}<br>                \frac{1}{7} &amp; &amp; -2 \leq x \leq 4 \<br>                0 &amp; &amp; otherwise<br>            \end{array}\right.$$</p>
<p>$$P_Y(y) = \left{\begin{array}{lcr}<br>                \frac{1}{21} &amp; &amp; y = -3/y = 5\<br>                \frac{2}{21} &amp; &amp; y = -2/y = 4\<br>                \frac{1}{7} &amp; &amp; -1 \leq y \leq 3\<br>                0 &amp; &amp; otherwise<br>            \end{array}\right.$$</p>
<p>期望$E[X] = 1, E[Y] = 1$。</p>
<h5 id="b-23"><a href="#b-23" class="headerlink" title="(b)"></a>(b)</h5><p>$$E[100X + 200Y] = \sum_{(x, y)}(100x + 200y)P_{X, Y}(x, y) = \frac{8200}{21} \approx 390$$</p>
<h4 id="25-1"><a href="#25-1" class="headerlink" title="25."></a>25.</h4><h5 id="a-24"><a href="#a-24" class="headerlink" title="(a)"></a>(a)</h5><p>因为答案被选中的可能性相等，故$P_{I, J}$满足均匀分布，所以<br>$$P_{I, J}(i, j) = \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{\sum_{k = 1}^{n}m_k} &amp; &amp; 0 &lt; j \leq m_i\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>                \right.$$</p>
<p>不难得到边缘分布列$P_I(i) = \frac{m_i}{\sum_{k = 1}^{n}m_k}, P_J(j) = \frac{\sum_{k = 1}^{n}[j \leq m_k]}{\sum_{k = 1}^{n}m_k}$。</p>
<h5 id="b-24"><a href="#b-24" class="headerlink" title="(b)"></a>(b)</h5><p>对于第i个学生，可以将其回答情况视为$m_i$个独立的题目的得分期望的和，所以总分的期望值为$\sum_{k = 0}^{m_i}ap_{i, k} + b(1 - p_{i, k})$。</p>
<h4 id="26-1"><a href="#26-1" class="headerlink" title="26."></a>26.</h4><h5 id="a-25"><a href="#a-25" class="headerlink" title="(a)"></a>(a)</h5><p>因为最低分不小于x的分布列$P(X_1 \geq x, X_2 \geq x, X_3 \geq x) = \Pi_{i = 1}^3\frac{111 - x}{10}$</p>
<p>所以$P_X(x) = (\frac{111 - x}{10})^3 - (\frac{110 - x}{10})^3$</p>
<h5 id="b-25"><a href="#b-25" class="headerlink" title="(b)"></a>(b)</h5><p>三天的平均得分为$\frac{x_1 + x_2 + x_3}{3}$，而X的期望$E[X] = \sum_{i = 101}^{110}x((\frac{111 - x}{10})^3 - (\frac{110 - x}{10})^3)$。</p>
<p>平均分的期望值为$105.5$，而X的期望为$103.025$。</p>
<h4 id="27-1"><a href="#27-1" class="headerlink" title="27."></a>27.</h4><p>略。</p>
<h4 id="28-1"><a href="#28-1" class="headerlink" title="28."></a>28.</h4><p>换一种方式陈述书上的答案，使得答案更加容易理解。</p>
<p>我们如果按照原来的顺序答题，奖金的期望$E[L] = p_1v_1 + p_1p_2v_2 + \dots + p_1p_2\dots p_nv_n$。</p>
<p>假定我们尝试着交换相邻两道题的顺序（交换任意两道题的顺序可以通过若干次相邻交换实现），<br>则交换后的期望值变为$E[L’] = p_1v_1 + \dots + p_1p_2\dots p_{k + 1}v_{k + 1} + p_1p_2\dots p_kp_{k + 1}v_k + \dots + p_1p_2\dots p_nv_n$。<br>将两个式子做差，可以得到$p_1p_2\dots p_{k - 1}(p_kv_k + p_kp_{k + 1}v_{k + 1} - p_{k + 1}v_{k + 1} - p_kp_{k + 1}v_k)$。括号外的式子一定为正，<br>我们只看括号内：$p_kv_k(1 - p_{k + 1}) + p_{k + 1}v_{k + 1}(p_k - 1)$，提出一个$(1 - p_{k + 1})(1 - p_{k})$，得到<br>$\frac{p_kv_k}{1 - p_k} - \frac{p_{k + 1}v_{k + 1}}{1 - p_{k + 1}}$。所以先选$\frac{p_iv_i}{1 - p_i}$是明智的。</p>
<p>证毕。</p>
<h4 id="29-1"><a href="#29-1" class="headerlink" title="29."></a>29.</h4><p>略。</p>
<h4 id="30-1"><a href="#30-1" class="headerlink" title="30."></a>30.</h4><p>略。</p>
<h4 id="31-1"><a href="#31-1" class="headerlink" title="31."></a>31.</h4><p>因为$P_{X, Y}(x, y) = P_Y(y)P_{X|Y}(x|y)$，所以 $P_{X,Y}(x, y) =\<br>         C_4^y(\frac{1}{6})^y(\frac{1}{6})^{4 - y}C_{4 - y}^x(\frac{1}{6})^x(\frac{5}{6})^{4 - y - x}$。</p>
<p>如果不满足$0 \leq x,y \leq 4$且$0 \leq x + y \leq 4$，则$P_{X, Y}(x, y) = 0$。</p>
<h4 id="32-1"><a href="#32-1" class="headerlink" title="32."></a>32.</h4><p>$$E[S|A = a] = \sum_{s}sP_{S|A = a}(s) = \sum_{s}\frac{sC_{m}^sC_{m - s}^{a - 2s}}{C_{2m}^a}$$</p>
<h4 id="33-1"><a href="#33-1" class="headerlink" title="33."></a>33.</h4><p>略。</p>
<h4 id="34-1"><a href="#34-1" class="headerlink" title="34."></a>34.</h4><p>略</p>
<h4 id="35-1"><a href="#35-1" class="headerlink" title="35."></a>35.</h4><p>略。</p>
<h4 id="36-1"><a href="#36-1" class="headerlink" title="36."></a>36.</h4><p>略。</p>
<h4 id="37-1"><a href="#37-1" class="headerlink" title="37."></a>37.</h4><p>略。</p>
<h4 id="38-1"><a href="#38-1" class="headerlink" title="38."></a>38.</h4><p>设每个路口遇到红灯的概率为p。</p>
<h5 id="a-26"><a href="#a-26" class="headerlink" title="(a)"></a>(a)</h5><p>$$P_X(x) = C_4^xp^x(1 - p)^{4 - x}$$ $$E[X] = 4p$$<br>$$var(X) = \sum_{i = 0}^4var(X_i) = 4p(1 - p)$$</p>
<h5 id="b-26"><a href="#b-26" class="headerlink" title="(b)"></a>(b)</h5><p>$$var(2X) = 16p(1 - p)$$</p>
<h4 id="39-1"><a href="#39-1" class="headerlink" title="39."></a>39.</h4><p>由独立性，有： $$E[X] = \sum_{i = 1}^{10}E[X_i] = 35$$<br>$$var(X) = \sum_{i = 1}^{10}var(X_i) = \frac{175}{6}$$</p>
<h4 id="40-1"><a href="#40-1" class="headerlink" title="40."></a>40.</h4><p>由于论文之间的成绩完全独立，我们不妨在论文与论文之间插入隔板，以此区分不同成绩的两批论文。<br>如果上交了k篇论文，每种评分的论文至少有一篇的概率就为$P(k) = \frac{5!C_{k - 1}^5}{(k + 1)^5}$<br>（一共需要安插5个隔板，分为6个不同评分的区间，所有的可能安插的位置一共是$k + 1$个，而合法的安插位置只能在中间的$k - 1$个中选择，且不能重复，隔板自身的顺序有$5!$种）。</p>
<p>其中$P(24) \approx 0.5$，$\lim_{k \to \infty} = 1$。</p>
<h4 id="41-1"><a href="#41-1" class="headerlink" title="41."></a>41.</h4><h5 id="a-27"><a href="#a-27" class="headerlink" title="(a)"></a>(a)</h5><p>不难得到$P(x) = C_{250}^x0.02^x0.98^{250 - x}$，所以$E[X] = 50$。</p>
<p>所以罚单数刚好等于50的概率为$P(50)$。</p>
<h5 id="b-27"><a href="#b-27" class="headerlink" title="(b)"></a>(b)</h5><p>利用泊松分布近似(a)中的结果：$P(50) \approx e^{-50}\frac{(50)^{50}}{50!} \approx 0.056$。</p>
<h5 id="c-7"><a href="#c-7" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[0.5 \times 10X + 0.3 \times 20X + 0.2 \times 50X] = 1050$$<br>$$var(0.5 \times 10X + 0.3 \times 20X + 0.2 \times 50X) = 2160.9$$</p>
<h5 id="d-3"><a href="#d-3" class="headerlink" title="(d)"></a>(d)</h5><p>因为$\sigma(X) \approx 2.21$，所以$p \in [0, 0.064]$。</p>
<h4 id="42-1"><a href="#42-1" class="headerlink" title="42."></a>42.</h4><h5 id="a-28"><a href="#a-28" class="headerlink" title="(a)"></a>(a)</h5><p>因为$P_{X_i} = S$，所以$E[X_i] = S$，故$E[S_n] = \frac{nS}{n} = S$。</p>
<p>对于单个点的方差$var(X_i) = S(1 - S)$是有限值，<br>所以由独立性$\\lim_{n \to \infty}var(S_n) = \frac{nS(1 - S)}{n^2} = 0$。</p>
<p>证毕。</p>
<h5 id="b-28"><a href="#b-28" class="headerlink" title="(b)"></a>(b)</h5><p>$$S_n = \frac{(n - 1)S_{n - 1} + X_n}{n}$$</p>
<h5 id="c-8"><a href="#c-8" class="headerlink" title="(c)"></a>(c)</h5><p>由于单位正方形的内切圆的半径为$\frac{1}{2}$，所以<br>$$\pi = 4E[S_n] = \frac{4}{n}\sum_{i = 1}^{10000}P_{X_i}$$<br>只要计算落在圆形内部的点的数量即可确定$\pi$的值。</p>
<h5 id="d-4"><a href="#d-4" class="headerlink" title="(d)"></a>(d)</h5><p>类似于(c)。</p>
<h4 id="43-1"><a href="#43-1" class="headerlink" title="43."></a>43.</h4><p>略。</p>
<h4 id="44-1"><a href="#44-1" class="headerlink" title="44."></a>44.</h4><p>略。</p>
<h4 id="45-1"><a href="#45-1" class="headerlink" title="45."></a>45.</h4><p>略。</p>
<h4 id="46-1"><a href="#46-1" class="headerlink" title="46."></a>46.</h4><p>略。</p>
<h3 id="第三章-一般随机变量"><a href="#第三章-一般随机变量" class="headerlink" title="第三章 一般随机变量"></a>第三章 一般随机变量</h3><h4 id="1-4"><a href="#1-4" class="headerlink" title="1."></a>1.</h4><p>因为X是$[0, 1]$上的均匀分布，所以$p_X(x) = 1, x \in [0, 1]$。<br>所以Y的分布列为 $$P_Y(y) = \left{<br>            \begin{array}{lcr}<br>                \int_{0}^{\frac{1}{3}}p_X(x)dx = \frac{1}{3} &amp; &amp; y = 1\<br>                \int_{\frac{1}{3}}^{1}p_X(x)dx = \frac{2}{3} &amp; &amp; y = 2\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>所以$E[Y] = \frac{5}{3}$，根据期望规则，<br>$E[Y] = \int_{0}^{\frac{1}{3}}dx + \int_{\frac{1}{3}}^{1}2dx = \frac{5}{3}$，<br>可以验证我们的结果是正确的。</p>
<h4 id="2-4"><a href="#2-4" class="headerlink" title="2."></a>2.</h4><p>$\int_{-\infty}^{\infty}\frac{\lambda}{2}e^{-\lambda |x|} = \int_{0}^{\infty}\lambda e^{-\lambda x}$，<br>可见积分值与指数分布相同，归一化得证。</p>
<p>所以拉普拉斯随机变量的期望$E[X] = \int_{-\infty}^{\infty}\frac{x\lambda}{2}e^{-\lambda |x|} = 0$，<br>则$var(X) = E[X^2] = \int_{-\infty}^{\infty}\frac{x^2\lambda}{2}e^{-\lambda |x|}dx = \frac{2}{\lambda^2}$。</p>
<p>拉普拉斯分布的更普遍形式为$p_X(x) = \frac{1}{2\lambda}e^{-\frac{|x - \mu|}{\lambda}}$，<br>这个形式下的$\E[X] = \mu, var(X) = 2\lambda^2$。</p>
<h4 id="3-2"><a href="#3-2" class="headerlink" title="3."></a>3.</h4><p>略。</p>
<h4 id="4-2"><a href="#4-2" class="headerlink" title="4."></a>4.</h4><p>略。</p>
<h4 id="5-2"><a href="#5-2" class="headerlink" title="5."></a>5.</h4><p>不妨设三角形的高为H，对应的底边长度为L，<br>则分布函数$P_X(x)$可以用梯形的面积除以三角形面积得到：<br>$P_X(x) = \frac{\frac{H - x}{H}L + L}{2}\frac{2}{HL} = \frac{x(2H - x)}{H^2}, x \in [0, H]$。</p>
<p>所以概率密度函数$f_X(x) = P_X’(x) = \frac{2(H - x)}{H^2}$。</p>
<h4 id="6-2"><a href="#6-2" class="headerlink" title="6."></a>6.</h4><p>有0个顾客的时候，等候的时间为0；有一个顾客的时候，等候时间是一个指数随机变量。<br>所以： $$P_X(x) = \left{<br>            \begin{array}{lcr}<br>                0 &amp; &amp; x &lt; 0\<br>                \frac{1}{2} &amp; &amp; x = 0\<br>                \frac{1}{2}(1 + \int_{0}^{x}\lambda e^{-\lambda t}dt) &amp; &amp; x &gt; 0<br>            \end{array}<br>        \right.$$</p>
<p>解之得： $$P_X(x) = \left{<br>            \begin{array}{lcr}<br>                0 &amp; &amp; x &lt; 0\<br>                \frac{1}{2} &amp; &amp; x = 0\<br>                \frac{1}{2}(2 - e^{-\lambda x}) &amp; &amp; x &gt; 0<br>            \end{array}<br>        \right.$$</p>
<h4 id="7-2"><a href="#7-2" class="headerlink" title="7."></a>7.</h4><h5 id="a-29"><a href="#a-29" class="headerlink" title="(a)"></a>(a)</h5><p>X的分布函数为两个圆形面积的比值，所以$P_X(x) = \frac{x^2}{r^2}, x \in [0, r]$。<br>求导后可以得到概率密度函数$f_X(x) = \frac{2x}{r^2}$。所以$E[X] = \int_{0}^{r}\frac{2x^2}{r^2}dx = \frac{2r}{3}$，<br>$var(X) = E[X^2] - E[X]^2 = \frac{r^2}{18}$。</p>
<h5 id="b-29"><a href="#b-29" class="headerlink" title="(b)"></a>(b)</h5><p>由上一问可知： $$P_S(s) = \left{<br>                \begin{array}{lcr}<br>                    0 &amp; &amp; s &lt; 0\<br>                    \frac{r^2 - t^2}{r^2} &amp; &amp; s = 0\<br>                    \frac{r^2 - t^2}{r^2} &amp; &amp; 0 &lt; s &lt; \frac{1}{t}\<br>                    \frac{r^2 - t^2}{r^2} + \frac{t^2 - (\frac{1}{s})^2}{r^2} &amp; &amp; s \geq \frac{1}{t}<br>                \end{array}<br>            \right.$$</p>
<p>是连续随机变量。</p>
<h4 id="8-2"><a href="#8-2" class="headerlink" title="8."></a>8.</h4><h5 id="a-30"><a href="#a-30" class="headerlink" title="(a)"></a>(a)</h5><p>X的分布函数为$P_X(X \leq x)$，由全概率公式，可以得到$P_X(X \leq x) = pP_Y(Y \leq x) + (1 - p)P_Z(Z \leq x)$。<br>对式子两侧分别求导可得$f_X(x) = pf_Y(x) + (1 - p)f_Z(x)$。</p>
<p>证毕。</p>
<h5 id="b-30"><a href="#b-30" class="headerlink" title="(b)"></a>(b)</h5><p>$$P_X(x) = \left{<br>                \begin{array}{lcr}<br>                    pe^{\lambda x} &amp; &amp; x &lt; 0\<br>                    1 + (p - 1)e^{-\lambda x} &amp; &amp; x \geq 0<br>                \end{array}<br>            \right.$$</p>
<h4 id="9-2"><a href="#9-2" class="headerlink" title="9."></a>9.</h4><p>略。</p>
<h4 id="10-2"><a href="#10-2" class="headerlink" title="10."></a>10.</h4><p>略。</p>
<h4 id="11-2"><a href="#11-2" class="headerlink" title="11."></a>11.</h4><h5 id="a-31"><a href="#a-31" class="headerlink" title="(a)"></a>(a)</h5><p>因为$\sigma_X = 1, \mu_X = 0$，所以X服从标准正态分布。<br>所以我们根据标准正态分布表，可以得到： $P(X \leq 1.5) \approx 0.9332,\<br>             P(X leq -1) = 1 - \Phi(1) \approx 0.1587$。</p>
<h5 id="b-31"><a href="#b-31" class="headerlink" title="(b)"></a>(b)</h5><p>因为$\sigma_Y = 2, \mu_Y = 1$，而$Z = \frac{Y - 1}{2}$，所以<br>$\sigma_Z = 1, \mu_Z = 0$，所以<br>$f_Z(z) = \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}$。</p>
<h5 id="c-9"><a href="#c-9" class="headerlink" title="(c)"></a>(c)</h5><p>由(b)可知：$P(-1 \leq Y \leq 1) = P(-1 \leq Z \leq 0) =\ 0.5 - P(Z \leq -1) \approx 0.3413$。</p>
<h4 id="12-2"><a href="#12-2" class="headerlink" title="12."></a>12.</h4><p>$P(X \geq k\sigma) = 1 - P(X \leq k\sigma)$，令$Y = \frac{X}{\sigma}$，<br>则$P(X \geq k\sigma) = 1 - P(Y \leq k)$，于是：$P(X \geq \sigma) \approx 0.1587,<br>        P(X \geq 2\sigma) \approx 0.0228, P(X \geq 3\sigma) \approx 0.0013$。</p>
<p>由于$\mu = 0$，所以$P(|X| \leq k\sigma) = 1 - 2P(X \geq k\sigma)$，<br>所以$P(X \geq \sigma) \approx 0.6826,<br>        P(X \geq 2\sigma) \approx 0.9544, P(X \geq 3\sigma) \approx 0.9974$。</p>
<h4 id="13-2"><a href="#13-2" class="headerlink" title="13."></a>13.</h4><p>因为$C = \frac{F - 32}{1.8}$，所以$P(F \leq 59) = P(C \leq 15)$，<br>其中C满足正态分布，$\mu_C = 10, \sigma_C = 10$。</p>
<p>再令$X = \frac{C - \mu_C}{\sigma_C}$，则$P(F \leq 59) = P(X \leq 0.5) \approx 0.6915$。</p>
<h4 id="14-2"><a href="#14-2" class="headerlink" title="14."></a>14.</h4><p>略。</p>
<h4 id="15-2"><a href="#15-2" class="headerlink" title="15."></a>15.</h4><h5 id="i-1"><a href="#i-1" class="headerlink" title="(i)"></a>(i)</h5><p>$$f_{X,Y}(x, y) = \left{<br>                \begin{array}{lcr}<br>                    \frac{2}{\pi r} &amp; &amp; x^2 + y^2 \leq r, y &gt; 0\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="ii-1"><a href="#ii-1" class="headerlink" title="(ii)"></a>(ii)</h5><p>由(i)得：$f_Y(y) = \int_{-\sqrt{r - y^2}}^{\sqrt{r - y^2}}\frac{2}{\pi r}dx = \frac{4}{\pi r}\sqrt{r - y^2}$，<br>所以$E[Y] =\ \int_{0}^{\sqrt{r}}\frac{4y}{\pi r}\sqrt{r - y^2}dy = \frac{4\sqrt{2}}{3 \pi}$。</p>
<h5 id="iii-1"><a href="#iii-1" class="headerlink" title="(iii)"></a>(iii)</h5><p>利用期望规则直接积分：<br>$E[Y] = \int_{0}^{\sqrt{r}}ydy\int_{-\sqrt{r - y^2}}^{\sqrt{r - y^2}}\frac{2}{\pi r}dx = \frac{4\sqrt{2}}{3 \pi}$。</p>
<h4 id="16-2"><a href="#16-2" class="headerlink" title="16."></a>16.</h4><p>不妨设针的中点到最近的水平线的距离为X，到最近的垂直线的距离为Y，针与水平线的夹角为$\Theta$，<br>根据布丰抛针问题，我们可以得到： $$f_{X, Y, \Theta} = \left{<br>            \begin{array}{lcr}<br>                \frac{8}{ab\pi} &amp; &amp; x \in [0, \frac{b}{2}], y \in [0, \frac{a}{2}], \theta \in [0, \frac{\pi}{2}]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>所以针与水平线相交的概率为<br>$\P(X \leq \frac{l}{2}sin\Theta) = \frac{8}{ab\pi}\int_{0}^{\frac{a}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{l}{2}sin\theta}dxd\theta dy = \frac{2l}{b\pi}$。</p>
<p>同理，针与垂直线相交的概率为：<br>$\P(Y \leq \frac{l}{2}cos\Theta) = \frac{8}{ab\pi}\int_{0}^{\frac{b}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{l}{2}cos\theta}dyd\theta dx = \frac{2l}{a\pi}$。</p>
<p>所以相交边数的期望值为$\frac{2l}{b\pi}(1 - \frac{2l}{a\pi}) + \frac{2l}{a\pi}(1 - \frac{2l}{b\pi}) + 2\frac{2l}{a\pi}\frac{2l}{b\pi} = \frac{2l}{b\pi} + \frac{2l}{a\pi}$，<br>至少交于一条边的概率为$\frac{2l}{b\pi} + \frac{2l}{a\pi} - \frac{2l}{a\pi}\frac{2l}{b\pi}$。</p>
<h4 id="17-2"><a href="#17-2" class="headerlink" title="17."></a>17.</h4><p>略。</p>
<h4 id="18-2"><a href="#18-2" class="headerlink" title="18."></a>18.</h4><h5 id="a-32"><a href="#a-32" class="headerlink" title="(a)"></a>(a)</h5><p>$$E[X] = \int_{1}^{3}\frac{x^2}{4}dx = \frac{13}{6}$$<br>$$P(A) = 1 - P(X \leq 2) = 1 - \int_{1}^{2}\frac{x}{4}dx = \frac{5}{8}$$<br>$$f_{X|A}(x) = \left{<br>                \begin{array}{lcr}<br>                    \frac{2x}{5} &amp; &amp; 2 \leq x \leq 3\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$<br>$$E[X|A] = \int_{2}^{3}\frac{2x^2}{5}dx = \frac{38}{15}$$</p>
<h5 id="b-32"><a href="#b-32" class="headerlink" title="(b)"></a>(b)</h5><p>$$E[Y] = E[X^2] = \int_{1}^{3}\frac{x^3}{4}dx = 5$$<br>$$var(Y) = E[Y^2] - E[Y]^2 = \int_{1}^{3}\frac{x^5}{4}dx - 25 = \frac{16}{3}$$</p>
<h4 id="19-2"><a href="#19-2" class="headerlink" title="19."></a>19.</h4><h5 id="a-33"><a href="#a-33" class="headerlink" title="(a)"></a>(a)</h5><p>由归一性：$\int_{1}^{2}cx^{-2}dx = 1$，解得$c = 2$。</p>
<h5 id="b-33"><a href="#b-33" class="headerlink" title="(b)"></a>(b)</h5><p>$$P(A) = 1 - P(X \leq 1.5) = 1 - \int_{1}^{1.5}\frac{2dx}{x^2} = \frac{1}{3}$$<br>$$f_{X|A}(x) = \left{<br>                \begin{array}{lcr}<br>                    \frac{6}{x^2} &amp; &amp; 1.5 \leq x \leq 2\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="c-10"><a href="#c-10" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[Y|A] = E[X^2|A] = \int_{1.5}^{2}6dx = 3$$<br>$$var(Y|A) = E[Y^2|A] - E[Y|A]^2 = \frac{1}{4}$$</p>
<h4 id="20-2"><a href="#20-2" class="headerlink" title="20."></a>20.</h4><p>因为共同的分布为指数分布，且期望值为30，所以$f_X(x) = \frac{1}{30}e^{-\frac{x}{30}}$。<br>第二个学生到达时，第一个学生已经到达了5分钟，所以$P(X \geq 5) = 1 - \int_{0}^{5}f_X(x) = e^{-\frac{1}{6}}$。<br>所以第一个学生答疑时间的概率密度函数变为$f_{X|A}(x) = \frac{e^{\frac{1}{6}}}{30}e^{-\frac{x}{30}}$，<br>其期望值为$E[X|A] = \int_{5}^{\infty}\frac{e^{\frac{1}{6}}x}{30}e^{-\frac{x}{30}}dx = 35$，所以所花时间的期望值为65。</p>
<h4 id="21-2"><a href="#21-2" class="headerlink" title="21."></a>21.</h4><h5 id="a-34"><a href="#a-34" class="headerlink" title="(a)"></a>(a)</h5><p>$$f_{X, Y}(x, y) = \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{xl} &amp; &amp; y \leq x \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="b-34"><a href="#b-34" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_{Y}(y) = \left{<br>                \begin{array}{lcr}<br>                    \int_{y}^{l}\frac{dx}{xl} &amp; &amp; 0 \leq y \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<p>解之得： $$f_{Y}(y) = \left{<br>                \begin{array}{lcr}<br>                    \frac{lnl - lny}{l} &amp; &amp; 0 \leq y \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="c-11"><a href="#c-11" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[Y] = \int_{0}^{l}\frac{y(lnl - lny)}{l}dy = \frac{l}{4}$$</p>
<h5 id="d-5"><a href="#d-5" class="headerlink" title="(d)"></a>(d)</h5><p>$E[Y] = E[X\frac{Y}{X}] = E[X]E[\frac{Y}{X}] = \frac{l}{2}\frac{1}{2} = \frac{l}{4}$。</p>
<h4 id="22-2"><a href="#22-2" class="headerlink" title="22."></a>22.</h4><h5 id="i-2"><a href="#i-2" class="headerlink" title="(i)"></a>(i)</h5><p>设随机选取的两个点距离左侧的距离为X和Y，不妨认为X为更靠右侧的那个点，则概率密度函数为：<br>$$f_{X, Y}(x, y) = \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{x} &amp; &amp; 0 \leq y \leq x \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<p>由于三角形的两边之和大于第三边，我们可以列出满足条件的三个式子：<br>$$\begin{array}{lcr}<br>                Y + X - Y &amp; &gt; &amp; 1 - X\<br>                Y + 1 - X &amp; &gt; &amp; X - Y\<br>                X - Y + 1 - X &amp; &gt; &amp; Y\<br>            \end{array}$$</p>
<p>整理后得到： $$\begin{array}{lccrr}<br>                X &amp; &gt; &amp; \frac{1}{2} &amp; \rightarrow &amp; A\<br>                Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; B\<br>                X - Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; C\<br>            \end{array}$$</p>
<p>所以$P(A) = 1 - \int_{0}^{\frac{1}{2}}\int_{0}^{x}\frac{dydx}{x} = \frac{1}{2}$，<br>$P(B|A) = \int_{\frac{1}{2}}^{1}\int_{0}^{\frac{1}{2}}\frac{2}{x}dydx = ln2$，<br>$P(C|(B|A)) = \int_{\frac{1}{2}}^{1}\int_{x - \frac{1}{2}}^{\frac{1}{2}}\frac{dydx}{xln2} = 1 - \frac{1}{2ln2}$。</p>
<h5 id="ii-2"><a href="#ii-2" class="headerlink" title="(ii)"></a>(ii)</h5><p>本质上和(i)是一样的，仅仅只是选取方向的不同。</p>
<h5 id="iii-2"><a href="#iii-2" class="headerlink" title="(iii)"></a>(iii)</h5><p>设Y为第一次选取的点距离最近的一端的距离，X为第二次选区的点距离Y的距离，则概率密度函数为：<br>$$f_{X, Y}(x, y) = \left{<br>                \begin{array}{lcr}<br>                    \frac{2}{1 - y} &amp; &amp; 0 \leq y \leq \frac{1}{2}, y \leq x \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<p>同(i)，我们可以列出以下的条件（注意，此时的三段杆的距离已经变成了$X, Y, 1 - X - y$）：<br>$$\begin{array}{lccrr}<br>                X &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; A\<br>                Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; B\<br>                X + Y &amp; &gt; &amp; \frac{1}{2} &amp; \rightarrow &amp; C\<br>            \end{array}$$</p>
<p>所以$P(A) = \int_{0}^{\frac{1}{2}}\int_{y}^{\frac{1}{2}}\frac{2}{1 - y}dxdy = 1 - ln2$，<br>$P(B|A) = 1$，$P(C|(B|A)) = 2ln\frac{4}{3} + \frac{1}{2}ln\frac{3}{2} - \frac{1}{4}$。</p>
<h4 id="23-2"><a href="#23-2" class="headerlink" title="23."></a>23.</h4><h5 id="a-35"><a href="#a-35" class="headerlink" title="(a)"></a>(a)</h5><p>$$f_{X, Y}(x, y) = \left{<br>                \begin{array}{lcr}<br>                    2 &amp; &amp; x \geq 0, y \geq 0, x + y \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="b-35"><a href="#b-35" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_Y(y) = \int_{0}^{1 - y}2dx = 2(1 - y)$$</p>
<h5 id="c-12"><a href="#c-12" class="headerlink" title="(c)"></a>(c)</h5><p>$$f_{X|Y = y}(x) = \frac{1}{1 - y}$$</p>
<h5 id="d-6"><a href="#d-6" class="headerlink" title="(d)"></a>(d)</h5><p>$$E[X|Y = y] = \frac{1 - y}{2}$$<br>$$E[X] = \int_{0}^{1}E[X|Y = y]F_{Y}(y)dy = \int_{0}^{1}(1 - y)^2dy = \frac{1}{3}$$</p>
<h5 id="e-1"><a href="#e-1" class="headerlink" title="(e)"></a>(e)</h5><p>利用对称性，$E[Y] = E[X] = \frac{1}{3}$。</p>
<h4 id="24-2"><a href="#24-2" class="headerlink" title="24."></a>24.</h4><p>概率密度函数： $$f_{X, Y}(x, y) = \left{<br>            \begin{array}{lcr}<br>                1 &amp; &amp; x \geq 0, y \geq 0, 2x + y \leq 2\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>边缘概率密度函数： $$f_X(x) = \int_{0}^{-2x + 2}dy = -2x + 2$$<br>$$f_Y(y) = \int_{0}^{\frac{2 - y}{2}}dx = \frac{2 - y}{2}$$</p>
<p>条件期望： $$E[X|Y = y] = \frac{2 - y}{4}$$ $$E[Y|X = x] = 1 - x$$</p>
<p>所以由全期望定理可得：<br>$$E[X] = \int_{0}^{1}E[X|Y = y]f_Y(y)dy = \frac{5}{24}$$<br>$$E[Y] = \int_{0}^{2}E[Y|X = x]f_X(x)dx = \frac{4}{3}$$</p>
<h4 id="25-2"><a href="#25-2" class="headerlink" title="25."></a>25.</h4><p>$(X, Y)$离原点的距离至少为c，故$\sqrt{X^2 + Y^2} \geq c$，<br>所以$P(A) =\ P(\sqrt{X^2 + Y^2} \geq c) = 1 - \int_{0}^{2\pi}\int_{0}^{c}\frac{\rho}{2\pi \sigma^2}e^{\frac{-\rho^2}{2\sigma^2}}d\rho d\theta = e^{\frac{-c^2}{2\sigma^2}}$。</p>
<p>于是得到条件联合概率密度函数：<br>$$f_{(X, Y)|A} = \frac{1}{2\pi \sigma^2}e^{\frac{-x^2 - y^2 + c^2}{2\sigma^2}}$$</p>
<h4 id="26-2"><a href="#26-2" class="headerlink" title="26."></a>26.</h4><p>略。</p>
<h4 id="27-2"><a href="#27-2" class="headerlink" title="27."></a>27.</h4><h5 id="a-36"><a href="#a-36" class="headerlink" title="(a)"></a>(a)</h5><p>$$\int_{(x, y) \in A}f_{X,Y|C}(x, y)dxdy = \frac{1}{P(C)}\int_{(x, y) \in A}f_{X, Y}(x, y)dxdy = \frac{P(C)}{P(C)} = 1$$<br>所以$f_{X,Y|C}(x, y)$是一个合格的联合概率密度函数。</p>
<h5 id="b-36"><a href="#b-36" class="headerlink" title="(b)"></a>(b)</h5><p>令$C = \bigcup_{i = 1}^{n}C_i$，因为$A_i$是二维平面的分割，所以$C_i$也是$C$的一个分割。<br>由(a)可知：$f_{X, Y}(x, y) = P(C)f_{X, Y|C}(x, y)$，所以由全概率公式即可得到<br>$f_{X, Y}(x, y) = \sum_{i = 1}^{n}P(C_i)f_{X, Y|C_i}(x, y)$。</p>
<h4 id="28-2"><a href="#28-2" class="headerlink" title="28."></a>28.</h4><p>略。</p>
<h4 id="29-2"><a href="#29-2" class="headerlink" title="29."></a>29.</h4><p>略。</p>
<h4 id="30-2"><a href="#30-2" class="headerlink" title="30."></a>30.</h4><p>略。</p>
<h4 id="31-2"><a href="#31-2" class="headerlink" title="31."></a>31.</h4><p>略。</p>
<h4 id="32-2"><a href="#32-2" class="headerlink" title="32."></a>32.</h4><p>略。</p>
<h4 id="33-2"><a href="#33-2" class="headerlink" title="33."></a>33.</h4><p>略。</p>
<h4 id="34-2"><a href="#34-2" class="headerlink" title="34."></a>34.</h4><h5 id="a-37"><a href="#a-37" class="headerlink" title="(a)"></a>(a)</h5><p>不妨设硬币正面朝上为事件A，由连续的全期望定理：$P(A) =\ \int_{0}^{1}p^2e^pdp = e - 2$</p>
<h5 id="b-37"><a href="#b-37" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_{P|A}(p) = \left{<br>                \begin{array}{lcr}<br>                    \frac{pe^p}{e - 2} &amp; &amp; p \in A\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p>
<h5 id="c-13"><a href="#c-13" class="headerlink" title="(c)"></a>(c)</h5><p>由连续贝叶斯准则：<br>$f_{P|X}(p|x) = \frac{f_{P}(p)P(X = x|P = p)}{P(x)} = \frac{p^2e^p}{e - 2}$，<br>所以第二次抛掷正面朝上的条件概率为$\int_{0}^{1}\frac{p^3e^p}{e - 2}dp = \frac{6 - 2e}{e - 2}$</p>
<h4 id="35-2"><a href="#35-2" class="headerlink" title="35."></a>35.</h4><p>略。</p>
<h3 id="第四章-随机变量的深入内容"><a href="#第四章-随机变量的深入内容" class="headerlink" title="第四章 随机变量的深入内容"></a>第四章 随机变量的深入内容</h3><h4 id="1-5"><a href="#1-5" class="headerlink" title="1."></a>1.</h4><p>由题意得，$f_X(x) = \frac{1}{2}$，所以<br>$F_Y(y) = P(Y \leq y) = P(\sqrt{|X|} \leq y) = P(|X| \leq y^2)$。</p>
<p>$$F_Y(y) = \left{<br>            \begin{array}{lcr}<br>                y^2 &amp; &amp; y \in [0, 1]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>求导后即可得到： $$f_Y(y) = \left{<br>            \begin{array}{lcr}<br>                2y &amp; &amp; y \in [0, 1]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>因为$F_Z(z) = P(Z \leq z) = P(-ln|X| \leq z) = P(|X| \geq e^{-z})$</p>
<p>$$F_Z(z) = \left{<br>            \begin{array}{lcr}<br>                1 - e^{-z} &amp; &amp; z \in [0, \infty]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>所以 $$f_Z(z) = \left{<br>            \begin{array}{lcr}<br>                e^{-z} &amp; &amp; z \in [0, \infty]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<h4 id="2-5"><a href="#2-5" class="headerlink" title="2."></a>2.</h4><p>令$Y = e^X$，则$F_Y(y) = P(Y \leq y) = P(e^X \leq y) = P(X \leq lny)$，<br>所以$F_Y(y) = \int_{-\infty}^{lny}f_X(x)dx$，求导后$f_Y(y) = \frac{f_X(lny)}{y}$。</p>
<p>当X服从$[0, 1]$上的均匀分布时： $$f_Y(y) = \left{<br>            \begin{array}{lcr}<br>                \frac{1}{y} &amp; &amp; y \in [1, e]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<h4 id="3-3"><a href="#3-3" class="headerlink" title="3."></a>3.</h4><p>令$Y = |X|^{\frac{1}{3}}, Z = |X|^{\frac{1}{4}}$，则：<br>$F_Y(y) = P(Y \leq y) = P(|X|^{\frac{1}{3}} \leq y) = P(|X| \leq y^3) = \int_{0}^{y^3}f_{|X|}(|x|)dx$，<br>所以$f_Y(y) = 3y^2f_{|X|}(y^3)$，同理$f_Z(z) = 4z^3f_{|X|}(y^4)$。</p>
<h4 id="4-3"><a href="#4-3" class="headerlink" title="4."></a>4.</h4><p>因为$F_Y(y) = P(Y \leq y)$，当$0 \leq y \leq 5$时，由全概率公式，<br>$P(Y \leq y) = \frac{1}{4}P(X \leq y) + \frac{3}{4}P(X - 5 \leq y) = \frac{1}{4}P(X \leq y) + \frac{3}{4}P(X \leq y + 5)$，<br>所以$P(Y \leq y) = \frac{1}{4}(\int_{0}^{y}f_{X_A}(x)dx + 3\int_{0}^{y + 5}f_{X_B}(x)dx) = \frac{y}{10} + \frac{1}{4}$，故$f_Y(y) = \frac{1}{10}$；<br>同理当$5 &lt; y \leq 15$时，$P(Y \leq y) = \frac{3}{4}\int_{0}^{y + 5}f_{X_B}(x)dx = \frac{y + 5}{20}$，所以$f_Y(y) = \frac{1}{20}$。</p>
<h4 id="5-3"><a href="#5-3" class="headerlink" title="5."></a>5.</h4><p>$|z| = 0.5$的情况</p>
<p><img src="https://s3.ax1x.com/2021/02/15/y6GADf.jpg"></p>
<p>由于$X, Y$均服从均匀分布，两个随机变量的联合概率可以用以上的正方形区域面积表示。<br>考虑$|X - Y| \leq z$，这个式子可以被拆开为两个式子$X - Y \leq z$和$Y - X \leq z$，在图中做出这两个函数的图像。<br>不难发现$P(Z \leq z)$就是两条平行线与正方形所包围的图形的面积，即$P(Z \leq z) = 2z - z^2$，所以$f_Z(z) = 2 - 2z$。</p>
<h4 id="6-3"><a href="#6-3" class="headerlink" title="6."></a>6.</h4><p>使用和第五题类似的做法：$P(Z \leq z) = P(|X - Y| \leq z) = \frac{\sqrt{2}z}{2}(\sqrt{2}z + \frac{\sqrt{2}}{2}(1 - z) * 2) = \frac{z}{2}$，<br>所以$f_Z(z) = \frac{1}{2}$。</p>
<h4 id="7-3"><a href="#7-3" class="headerlink" title="7."></a>7.</h4><p>不难看出这题的随机变量就是第五题中的Z。所以$E[Z] = \int_{0}^{1}2z^2 - z^3dz = \frac{1}{3}$。</p>
<p>证毕。</p>
<h4 id="8-3"><a href="#8-3" class="headerlink" title="8."></a>8.</h4><p>$$f_Z(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)dx = \int_{0}^{z}\lambda^2e^{-\lambda x}e^{-\lambda (z - x)}dx = \lambda^2ze^{-\lambda z}$$</p>
<h4 id="9-3"><a href="#9-3" class="headerlink" title="9."></a>9.</h4><p>当$z \leq 0$， $$\begin{array}{l}<br>            F_Z(z) = P(X - Y \leq z) = 1 - P(X - Y &gt; z)\<br>            = 1 - \int_{0}^{\infty}\mu e^{-\mu y}dy\int_{z + y}^{\infty}\lambda e^{-\lambda x}dx\<br>            = 1 - \frac{\mu}{\lambda + \mu}e^{-\lambda z}<br>        \end{array}$$</p>
<p>当$z &lt; 0$时，$-z \leq 0$，所以$F_Z(z) = 1 - F_Z(-z) = \frac{\mu}{\lambda + \mu}e^{\lambda z}$。</p>
<h4 id="10-3"><a href="#10-3" class="headerlink" title="10."></a>10.</h4><p>由离散卷积公式$f_Z(z) = \sum f_X(x)f_Y(z - x)$可以求得：<br>$$p_Z(z) = \left{<br>            \begin{array}{lcr}<br>                \frac{1}{6} &amp; &amp; z = 1\<br>                \frac{5}{18} &amp; &amp; z = 2\<br>                \frac{1}{3} &amp; &amp; z = 3\<br>                \frac{1}{6} &amp; &amp; z = 4\<br>                \frac{1}{18} &amp; &amp; z = 5<br>            \end{array}<br>        \right.$$</p>
<h4 id="11-3"><a href="#11-3" class="headerlink" title="11."></a>11.</h4><p>设$P_X(k) = e^{-\lambda}\frac{\lambda^k}{k!}, P_Y(k) = e^{-\mu}\frac{\mu^k}{k!}$，<br>由卷积公式，有$P_Z(z) =\ \sum P_X(k)P_Y(z - k)$，因为$C_z^k = \frac{z!}{k!(z - k)!}$，<br>所以$P_Z(z) = \frac{e^{(-\lambda + \mu)}}{z!}\sum C_z^k \lambda^k \mu^{z - k}\ = e^{-(\lambda + \mu)}\frac{(\lambda + \mu)^z}{z!}$。</p>
<p>证毕。</p>
<h4 id="12-3"><a href="#12-3" class="headerlink" title="12."></a>12.</h4><p>先求$W = X + Y$的概率密度函数： $$f_W(w) = \left{<br>            \begin{array}{llccr}<br>                \int_{0}^{w}dx &amp; = &amp; w &amp; &amp; 0 \leq w \leq 1\<br>                \int_{w - 1}^{1}dx &amp; = &amp; 2 - w &amp; &amp; 1 &lt; w \leq 2\<br>                0 &amp; &amp; &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<p>再求$R = W + Z$： $$f_R(r) = \left{<br>            \begin{array}{llccr}<br>                \int_{0}^{r}wdw &amp; = &amp; \frac{r^2}{2} &amp; 0 \leq r \leq 1\<br>                \int_{r - 1}^{1}wdw + \int_{1}^{r}2-wdw &amp; = &amp; 3r - r^2 - \frac{3}{2} &amp; 1 &lt; r \leq 2\<br>                \int_{r - 1}^{2}2 - wdw &amp; = &amp; \frac{r^2}{2} - 3r + \frac{9}{2} &amp; 2 &lt; r \leq 3\<br>                0 &amp; &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<h4 id="13-3"><a href="#13-3" class="headerlink" title="13."></a>13.</h4><p>因为概率密度函数的对称轴为$\frac{a + b}{2}$，所以有$f(x) = f(a + b - x)$，<br>而$X + Y$和$X - Y$的积分区间正好差着$a + b$的距离，所以只需要将$f_{X + Y}$平移即可。</p>
<h4 id="14-3"><a href="#14-3" class="headerlink" title="14."></a>14.</h4><p>$$\begin{array}{l}<br>            P_Z(z) = P(min{X, Y} \leq z)\<br>            = 1 - P((min{X, Y} &gt; z)\<br>            = 1 - P(X &gt; z)P(Y &gt; z)\<br>            = 1 - (1 - P(X \leq z))(1 - P(Y \leq z))\<br>            = 1 - (1 - \int_{0}^{z}\lambda e^{-\lambda x}dx)(1 - \int_{0}^{z}\mu e^{-\mu y}dy)\<br>            = 1 - e^{-(\lambda + \mu)z}<br>        \end{array}$$</p>
<p>所以$Z = min{X, Y}$服从参数为$\lambda + \mu$的指数分布。</p>
<p>证毕。</p>
<h4 id="15-3"><a href="#15-3" class="headerlink" title="15."></a>15.</h4><p>略。</p>
<h4 id="16-3"><a href="#16-3" class="headerlink" title="16."></a>16.</h4><p>略。</p>
<h4 id="17-3"><a href="#17-3" class="headerlink" title="17."></a>17.</h4><p>因为$var(X) = var(Y)$，所以 $$\begin{array}{l}<br>            cov(X + Y, X - Y) = cov(X + Y, X) - cov(X + Y, Y)\<br>            = cov(X, X) + cov(Y, X) - cov(X, Y) - cov(Y, Y)\<br>            = cov(X, X) - cov(Y, Y)\<br>            = var(X) - var(Y)\<br>            = 0<br>        \end{array}$$</p>
<p>所以$X + Y, X - Y$不相关。</p>
<p>证毕。</p>
<h4 id="18-3"><a href="#18-3" class="headerlink" title="18."></a>18.</h4><p>由题意得：<br>$cov(W, X) = cov(W, Y) = cov(W, Z) = cov(X, Y) =\ cov(X, Z) = cov(Y, Z) = 0$，<br>所以： $$\begin{array}{l}<br>            \rho(R, S) = \frac{cov(R, S)}{\sqrt{var(R)var(S)}}\<br>            = \frac{cov(W + X, X + Y)}{\sqrt{var(W + X)var(X + Y)}}\<br>            = \frac{cov(W, X) + cov(W, Y) + cov(X, X) + cov(X, Y)}{\sqrt{(var(W) + var(X) + 2cov(W, X))(var(X) + var(Y) + 2cov(X, Y))}}\<br>            = \frac{var(X)}{\sqrt{(var(W) + var(X))(var(X) + var(Y)}}\<br>            = \frac{1}{2}<br>        \end{array}$$</p>
<p>同理$\rho(S, T) = \frac{1}{2}$</p>
<h4 id="19-3"><a href="#19-3" class="headerlink" title="19."></a>19.</h4><p>$$\begin{array}{l}<br>            \rho(X, Y) = \frac{cov(X, Y)}{\sqrt{var(X)var(Y)}}\<br>            = \frac{cov(X, a + bX + cX^2)}{\sqrt{var(X)var(a + bX + cX^2)}}\<br>            = \frac{cov(X, bX) + cov(X, cX^2)}{\sqrt{var(X)(var(a + bX) + var(cX^2) +2cov(a + bX, cX^2))}}\<br>            = \frac{b\times var(X) + c \times cov(X, X^2)}{\sqrt{var(X)(a^2var(X) + c^2var(X^2) + 2bc \times cov(X, X^2))}}\<br>            = \frac{b}{\sqrt{a + 2c^2}}<br>        \end{array}$$</p>
<h4 id="20-3"><a href="#20-3" class="headerlink" title="20."></a>20.</h4><p>略。</p>
<h4 id="21-3"><a href="#21-3" class="headerlink" title="21."></a>21.</h4><p>略。</p>
<h4 id="22-3"><a href="#22-3" class="headerlink" title="22."></a>22.</h4><p>设第i次赌博的收益比率为常量$R_i$，资产值为随机变量$X_i$，我们可以得到：</p>
<ul>
<li><p>  $E[X_1] = (pR_1 + 2(1 - p)^2)x$</p>
</li>
<li><p>  $E[X_2] = E[E[X_2|X_1]] = (pR_2 + 2(1 - p)^2)X_1$</p>
</li>
<li><p>  $E[X_3] = E[E[X_3|X_2]] = (pR_3 + 2(1 - p)^2)X_2$</p>
</li>
<li><p>  …</p>
</li>
</ul>
<p>综上：$E[X_n] = x\Pi_{i = 1}^n(pR_i + 2(1 - p)^2)$</p>
<h4 id="23-3"><a href="#23-3" class="headerlink" title="23."></a>23.</h4><h5 id="a-38"><a href="#a-38" class="headerlink" title="(a)"></a>(a)</h5><p>当$X \leq 1$时，纳特的等待时间为0；当$X &gt; 1$时，等待时间的期望值为$\int_{1}^{2}\frac{x - 1}{2}dx = \frac{1}{4}$，<br>所以总的等待时间的期望值为15分钟。</p>
<h5 id="b-38"><a href="#b-38" class="headerlink" title="(b)"></a>(b)</h5><p>当$X \leq 1$时，约会的时长是3小时；当$X &gt; 1$时，约会时间的期望值为$E[Y] = E[E[Y|X]]$，<br>因为$E[Y|X] = \frac{3 - X}{2}$，所以$E[Y] = \frac{3}{2} - \frac{E[X]}{2} = 1$。<br>所以总的约会时间的期望值为2小时。</p>
<h5 id="c-14"><a href="#c-14" class="headerlink" title="(c)"></a>(c)</h5><p>TODO:</p>
<h4 id="24-3"><a href="#24-3" class="headerlink" title="24."></a>24.</h4><h5 id="a-39"><a href="#a-39" class="headerlink" title="(a)"></a>(a)</h5><p>由重期望法则： $$\begin{array}{l}<br>                E[X] = E[E[X|Y]] \<br>                = E[5 - Y] \<br>                = 5 - E[Y] \<br>                = 3<br>            \end{array}$$</p>
<h5 id="b-39"><a href="#b-39" class="headerlink" title="(b)"></a>(b)</h5><p>$E[X + Y] = E[X] + E[Y] = 3 + 2 = 5$小时，所以是下午两点。</p>
<h5 id="c-15"><a href="#c-15" class="headerlink" title="(c)"></a>(c)</h5><p>TODO:</p>
<h4 id="25-3"><a href="#25-3" class="headerlink" title="25."></a>25.</h4><p>略。</p>
<h4 id="26-3"><a href="#26-3" class="headerlink" title="26."></a>26.</h4><p>略。</p>
<h4 id="27-3"><a href="#27-3" class="headerlink" title="27."></a>27.</h4><p>略。</p>
<h4 id="28-3"><a href="#28-3" class="headerlink" title="28."></a>28.</h4><p>略。</p>
<h4 id="29-3"><a href="#29-3" class="headerlink" title="29."></a>29.</h4><p>$$\begin{array}{l}<br>            M(s) = \sum_{x}e^{sx}p_X(x)\<br>            = e^s \times \frac{1}{2} + e^{2s} \times \frac{1}{4} + e^{3s} \times \frac{1}{4}\<br>        \end{array}$$</p>
<p>所以：</p>
<ul>
<li><p>  $E[X] = \frac{d}{ds}M(s)|_{s = 0} = \frac{7}{4}$</p>
</li>
<li><p>  $E[X^2] = \frac{d^2}{ds^2}M(s)|_{s = 0} = \frac{15}{4}$</p>
</li>
<li><p>  $E[X^3] = \frac{d^3}{ds^3}M(s)|_{s = 0} = \frac{37}{4}$</p>
</li>
</ul>
<h4 id="30-3"><a href="#30-3" class="headerlink" title="30."></a>30.</h4><p>标准正态分布$f_X(x) = \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}$，<br>其矩母函数为 $$\begin{array}{l}<br>            M(s) = \int_{-\infty}^{\infty}e^{sx}f_X(x)dx\<br>            = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{\frac{-x^2}{2} + sx}dx\<br>            = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{x^2 - 2sx + s^2 - s^2}{2}}dx\<br>            = \frac{e^{\frac{s^2}{2}}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{(x - s)^2}{2}}dx\<br>            = e^{\frac{s^2}{2}}<br>        \end{array}$$</p>
<p>所以$E[X^3] = 0, E[X^4] = 3$。</p>
<h4 id="31-3"><a href="#31-3" class="headerlink" title="31."></a>31.</h4><p>$M(s) = \int_{0}^{\infty}\lambda e^{-\lambda x + sx}dx = \frac{-\lambda}{s - \lambda}$，<br>所以$E[X^3] = \frac{6}{\lambda^4}, E[X^4] = \frac{24}{\lambda^5},\ E[X^5] = \frac{120}{\lambda^6}$</p>
<h4 id="32-3"><a href="#32-3" class="headerlink" title="32."></a>32.</h4><h5 id="a-40"><a href="#a-40" class="headerlink" title="(a)"></a>(a)</h5><p>第二个不是矩母函数。根据矩母函数的定义$M_X(s) = E[e^{sX}]$,$M_X(0) = E[1] = 1$，<br>很明显第二个函数不满足这个条件。</p>
<h5 id="b-40"><a href="#b-40" class="headerlink" title="(b)"></a>(b)</h5><p>设N为满足$\lambda = 2$的泊松分布的随机变量，Y为满足$\lambda = 1$的泊松分布的随机变量，<br>所以X为N个Y之和（见4.5节），$P(X = 0) = \sum_{n = 0}^{\infty}p_N(n)p_Y(0)\ = e^{-1}$</p>
<h4 id="33-3"><a href="#33-3" class="headerlink" title="33."></a>33.</h4><p>不难看出加号两侧是指数分布的矩母函数的形式，所以概率密度函数：<br>$$f_X(x) = \left{<br>            \begin{array}{lcr}<br>                \frac{1}{3} \times 2e^{-2x} + \frac{2}{3} \times 3e^{-3x} &amp; &amp; x \geq 0\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p>
<h4 id="34-3"><a href="#34-3" class="headerlink" title="34."></a>34.</h4><p>利用卷积计算的分布列： $$p(x) = \left{<br>            \begin{array}{lcr}<br>                1 - p_1 - p_2 - p_3 + p_1p_2 + p_1p_3 + p_2p_3 - p_1p_2p_3 &amp; &amp; x = 0\<br>                3p_1p_2p_3 + p_1 + p_2 + p_3 - 2(p_1p_2 + p_1p_3 + p_2p_3) &amp; &amp; x = 1\<br>                p_1p_2 + p_1p_3 + p_2p_3 - 3p_1p_2p_3 &amp; &amp; x = 2\<br>                p_1p_2p_3 &amp; &amp; x = 3<br>            \end{array}<br>        \right.$$</p>
<p>对于某一个球员，其矩母函数为$M_{X_i}(s) = 1 - p_i + p_ie^s$，<br>又因为独立随机变量的和的矩母函数是和项的矩母函数的乘积，<br>所以$M_X(s) = \Pi_{i = 1}^{3}(1 - p_i + p_ie^s)$，展开后去掉$e^{sk}$，<br>形式与上面的分布列是一样的。</p>
<h4 id="35-3"><a href="#35-3" class="headerlink" title="35."></a>35.</h4><p>由$M(0) = 1$，可以计算得$c = \frac{2}{9}$，所以$E[X] = \frac{d}{ds}M(s)|_{s = 0} = \frac{37}{18}$；<br>令$\alpha = \frac{e^s}{3}$，那么$M(s) = \frac{c(3 + 4e^2 + 2e^3)}{3(1 - \alpha)} = \frac{2}{27}(3 + 4e^2 + 2e^3)(1 + \alpha + \alpha^2 + \dots)$，<br>所以$p_X(1) = \frac{2}{27}, p_X(0) = \frac{2}{9}$，由条件期望：$E[X|X \ne 0] = \frac{E[X]}{1 - p_X(0)} = \frac{37}{14}$。</p>
<h4 id="36-2"><a href="#36-2" class="headerlink" title="36."></a>36.</h4><h5 id="a-41"><a href="#a-41" class="headerlink" title="(a)"></a>(a)</h5><p>由题意得：$M_{XY}(s) = \frac{1}{3}M_Y(s) = \frac{1}{3}\frac{s}{2 - s}$，<br>$M_{(1 - X)Z} = \frac{2}{3}M_Z(s) = \frac{2}{3}e^{3(e^s - 1)}$，所以$M_{U}(s) = \frac{2s}{9(2 - s)}e^{3(e^s - 1)}$。</p>
<h5 id="b-41"><a href="#b-41" class="headerlink" title="(b)"></a>(b)</h5><p>$M_{2Z + 3}(s) = M_{2Z}(s)M_{3}(s) = e^{3(e^{2s} - 1) + 3s}$</p>
<h5 id="c-16"><a href="#c-16" class="headerlink" title="(c)"></a>(c)</h5><p>$M_{Y + Z}(s) = \frac{s}{2 - s}e^{3(e^s - 1)}$</p>
<h4 id="37-2"><a href="#37-2" class="headerlink" title="37."></a>37.</h4><p>TODO:</p>
<h4 id="38-2"><a href="#38-2" class="headerlink" title="38."></a>38.</h4><p>略。</p>
<h4 id="39-2"><a href="#39-2" class="headerlink" title="39."></a>39.</h4><p>略。</p>
<h4 id="40-2"><a href="#40-2" class="headerlink" title="40."></a>40.</h4><p>略。</p>
<h4 id="41-2"><a href="#41-2" class="headerlink" title="41."></a>41.</h4><h5 id="a-42"><a href="#a-42" class="headerlink" title="(a)"></a>(a)</h5><p>因为$X_i$服从$[0, 1]$上的均匀分布，所以$M_X(s) = \frac{e^s - 1}{s}$，<br>由于人数服从泊松分布，所以$M_N(s) = e^{\lambda(e^s - 1)}$，将$e^s$替换后得到<br>$M_Y(s) = e^{\lambda(\frac{e^s - 1}{s} - 1)}$</p>
<h5 id="b-42"><a href="#b-42" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)中的矩母函数可得：$E[Y] = \frac{d}{ds}M_Y(s)|_{s = 0} = \frac{\lambda}{2}$</p>
<h5 id="c-17"><a href="#c-17" class="headerlink" title="(c)"></a>(c)</h5><p>由重期望法则：$E[Y] = E[E[Y|N]] = E[X]E[N] = \frac{\lambda}{2}$，与(b)中的结果一致。</p>
<h4 id="42-2"><a href="#42-2" class="headerlink" title="42."></a>42.</h4><p>个数服从参数为$\lambda$的泊松分布的情况下，<br>$M_X(s) = e^{\lambda(e^{\frac{\sigma^2s^2}{2}} + \mu s) - 1}$，<br>显然不满足正态分布的矩母函数的形式。</p>
<h4 id="43-2"><a href="#43-2" class="headerlink" title="43."></a>43.</h4><h5 id="a-43"><a href="#a-43" class="headerlink" title="(a)"></a>(a)</h5><p>由全概率定理： $P(X \leq x) =<br>            \frac{1}{16}(1 + 4\int_{0}^{x}f_Y(y)dy + 6\int_{0}^{x}f_{2Y}(y)dy + 4\int_{0}^{x}f_{3Y}(y)dy + \int_{0}^{x}f_{4Y}(y)dy)$<br>混合分布的对应的矩母函数为：<br>$M_X(s) = \frac{1}{16}(1 + 4e^{\frac{s^2}{8} + s} + 6e^{\frac{s^2}{4} + 2s} + 4e^{\frac{3s^2}{8} + 3s} + e^{\frac{s^2}{2} + 4s})$，<br>所以超过4分钟的概率为$1 - P(X \leq 4) \approx 0.06$。X不是正态的。</p>
<h5 id="b-43"><a href="#b-43" class="headerlink" title="(b)"></a>(b)</h5><p>二项分布的矩母函数：$M_N(s) = (\frac{1}{2} + \frac{1}{2}e^s)^4$，用正态分布的矩母函数替换：<br>$M_X(s) = \frac{1}{16}(1 + e^{\frac{s^2}{8}} + s)^4$，展开后与(a)中结果一致。</p>
<h4 id="44-2"><a href="#44-2" class="headerlink" title="44."></a>44.</h4><h5 id="a-44"><a href="#a-44" class="headerlink" title="(a)"></a>(a)</h5><p>$E[N] = E[M]E[K], var(N) = E[M]var(K) + E[K]^2var(M)$</p>
<h5 id="b-44"><a href="#b-44" class="headerlink" title="(b)"></a>(b)</h5><p>$E[Y] = E[X]E[N] = E[X]E[M]E[K], var(Y) = E[N]var(X) +\ E[X]^2var(N) = E[M]E[K]var(X) + E[X]^2(E[M]var(K) + E[K]^2var(M))$</p>
<h5 id="c-18"><a href="#c-18" class="headerlink" title="(c)"></a>(c)</h5><p>由题意得：$E[K] = \mu, E[X] = \frac{1}{\lambda}, E[M] = \frac{1}{p}, var(K) = \mu, var(X) = \frac{1}{\lambda^2}, var(M) = \frac{1 - p}{p^2}$，<br>由(b)中的公式，我们可以得到：$E[Y] = \frac{\mu}{p\lambda}, var(Y) = \frac{\mu}{p\lambda^2} + \frac{\mu p + \mu^2 - \mu^2p}{\lambda^2p^2}$</p>
<h4 id="45-2"><a href="#45-2" class="headerlink" title="45."></a>45.</h4><p>略。</p>
<h3 id="第五章-极限理论"><a href="#第五章-极限理论" class="headerlink" title="第五章 极限理论"></a>第五章 极限理论</h3><h4 id="1-6"><a href="#1-6" class="headerlink" title="1."></a>1.</h4><h5 id="a-45"><a href="#a-45" class="headerlink" title="(a)"></a>(a)</h5><p>因为$\sigma = 1, var(M_n) = \frac{\sigma^2}{n}$，<br>要求$var(M_n) = \frac{\sigma^2}{n} \leq 0.0001$， 所以$n \geq 10000$。</p>
<h5 id="b-45"><a href="#b-45" class="headerlink" title="(b)"></a>(b)</h5><p>由切比雪夫不等式：<br>$P(|X - h| \geq 0.05) \leq \frac{1^2}{0.05^2n} \leq 0.01$，<br>解之得$n \geq 40000$。</p>
<h5 id="c-19"><a href="#c-19" class="headerlink" title="(c)"></a>(c)</h5><p>令$\sigma = \frac{(b - a)}{2} = 0.3$，<br>则(a)中的不等式变为$\frac{0.09}{n} \leq 0.0001$，解之得$n \geq 900$；<br>(b)中的不等式变为$\frac{0.09}{0.0025n} \leq 0.01$，解之得$n \geq 3600$。</p>
<h4 id="2-6"><a href="#2-6" class="headerlink" title="2."></a>2.</h4><p>略。</p>
<h4 id="3-4"><a href="#3-4" class="headerlink" title="3."></a>3.</h4><p>略。</p>
<h4 id="4-4"><a href="#4-4" class="headerlink" title="4."></a>4.</h4><p>由弱大数定律和切比雪夫不等式，我们可以得到：<br>$P(|M_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} = \delta$：</p>
<h5 id="a-46"><a href="#a-46" class="headerlink" title="(a)"></a>(a)</h5><p>当$\epsilon$缩小为原来的一半时，为了维持不等号不变，n至少要扩大到原来的4倍。</p>
<h5 id="b-46"><a href="#b-46" class="headerlink" title="(b)"></a>(b)</h5><p>当$\delta$缩小为原来的一半时，为了维持不等号不变，n至少要扩大到原来的2倍。</p>
<h4 id="5-4"><a href="#5-4" class="headerlink" title="5."></a>5.</h4><p>由题意得：$f_{X_i}(x) = \left{<br>            \begin{array}{lcr}<br>                \frac{1}{2} &amp; &amp; x \in [-1, 1]\<br>                 0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$</p>
<h5 id="a-47"><a href="#a-47" class="headerlink" title="(a)"></a>(a)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                = \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                = P(0 \geq \epsilon)\<br>                = 0<br>            \end{array}$$</p>
<p>利用夹逼定理，容易得出极限值为0。</p>
<h5 id="b-47"><a href="#b-47" class="headerlink" title="(b)"></a>(b)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                = \lim_{n \to \infty}P(|X_n^n| \geq \epsilon)\<br>                = \lim_{n \to \infty}2P(X_n \geq \epsilon^n)\<br>                = \lim_{n \to \infty}2P(X_n \geq 1)\<br>                = 0<br>            \end{array}$$</p>
<p>不难看出$Y_n$的极限不存在。</p>
<h5 id="c-20"><a href="#c-20" class="headerlink" title="(c)"></a>(c)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                = \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                = P(0 \geq \epsilon)\<br>                = 0<br>            \end{array}$$</p>
<p>$Y_n$的极限为0。</p>
<h5 id="d-7"><a href="#d-7" class="headerlink" title="(d)"></a>(d)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                = \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                = \lim_{n \to \infty}P(max{X_1, X_2, \dots X_n}) \geq \epsilon)\<br>                = \lim_{n \to \infty}(\frac{1 - \epsilon}{2})^n\<br>                = 0<br>            \end{array}$$</p>
<p>$Y_n$的极限为1。</p>
<h4 id="6-4"><a href="#6-4" class="headerlink" title="6."></a>6.</h4><p>略。</p>
<h4 id="7-4"><a href="#7-4" class="headerlink" title="7."></a>7.</h4><p>略。</p>
<h4 id="8-4"><a href="#8-4" class="headerlink" title="8."></a>8.</h4><p>我们设第i次转动的结果为$X_i$，当转到奇数的时候值为1，偶数时为0。<br>如果轮盘公正，那么$\mu = 0.5, \sigma^2 = 0.25$，<br>则由中心极限定理，转到奇数的次数大于55次的概率为<br>$P(Z_n &gt; x) = P(Z_n &gt; \frac{55 - 100\mu}{10\sigma}) = 1 - P(Z_n \leq 1) = 2 - \Phi(1) \approx 0.1587$</p>
<h4 id="9-4"><a href="#9-4" class="headerlink" title="9."></a>9.</h4><h5 id="a-48"><a href="#a-48" class="headerlink" title="(a)"></a>(a)</h5><p>设第i天内是否死机为$X_i$，当死机发生时记为1，没有发生记为0。<br>由题意得$\mu = 0.95, \sigma^2 = 0.0475$，<br>所以由中心极限定理，至少有45天没有死机的概率为<br>$P(Z_n &gt; \frac{45 - 50\mu}{\sqrt{50}\sigma}) \approx \Phi(1.62) = 0.9474$</p>
<h5 id="b-48"><a href="#b-48" class="headerlink" title="(b)"></a>(b)</h5><p>由题意得：$\lambda = np = 47.5$，<br>那么$P(Z_n \leq 5) = \sum_{k = 0}^{5}e^{-\lambda}\frac{\lambda^k}{k!} \approx 1$</p>
<h4 id="10-4"><a href="#10-4" class="headerlink" title="10."></a>10.</h4><h5 id="a-49"><a href="#a-49" class="headerlink" title="(a)"></a>(a)</h5><p>$P(Z_n &gt; x) = P(Z_n &gt; \frac{440 - 100\mu}{90}) = 1 - P(Z_n \leq -0.667) = \Phi(0.667) \approx 0.7454$</p>
<h5 id="b-49"><a href="#b-49" class="headerlink" title="(b)"></a>(b)</h5><p>由题意得：$P(X_1 + \dots + X_n \leq 200 + 5n) = P(X_1 + \dots + X_n \leq \frac{200}{3\sqrt{n}}) \geq 0.95$，<br>所以$\frac{200}{3\sqrt{n}} \geq 1.65$，解得n的最大值为1632。</p>
<h5 id="c-21"><a href="#c-21" class="headerlink" title="(c)"></a>(c)</h5><p>若恰好220天生产到第1000个，则每天平均需要生产约4.55个；<br>若219天就可以生产到第1000个，则每天平均需要生产约4.57个。<br>所以，由中心极限定理，我们有 $P(N \geq 220) \approx<br>            P(4.55 \times 220 \leq S_n \leq 4.57 \times 220) \approx<br>            \Phi(-2.12) - (1 - \Phi(2.22)) = \Phi(2.22) - \Phi(2.12) \approx 0.0038$</p>
<h4 id="11-4"><a href="#11-4" class="headerlink" title="11."></a>11.</h4><p>不妨令$W_i = X_i - Y_i$，由于$X_i, Y_i$都是$[0, 1]$上的均匀分布，<br>所以$E[W_i] = E[X_i] - E[Y_i] = 0,<br>        var(W_i) = var(X_i) + var(Y_i) + 2cov(X_i + Y_i) = 1$。<br>由中心极限定理，原式$P(|W - E[W]| &lt; 0.001) = P(|W| \leq 0.001)<br>        = 2P(W \leq 0.001) \approx 2\Phi(0) = 1$。</p>
<h4 id="12-4"><a href="#12-4" class="headerlink" title="12."></a>12.</h4><p>略。</p>
<h4 id="13-4"><a href="#13-4" class="headerlink" title="13."></a>13.</h4><p>略。</p>
<h4 id="14-4"><a href="#14-4" class="headerlink" title="14."></a>14.</h4><p>略。</p>
<h4 id="15-4"><a href="#15-4" class="headerlink" title="15."></a>15.</h4><p>略。</p>
<h4 id="16-4"><a href="#16-4" class="headerlink" title="16."></a>16.</h4><p>略。</p>
<h4 id="17-4"><a href="#17-4" class="headerlink" title="17."></a>17.</h4><p>略。</p>
<h4 id="18-4"><a href="#18-4" class="headerlink" title="18."></a>18.</h4><p>略。</p>
]]></content>
  </entry>
  <entry>
    <title>《概率导论》学习笔记(持续更新中)</title>
    <url>/2021/20210122/</url>
    <content><![CDATA[<p>开始时间：2021/01/22</p>
<span id="more"></span>

<h3 id="第一章-样本空间与概率"><a href="#第一章-样本空间与概率" class="headerlink" title="第一章 样本空间与概率"></a>第一章 样本空间与概率</h3><p>**样本空间$\Omega$**：这是一个试验的所有可能结果的集合.</p>
<p><strong>概率律</strong>：概率律为试验结果的集合$A$(称为<strong>事件</strong>)确定一个非负数$P(A)$(称为事件$A$的概率).</p>
<p>概率律满足以下几条公理(<strong>概率公理</strong>)：</p>
<blockquote>
<p>(1)**(非负性)<strong>对一切事件$A$,满足$P(A)\geq 0$.<br>(2)</strong>(可加性)<strong>设$A$和$B$为两个互不相交的集合(概率论中称为互不相容的事件),则它们的并满足$P(A\cup B)=P(A)+P(B)$.<br>(3)</strong>(归一化)**整个样本空间$\Omega$(称为必然事件)的概率为1,即$P(\Omega)=1$.</p>
</blockquote>
<p>由概率公理可导出处许多性质,下面为部分性质</p>
<blockquote>
<p><strong>概率论的若干性质：</strong><br>考虑一个概率律,令$A、B、C$为事件.<br>(a)若$A\subset B$,则$P(A)\leq P(B)$.<br>(b)$P(A\cup B)=P(A)+P(B)-P(A\cap B)$.<br>(c)$P(A\cup B)\leq P(A)+P(B)$.<br>(d)$P(A\cup B\cup C)=P(A)+P(A^c\cap B)+P(A^c\cap B\cap C)$.</p>
</blockquote>
<p>条件概率：给定的事件$B$发生了,另一个给定的事件$A$发生的概率,记为$P(A \mid B)=\frac{P(A\cap B)}{P(B)}$.<br>显然地,条件概率是一个概率律,其满足概率律的三条公理.</p>
<p><strong>相关定理:</strong></p>
<blockquote>
<p><strong>乘法法则：</strong><br>假定所有涉及的条件概率都是正的,我们有:</p>
<p>$$<br>P(\cap _{i=1}^n A_i)=<br>P(A_1)P(A_2 \mid A_1)P(A_3 \mid A_1\cap A_2)…P(A_n \mid \cap _{i=1}^{n-1} A_i)<br>$$</p>
</blockquote>
<blockquote>
<p><strong>全概率定理：</strong><br>设$A_1,A_2,…A_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个发生).又假定对每一个$i$,$P(A_i)&gt;0$.则对于任何事件$B$,下列公式成立<br>$$<br>P(B)=P(A_1\cap B)+…+P(A_n\cap B)</p>
<p>=P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n).<br>$$</p>
</blockquote>
<blockquote>
<p><strong>关于条件概率的全概率公式：</strong><br>设$C_1,C_2,…C_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个发生).令A和B是两个事件，满足$P(B\cap C_i)&gt;0$对一切$i$成立，则：<br>$$<br>P(A\mid B)=\sum_{i=1}^n P(C_i\cap B)+…+P(A\mid B\cap C_i)<br>$$</p>
</blockquote>
<blockquote>
<p><strong>贝叶斯准则</strong><br>设$A_1,A_2,…A_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个事件发生).又假定对每一个$i$,$P(A_i)&gt;0.$则对于任何事件$B$,只要它满足$P(B)&gt;0$,下列公式成立:<br>$$<br>P(A_i \mid B)=\frac{P(A_i)P(B \mid A_i)}{P(B)}<br>$$</p>
<p>$$<br>=\frac{P(A_i)P(B \mid A_i)}{P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n)}.<br>$$</p>
<p>$=\frac{P(A_i)P(B \mid A_i)}{P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n)}$.</p>
</blockquote>
<p><strong>独立性:</strong></p>
<ul>
<li><p>当等式$P(A\cap B)=P(A)P(B)$成立时,我们称$A$和$B$是相互独立的事件.<br>若$B$还满足$P(B)&gt;0$,则独立性等价于$P(A \mid B)=P(A)$.</p>
</li>
<li><p>$A$与$B$相互独立,则$A$与$B^c$也相互独立</p>
</li>
<li><p>设事件$C$满足$P(C)&gt;0$,两个事件$A$和$B$称为给定$C$的条件下条件独立,如果它们满足$P(A\cap B \mid C)=P(A \mid C)P(B \mid C)$.<br>若进一步假定$P(B\cap C)&gt;0$,则$A$和$B$在给定$C$的条件下的条件独立性和以下条件是等价的：$P(A \mid B\cap C)=P(A \mid C)$.</p>
</li>
<li><p>独立性并不蕴含条件独立性,反之亦然.</p>
</li>
<li><p><strong>几个事件的相互独立性的定义</strong>：<br>设$A_1,…A_n$为n个事件,<br>若它们满足<br>:<br>$$<br>P(\cup_{i\in S}A_i)=\prod_{i\in S}P(A_i)对{1,2,3…n}的任意子集S成立<br>$$<br>则称$A_1,…A_n$为相互独立的事件.</p>
</li>
</ul>
<p><strong>德摩根公式：</strong>  $(A\cup B)^c=A^c\cap B^c,(A\cap B)^c=A^c\cup B^c$<br><strong>邦费罗尼不等式(Bonferroni’s inequality):</strong><br>$$<br>P(A\cap B)\geq P(A)+P(B)-1<br>$$<br>可推广到n个事件$A_1,A_2…A_n$的情况：<br>$$<br>P(A\cap B\cap…\cap A_n)\geq P(A)+P(B)+…+P(A_n)-(n-1)<br>$$</p>
<p><strong>容斥原理：</strong><br>设$A_1,A_2…A_n$为n个事件.记$S_1=\left{i\mid 1\geq i\geq n\right},S_2=\left{(i_1,i_2)\mid 1\leq i_1\leq  i_2 &lt; n\right}$…<br>$$<br>P(\cup_{k=1}^n)<br>=\sum_{i\in S_1}P(A_i)-\sum_{(i_1,i_2)\in S_2}P(A_{i_1}\cap A_{i_2})<br>+\sum_{(i_1,i_2,i_3)\in S_3}P(A_{i_1}\cap A_{i_2}\cap A_{i_3})<br>-…+(-1)^{n-1}P(\cap_{k=1}^n A_k)<br>$$</p>
<h3 id="第二章-离散随机变量"><a href="#第二章-离散随机变量" class="headerlink" title="第二章 离散随机变量"></a>第二章 离散随机变量</h3><p><strong>与随机变量相关的主要概念</strong></p>
<p>在一个试验的概率模型之下：</p>
<ul>
<li><p><strong>随机变量</strong>是试验结果的实值函数；</p>
</li>
<li><p><strong>随机变量的函数</strong>定义了另一个随机函数；</p>
</li>
<li><p>对于一个随机变量,我们可以定义一些平均量,例如<strong>均值</strong>和<strong>方差</strong>；</p>
</li>
<li><p>可以在某事件或某随机变量的<strong>条件</strong>之下定义一个随机变量；</p>
</li>
<li><p>存在一个随机变量与某事件或某随机变量相互<strong>独立</strong>的概念；</p>
</li>
</ul>
<p><strong>与离散随机变量相关的主要概念</strong></p>
<p>在一个试验的概率模型之下：</p>
<ul>
<li><p><strong>离散随机变量</strong>是试验结果的一个实值函数,但是它的取值范围只能是有限多个值或可数无限多个值；</p>
</li>
<li><p>一个离散随机变量有一个<strong>分布列</strong>,它对于随机变量的每一个取值,给出一个概率；</p>
</li>
<li><p><strong>离散随机变量的函数</strong>也是一个离散随机变量,它的分布列可以从原随机变量的分布列得到.</p>
</li>
</ul>
<p><strong>一些分布列：</strong></p>
<blockquote>
<p><strong>二项随机变量</strong>:将一枚硬币抛掷n次,每次抛掷,正面出现概率为p,反面出现的概率为1-p,而且每次抛掷是相互独立的.令X为n次抛掷得到正面的次数.我们称X为<strong>二项随机变量</strong>.<br>$$<br>P(X=k)=\binom{n}{k}p^k(1-p)^{n-k},k=0,1,…n.<br>$$<br><strong>几何随机变量：</strong>在上述抛硬币试验中,令X为连续地抛掷一枚硬币,直到第一次正面所需要抛掷的次数.<br>$$<br>P(X=k)=(1-p)^{k-1}p,k=1,2…<br>$$<br><strong>泊松随机变量：</strong>设随机变量X的分布列由下式给出：</p>
<p>$$<br>P(X=k)=e^{-\lambda }\frac{\lambda ^k}{k!},k=0,1,2…<br>$$</p>
</blockquote>
<p><strong>期望:</strong></p>
<p>设随机变量X的分布列为p{X},X的<strong>期望值</strong>(也称<strong>期望</strong>或<strong>均值</strong>)由下式给出：<br>$$<br>E[X]=\sum_{x} xp_{X}(x).<br>$$<br><strong>方差：</strong>(记作$var(X)$)</p>
<p>随机变量$X$的方差由下列公式所定义：</p>
<p>$$<br>var(X)=E[(X-E[x])^2].<br>$$<br>并且可以用下式进行计算：</p>
<p>$$<br>var(X)=\sum_{x}(x-E[X])^2p_{X}(x)<br>$$</p>
<p>$$<br>var(X)=E[X^2]-(E[X])^2<br>$$</p>
<p>它是非负的,其平方根称为<strong>标准差</strong></p>
<p><strong>随机变量的线性函数的均值与方差：</strong></p>
<p>设$X$为随机变量,令$Y=aX+b$,其中a和b为给定的常数,则</p>
<p>$$<br>E[Y]=aE[X]+b,var(Y)=a^2var(X)<br>$$</p>
<p><strong>某些常用的随机变量的均值及方差：</strong></p>
<blockquote>
<p><strong>伯努利随机变量：</strong>$E[X]=p$.$var(X)=p(1-p)$</p>
<p><strong>泊松随机变量：</strong>$E[X]=\lambda$.$var(X)=\lambda$.</p>
</blockquote>
<p><strong>关于联合分布列：</strong></p>
<p>设$X$和$Y$为在某个试验中的随机变量.</p>
<ul>
<li>$X$和$Y$的联合分布列$p_{X,Y}$由下式定义：</li>
</ul>
<p>$$<br>p_{X,Y}(x,y)=P(X=x,Y=y)<br>$$</p>
<ul>
<li>$X$和$Y$的边缘分布列可由下式得到：</li>
</ul>
<p>$$<br>p_{X}(x)=\sum_{y} p_{X,Y}(x,y) , p_{Y}(y)=\sum_{x} p_{X,Y}(x,y).<br>$$</p>
<ul>
<li>$X$和$Y$的函数$g(X,Y)$是一个随机变量,并且</li>
</ul>
<p>$$<br>E[g(X,Y)]=\sum_x \sum_y g(x,y)p_{X,Y}(x,y)<br>$$</p>
<p>若g是线性的,且$g=aX+bY+x$,则<br>$$<br>E[aX+bY+c]=aE[X]+bE[Y]+c.<br>$$<br>.</p>
<ul>
<li>上面的结论可以类似地自然地推广到两个以上的随机变量的情况.</li>
</ul>
<p><strong>关于条件分布列</strong></p>
<p>设$X$和$Y$为某一试验中的两个随机变量.</p>
<ul>
<li><p>条件分布列与之前所学的无条件分布列完全相似,其差别只是前者是在已知某事件发生的条件下的随机变量的分布列.、</p>
</li>
<li><p>设A为某事件,$P(A)&gt;0$.随机变量X在给定A发生的条件下的条件分布列为：</p>
</li>
</ul>
<p>$$<br>p_{X \mid A}(x)=P(X=x \mid A)<br>$$</p>
<p>并且满足<br>$$<br>\sum_{x}p_{X \mid A}(x)=1 .<br>$$</p>
<ul>
<li>设$A_1,…,A_n$是一组互不相容的事件,并且形成样本空间的一个分割,进一步假定$p(A_i)&gt;0$,则<br>$$<br>p_{X}(x)=\sum_{i=1}^{n}P(A_i)p_{X \mid A_i}(x)<br>$$</li>
</ul>
<p>(这是全概率定理的一种特殊情况.)进一步假定事件B满足对一切$i$,$P(A_i\cap B)&gt;0$.则</p>
<p>$$<br>p_{X \mid B}(x)=\sum_{i=1}^n P(A_i \mid b)p_{X \mid A_i \cap B}(x)<br>$$</p>
<ul>
<li>给定$Y =y$ 的条件下 $X$ 的条件分布列与联合分布列之间有下列关系:</li>
</ul>
<p>$$<br>p_{X, Y}(x, y)=p_{Y}(y) p_{X \mid Y}(x \mid y)<br>$$</p>
<ul>
<li>给定 $Y$ 之下的 $X$ 的条件分布列可以通过以下公式计算 $X$ 的边缘分布列:<br>$$<br>p_{X}(x)=\sum_{y} p_{Y}(y) p_{X \mid Y}(x \mid y)<br>$$</li>
<li>同样地,上面的结论可以自然推广到两个以上的随机变量的情况.</li>
</ul>
<p><strong>关于条件期望：</strong></p>
<p>设$X$ 和 $Y$ 为某一试验中的两个随机变量.</p>
<ul>
<li><p>设A为某事件,$P(A)&gt;0$.随机变量X在给定A发生的条件下的条件期望为<br>$$<br>E [X \mid A]=\sum_{\infty} x p_{X \mid A}(x)<br>$$<br>对于函数数 $g(X)$. 我们有<br>$$<br>E [g(X) \mid A]=\sum_{x} g(x) p_{X \mid A}(x)<br>$$</p>
</li>
<li><p>给定 $Y=y$ 的条件下 $X$ 的条件期望由下式定义<br>$$<br>E [X \mid Y=y]=\sum_{x} x p_{X \mid Y}(x \mid y)<br>$$</p>
</li>
<li><p>设$A_1,…,A_n$ 是互不相容的事件并且形成样本空间的一个分割.</p>
<p>假定$P(A_{i})&gt;0 \text{ 对一切 } i$ 成立. 则<br>$$<br>E [X]=\sum_{i=1}^{n} P \left(A_{i}\right) E \left[X \mid A_{i}\right]<br>$$<br>进一步假定事件 $B$ 满足对一切 $i$, $P (A_{i} \cap B)&gt;0,$ 则<br>$$<br>E [X \mid B]=\sum_{i=1}^{n} P \left(A_{ i } \mid B\right) E \left[X \mid A_{i} \cap B\right]<br>$$</p>
</li>
<li><p>我们有</p>
</li>
</ul>
<p>$$<br>E [X]=\sum_{y} p_{Y}(y) E [X \mid Y=y]<br>$$</p>
<p><strong>关于独立随机变量：</strong><br>设在某一试验中,A是一个时间段,满足条件$P(A)&gt;0$,又设X和Y是在同一试验中的两个随机变量.</p>
<ul>
<li>称X为相对于事件A独立,如果满足</li>
</ul>
<p>$$<br>p_{ X \mid A}(x)=p_{X}(x) \text { 对一切 } x \text { 成立 }<br>$$<br>即对一切x,事件${X=x}$与A相互独立.</p>
<ul>
<li>称$X$和$Y$为相互独立的随机变量,如果对一切可能的数对(x,y),事件${X=x}$ 和 ${Y=y}$ 相互独立, 或等价地</li>
</ul>
<p>$$<br>p_{X, Y}(x, y)=p_{X}(x) p_{Y}(y) \text { 对一切 } x \text { 和 } y \text { 成立 }<br>$$</p>
<ul>
<li>若 $X$ 和 $Y$ 相互独立, 则<br>$$<br>E [X Y]= E [X] E [Y]<br>$$<br>进一步地,对于任意函数$g$和$h$,随机变量$g(X)$和$h(Y)$也是相互独立的,并且<br>$$<br>E [g(X) h(Y)]= E [g(X)] E [h(Y)]<br>$$</li>
<li>若 $X$ 和 $Y$ 相互独立, 则<br>$$<br>var(X+Y)=var(X)+var(Y)<br>$$</li>
</ul>
<h3 id="第三章-一般随机变量"><a href="#第三章-一般随机变量" class="headerlink" title="第三章 一般随机变量"></a>第三章 一般随机变量</h3><p><strong>关于概率密度函数(PDF)：</strong></p>
<p>对于随机变量X,若存在一个非负函数$f_X$,使得<br>$$<br>P(X\in B)=\int <em>B f</em>{X}(x)dx<br>$$<br>对每一个实数轴上的集合B都成立,则称X为连续的随机变量,函数$f_X$就称为X的概率密度函数,或简称PDF.</p>
<ul>
<li><p>$f_{X}(x) \geq 0$ 对一切 $x$ 成立</p>
</li>
<li><p>归一化$ \int_{-\infty}^{\infty} f_{X}(x) d x=1 $</p>
</li>
<li><p>设 $ \delta $ 是一个充分小的正数, 则 $P ([x, x+ \delta ]) \approx f_{X}(x) \cdot \delta .$</p>
<blockquote>
<p>注：由于$f_X(x)$是概率律而非某一事件的概率,故其可以取任意大的值,例如：<br>$$<br>f_{X}(x)=\left{\begin{array}{ll}\frac{1}{2\sqrt{x}}, \text{若}0&lt;x\leq 1 \<br>0, \text { 其他 }<br>\end{array}\right.<br>$$<br>在$x$趋于0时,$f_X(x)$可以任意的大</p>
</blockquote>
</li>
</ul>
<p><strong>连续随机变量的期望：</strong></p>
<p>记$X$为连续随机变量,其相应的PDF为$f_{X}x$.</p>
<ul>
<li><p>X的期望由下式定义：<br>$$<br>E[X]=\int_{-\infty}^{\infty} xf_{X}(x)dx.<br>$$</p>
</li>
<li><p>关于随机变量$g(X)$的期望规则为：<br>$$<br>E[g(X)]=\int_{-\infty}^{\infty} g(x)f_{X}(x) d x<br>$$</p>
</li>
<li><p>X的方差由下式给出：<br>$$<br>var(X)=E[(X-E[X])^2]=\int_{-\infty}^{\infty} (X-E[X])^2f_{X}(x) d x<br>$$</p>
</li>
<li><p>关于方差,下列公式成立：<br>$$<br>0\leq var(X)=E[X^2]-(E[X])^2.<br>$$</p>
</li>
<li><p>设$Y=aX+b$,其中a和b为常数,则<br>$$<br>E[Y]=aE[X]+b,var(Y)=a^2var(X)<br>$$</p>
</li>
</ul>
<p><strong>$分布函数(CDF)$ 的性质:</strong></p>
<p>随机变量$X$的$CDF$ $F_X$由下式定义<br>$$<br>\text { 对每一个 } x, F_{X}(x)= P (X \leqslant x)<br>$$<br>并且 $F_{X}$ 具有下列性质._</p>
<ul>
<li> $F_{X}$ 是单调非减函数:</li>
</ul>
<p>$$<br>\text { 若 } x \leqslant y, \quad \text { 则 } F_{X}(x) \leqslant F_{X}(y) .<br>$$</p>
<ul>
<li><p>当 $x \rightarrow-\infty$ 的时侯, $F_{X}(x)$ 趋于 0 . 当 $x \rightarrow \infty$ 的时侯, $F_{X}(x)$ 趋于 1 .</p>
</li>
<li><p> 当 $X$ 是离散随机变量的时候,$F_{X}(x)$为$x$的阶梯函数.</p>
</li>
<li><p> 当 $X$ 是连续随机变量的时候,$F_{X}(x)$为$x$的连续函数.</p>
</li>
<li><p>当 $X$ 是离散随机变量并且取整数值的时,分布函数和分布列可以利用求和或差分互求:</p>
</li>
</ul>
<p>$$<br>\begin{array}{c}<br>F_{X}(k)=\sum_{i=-\infty}^{k} p_{X}(i) \<br>p_X(k)=P(X \leqslant k)- P (X \leqslant k-1)=F_{X}(k)-F_{X}(k-1),<br>\end{array}<br>$$<br>其中 $k$ 可以是任意整数.</p>
<ul>
<li>当$X$是连续随机变量的时候,分布函数和概率密度函数可以利用积分或微分互求：<br>$$<br>F_X(x)=\int ^x_{-\infty}f_{X}(t)dt,f_{X}(t)=\frac{dF_X}{dx}(x).<br>$$<br>(第二个等式只在分布函数可微的那些点上成立.)</li>
</ul>
<p><strong>正态随机变量：</strong></p>
<p>一个连续随机变量X称为正态的或高斯的,若它的概率密度函数具有下列形式：<br>$$<br>f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} e ^{-(x-\mu)^{2} /\left(2 \sigma^{2}\right)}<br>$$<br>其中$\mu、\sigma$是概率密度的两个参数,其中$\sigma $还必须是正数.<br>其均值及方差：<br>$$<br>E [ X ]=\mu, \quad \operatorname{var}(X)=\sigma^{2}<br>$$</p>
<blockquote>
<p>线性变换下($Y=aX+b$)随机变量的正态性保持不变.</p>
<p>其均值和方差由下式给出：<br>$$<br>E[Y]=a\mu+b,var(Y)=a^2{\sigma}^2<br>$$</p>
</blockquote>
<p><strong>多元连续随机变量性质：</strong></p>
<p>令X和Y为联合连续随机变量,其联合概率密度函数为$f_{X,Y}$</p>
<ul>
<li>利用<strong>联合概率密度函数</strong>可以进行概率计算：</li>
</ul>
<p>$$<br>P ((X, Y) \in B)=\int_{(x, y) \in B} \int f_{X, Y}(x, y) d x d y<br>$$</p>
<ul>
<li>X和Y的边缘概率密度函数可利用联合概率密度函数进行计算得到：</li>
</ul>
<p>$$<br>f_{X}(x)=\int_{-\infty}^{\infty} f_{(X, Y)}(x, y) d y, \quad f_{Y}(y)=\int_{-\infty}^{\infty} f_{(X, Y)}(x, y) d x<br>$$</p>
<ul>
<li><strong>联合分布函数</strong>由公式$F_{X,Y}(x,y)=P(X\leq x,Y\leq y)$定义,并且,在联合概率密度函数的连续点上,下面的公式成立：</li>
</ul>
<p>$$<br>f_{X, Y}(x, y)=\frac{\partial^{2} F_{X, Y}}{\partial x \partial y}(x, y)<br>$$</p>
<ul>
<li>X和Y的函数$g(X,Y)$定义了一个新的随机变量,并且</li>
</ul>
<p>$$<br>E [g(X, Y)]=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X, Y}(x, y) d x d y<br>$$</p>
<ul>
<li>若g是一个线性函数$aX+bY+c$,则</li>
</ul>
<p>$$<br>E[a X+b Y+c]=a B[X]+b E[Y]+c<br>$$</p>
<ul>
<li>上面的结论能够很自然地推广到多于两个随机变量的情况.</li>
</ul>
<p><strong>以事件为条件的条件概率密度函数：</strong></p>
<ul>
<li>对于给定的事件$A(P(A)&gt;0)$,连续随机变量X的条件概率密度函数$f_{X|A}$是满足下列条件的函数：</li>
</ul>
<p>$$<br>P (X \in B \mid A)=\left.\int_{B} f_{X}\right|<em>{A}(x) d x<br>$$<br>其中 $B$ 是实数轴上的任意集合.<br>$$<br>f</em>{X \mid{X \in A}}(x)=\left{\begin{array}{ll}<br>\frac{f_{X}(x)}{P(X \in A)}, &amp; \text { 若 } x \in A, \<br>0, &amp; \text { 其他. }<br>\end{array}\right.<br>$$</p>
<ul>
<li>设$A_1,A_2,…,A_n$为互不相容的n个事件,对每个$i$,$P(A_i)&gt;0$,并且这些事件形成样本空间的一个分割.则</li>
</ul>
<p>$$<br>f_{X}(x)=\sum_{i=1}^{n} P\left(A_{i}\right) f_{X \mid A_{i}}(x)<br>$$<br>(全概率定理的一种变形).</p>
<p><strong>以另一个随机变量为条件的条件概率密度函数：</strong><br>设X和Y为联合连续随机变量,其联合概率密度函数为$f_{X,Y}$.</p>
<ul>
<li>X和Y的联合、边缘和条件概率密度函数是相互关联的.它们的关系用下面的公式表示：</li>
</ul>
<p>$$<br>\begin{aligned}<br>f_{X, Y}(x, y) &amp;=f_{Y}(y) f_{X \mid Y}(x \mid y) \<br>f_{X}(x) &amp;=\int_{-\infty}^{\infty} f_{Y}(y) f_{X \mid Y}(x \mid y) d y .<br>\end{aligned}<br>$$<br>条件概率密度函数$f_{X|Y}(x|y)$只在集合${y|f_Y(y)&gt;0}$上有定义.</p>
<ul>
<li>关于条件概率我们有<br>$$<br>P(X\in A \mid Y=y)=\int_{A} f_{X\mid Y}(x\mid y)dx<br>$$</li>
</ul>
<p><strong>条件期望性质：</strong></p>
<p>记 $X$ 和 $Y$ 为联合连续随机变量, $A$ 是满足 $P (A)&gt;0$ 的事件.</p>
<ul>
<li>$X$ 在给定事件$A$ 之下的条件期望由下式定义</li>
</ul>
<p>$$<br>E [X \mid A]=\int_{-\infty}^{\infty} x f_{X \mid A}(x) d x<br>$$<br>给定 $Y=y$ 之下的条件期望由下式定义<br>$$<br>E[X \mid Y=y]=\int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid y) d x<br>$$</p>
<ul>
<li><strong>期望规则</strong>仍然有效:</li>
</ul>
<p>$$<br>\begin{array}{c}<br>E [g(X) \mid A]=\int_{-\infty}^{\infty} g(x) f_{X \mid A}(x) d x \<br>E [g(X) \mid Y=y]=\int_{-\infty}^{\infty} g(x) f_{X \mid Y}(x \mid y) d x<br>\end{array}<br>$$</p>
<ul>
<li><strong>全期望定理：</strong>设$A_1,A_2,…,A_n$为互不相容的n个事件,对每个$i$,$P(A_i)&gt;0$,并且这些事件形成样本空间的一个分割,则</li>
</ul>
<p>$$<br>E (X]=\sum_{i=1}^{n} P \left( A <em>{i}\right) E \left[X \mid A</em>{i}\right]<br>$$<br>相似地,<br>$$<br>E [X]=\int_{-\infty}^{\infty} E \left[X|Y=y| f_{Y}(y) d y\right.<br>$$</p>
<ul>
<li><p>涉及几个随机变量的函数的情况,具有完全相似的结果,例如：<br>$$<br>E \left[g\left(X,Y\right)|Y=y|\right.=\int g(x, y) f_{X \mid Y}(x \mid y) d x \</p>
<p>E[g(X, Y) \mid=\int E [g(X, Y) \mid Y=y] f_{Y}(y) d y<br>$$<br><strong>连续随机变量的相互独立性</strong></p>
<p>令X和Y为联合连续随机变量.</p>
</li>
<li><p>若</p>
<p>$$<br>f_{X, Y}(x, y)=f_{X}(x) f_{Y}(y) \quad \text { 对一切 }x\text{和}y\text{成立}<br>$$</p>
<p>则 $X$ 和 $Y$ 相互独立.</p>
</li>
<li><p>若X 和 $Y$ 相互独立, 则</p>
<p>$$<br>E [X Y]= E [X] E [Y]<br>$$<br>进一步,对于任意函数$g$和$h$,随机变量$g(X)$和$h(Y)$也是相互独立的, 于是<br>$$<br>E [g(X) h(Y)]= E [g(X)] E [h(Y)]<br>$$</p>
</li>
<li><p>若 $X$ 和 $Y$ 相互独立,则</p>
<p>$$<br>\operatorname{var}(X+Y)=\operatorname{var}(X)+\operatorname{var}(Y)<br>$$</p>
</li>
</ul>
<p><strong>连续随机变量的贝叶斯准则:</strong></p>
<p>令Y为连续随机变量.</p>
<ul>
<li>若X为连续随机变量,我们有</li>
</ul>
<p>$$<br>\begin{array}{c}<br>f_{X} y(x \mid y) f_{Y}(y)=f_{X}(x) f_{Y \mid X}(y \mid x) \<br>\end{array}<br>$$</p>
<p>和<br>$$<br>\begin{array}{c}<br>f_{X \mid Y}(x \mid y)=\frac{f_{X}(x) f_{Y \mid X}(y \mid x)}{f_{Y}(y)}=\frac{f_{X}(x) f_{Y \mid X}(y \mid x)}{\int_{-\infty}^{\infty} f_{X}(t) f_{Y} x(y \mid t) d t}<br>\end{array}<br>$$</p>
<ul>
<li>若N为离散随机变量,我们有</li>
</ul>
<p>$$<br>f_{Y}(y) P (N=n \mid Y=y)=p_{N}(n) f_{Y \mid N}(y \mid n)<br>$$<br>得到的贝叶斯公式为<br>$$<br>P ( N =n \mid Y=y)=\frac{p_{ N }(n) f_{Y \mid N}(y \mid n)}{f_{Y}(y)}=\frac{p_{N}(n) f_{Y} \mid N(y \mid n)}{\left.\sum_{i} p_{N}(\hat{i}) f_{Y}\right|<em>{N}(u \mid i)}<br>$$<br>和<br>$$<br>f</em>{Y \mid N}(y \mid n)=\frac{f_{Y}(y) P (N=n \mid Y=y)}{p_{N}(n)}=\frac{f_{Y}(y) P(N=n Y=y)}{\int_{-\infty}^{\infty} f_{Y}(t) P (N=n \mid Y=t) d t}<br>$$</p>
<ul>
<li>对于事件A,关于$P(A\mid Y=y)$和$f_{Y\mid A}(y)$具有类似的贝叶斯公式.</li>
</ul>
<h3 id="第四章-随机变量的深入内容"><a href="#第四章-随机变量的深入内容" class="headerlink" title="第四章 随机变量的深入内容"></a>第四章 随机变量的深入内容</h3><p><strong>计算连续随机变量X的函数$Y=g(X)$的概率密度函数(PDF)：</strong></p>
<p>(1)使用如下公式计算Y的概率函数(CDF)$F_Y$<br>$$<br>F_{Y}(y)=P(g(X)\leq y)=\int_{x\mid g(x)\leq y}f_X(x)dx.<br>$$<br>(2)对$F_Y$求导,得到Y的PDF：<br>$$<br>f_{Y}(y)=\frac{dF_{Y}}{dy}(y)<br>$$<br><strong>随机变量X的线性函数的概率密度函数：</strong></p>
<p>假设X是连续随机变量,概率密度函数为$f_X$,a和b是实数且$a\neq 0$,如果<br>$$<br>Y=aX+b<br>$$<br>则<br>$$<br>f_Y(y)=\frac{1}{\mid X\mid}f_X(\frac{y-b}{a}).<br>$$</p>
<p><strong>连续随机变量X的严格单调函数$Y=g(x)$的概率密度函数计算公式：</strong></p>
<p>假设g是严格单调函数,其逆函数h满足：对X的取值空间内任意一点$x$,$Y=g(x) \quad$ 当且仅当 $\quad x=h(y)$,<br>而且函数 $h$ 是可微的,则 $Y$ 在支撑集$y \mid f_{Y}(y)&gt;0$内的概率密度函数是<br>$$<br>f_{Y}(y)=f_{X}(h(y))\left|\frac{ d h}{ d y}(y)\right|<br>$$<br><strong>卷积：</strong></p>
<p>定义：设X和Y是两个独立的随机变量考虑他们的和Z=X+Y的分布.<br>$$<br>p_Z(z)=P(X+Y=z)<br>$$<br>分布列$p_Z$称为X和Y的分布列的卷积.</p>
<p><strong>卷积公式：</strong></p>
<p>变量$Z=X+Y$的概率密度函数为<br>$$<br>f_Z(z)=\int_{-\infty}^{\infty} f_X(x)f_Y(z-x)dx<br>$$<br><strong>协方差和相关：</strong></p>
<p><strong>协方差：</strong>X和Y的协方差记为cov(X,Y),其定义如下：$cov(X,Y)=E[(X-E(X))(Y-E(Y))]$.当cov(X,Y)=0时,我们说X和Y不相关的.</p>
<p>另一种表达为$cov(X,Y)=E[XY]-E[X]E[Y]$.</p>
<blockquote>
<p><strong>一些性质：</strong></p>
<p>$cov(X,X)=var(X)$</p>
<p>$cov(X,aY+b)=a*cov(X,Y)$</p>
<p>$cov(X,Y+Z)=cov(X,Y)+cov(X,Z)$</p>
</blockquote>
<p>注：X和Y是相互独立的,则E[XY]=E[X]E[Y],故cov(X,Y)=0,它们是不相关的,但是逆命题并不成立.</p>
<p>有下列结论：</p>
<blockquote>
<p>假设$E[X\mid Y=y]=E[X]$ 对任意的y成立,则如果X和Y是离散变量时,E[XY]=E[X]E[Y].(在连续的情形下依然成立.)</p>
</blockquote>
<p><strong>相关系数：</strong></p>
<p>两个方差非零的随机变量X和Y的相关系数$\rho (X,Y)$定义如下：<br>$$<br>\rho (X,Y)=\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}<br>$$<br>易证$\rho\in [-1,1]$</p>
<p><strong>随机变量和的方差：</strong></p>
<p>协方差可以用于计算多个随机变量(不必独立)之和的方差.特别地,设随机变量$X_1…X_n$具有有限的方差,则<br>$$<br>var(X_1+X_2)=var(X_1)+var(X_2)+2cov(X1,X2)<br>$$<br>更一般的结论是<br>$$<br>var(\sum_{i=1}^n X_i)=\sum_{i=1}^n var(X_i)+\sum_{\left{ (i,j)\mid i\neq j\right} } cov(X_i,X_j).<br>$$<br><strong>条件期望和条件方差的性质</strong>：</p>
<ul>
<li>$E[X\mid Y=y]$的值依赖于y.</li>
<li>$E[X\mid Y]$是随机变量Y的函数,因此它也是一个随机变量.当Y的值为y时,它的值就等于$E[X\mid Y=y]$</li>
<li>$E[E[X\mid Y]]=E[X]$ (<strong>重期望法则</strong>)</li>
<li>$E[X\mid Y=y]$可视为已知Y=y时对X的估计.相应的估计误差$E[X\mid Y]-X$是一个零均值的随机变量,且与$E[X\mid Y]$是不相关的.</li>
<li>$var(X)=E[var(X\mid Y)]+var(E[X\mid Y])$ (<strong>全方差法则</strong>)</li>
</ul>
<p><strong>矩母函数：</strong></p>
<p>一个与随机变量X相关的矩母函数是参数s的函数$M_X(s)$,定义如下:<br>$$<br>M_X(s)=E[e^{sX}]<br>$$<br>当X是离散随机变量,相关矩母函数为$M(s)=\sum _x e^{sx}p_X(x)$.</p>
<p>当X是连续随机变量,相关矩母函数为$M(s)=\int_{-\infty}^{\infty} e^{sx}f_X(x)dx$.</p>
<blockquote>
<p><strong>利用矩母函数计算随机变量的各阶矩：</strong><br>$$<br>M_X(0)=1,\ \ \ \ \ \frac{d}{ds}M_X(s) \bigg|<em>{s=0}=E[X], \ \ \  \frac{d^n}{ds^n}M_X(s)\bigg|</em>{s=0}=E[X^n].<br>$$</p>
</blockquote>
<p>若$Y=aX+b$,则$M_Y(s)=e^{sb}M_X(as)$.</p>
<p>若X和Y相互独立,则$M_{X+Y}(s)=M_X(s)M_Y{s}$.</p>
<p><strong>常见的离散随机变量的矩母函数：</strong></p>
<ul>
<li><p>参数为p的伯努利分布(k=0,1)<br>$$<br>p_X(k)=\left{\begin{array}{}<br>p, &amp; \text{若}k=1,<br>\1-p,&amp;  \text { 若k=0, }<br>\end{array}\right.<br>M_X(s)=1-p+pe^s.<br>$$</p>
</li>
<li><p>参数为(n,p)的二项分布(k=0,1,…,n)<br>$$<br>p_X(k)=\binom{n}{k}p^k(1-p)^{n-k},M_X(s)=(1-p+pe^s)^n.<br>$$</p>
</li>
<li><p>参数为 $p$ 的几何分布 $(k=1,2 \ldots)$<br>$$<br>p_{X}(k)=p(1-p)^{k-1}, \quad M_{X}(s)=\frac{p e^{s}}{1-(1-p) e^{s}}<br>$$</p>
</li>
<li><p>参数为$\lambda$的泊松分布 $(k=1,2 \ldots)$<br>$$<br>p_{X}(k)=\frac{e^{-\lambda} \lambda^{k}}{k !}, \quad M_{X}(s)=e^{\lambda\left(e^{s}-1\right)}<br>$$<br>$\bullet(a, b)$ 上的均匀分布 $(k=a, a+1, \cdots, b)$<br>$$<br>p x(k)=\frac{1}{b-a+1}, \quad M_{X}(s)=\frac{e^{a s}}{b-a+1} \cdot \frac{e^{(b-a+1) s}-1}{e^{s}-1}<br>$$</p>
</li>
</ul>
<p><strong>常见连续随机变量的矩母函数：</strong></p>
<ul>
<li><p>$(a, b)$ 上的均匀分布 $(a \leqslant x \leqslant b)$<br>$$<br>f_{X}(x)=\frac{1}{b-a},<br>M_{X}(s)=\frac{1}{b-a} \cdot \frac{e^{s b}-e^{s a}}{s}<br>$$</p>
</li>
<li><p>参数为$\lambda$的指数分布$(x\geq 0)$<br>$$<br>f_{X}(x)=\lambda e^{\lambda x}<br>,<br>M_{X}(s)=\frac{\lambda}{\lambda-s}, (s&lt;\lambda)<br>$$</p>
</li>
<li><p>参数为$(\lambda,\mu)$的正态分布$(-\infty&lt;x\infty)$<br>$$<br>f_{X}(x)=\frac{1}{\sqrt{2 \pi} \sigma} e ^{-(x-\mu)^{2} / 2 \sigma^{2}}, \quad M_{X}(s)= e ^{\left(\sigma^{2} s^{2} / 2\right)+\mu s}<br>$$</p>
</li>
</ul>
<h3 id="第五章-极限理论"><a href="#第五章-极限理论" class="headerlink" title="第五章 极限理论"></a>第五章 极限理论</h3><p><strong>马尔科夫不等式:</strong></p>
<p>设随机变量$X$只取非负值，则对任意$a&gt;0$，<br>$$<br>P(X\geq a)\leq \frac{E[X]}{a}<br>$$<br><strong>切尔雪夫不等式：</strong></p>
<p>设随机变量$X$的均值为$\mu$,方差为$\sigma ^2$，则对任意$c&gt;0$，<br>$$<br>P(|X-\mu|\geq c)\leq \frac{\sigma ^2 }{c^2}<br>$$<br><strong>弱大数定律：</strong></p>
<p>设$X _1,…,X _n$独立同分布, 其分布的均值为$\mu$, </p>
<p>则对任意的$\epsilon&gt;0$, 当$n \rightarrow \infty$ 时,<br>$$<br>P \left(\left|M_{n}-\mu\right| \geqslant \epsilon\right)= P \left(\left|\frac{X_{1}+\cdots+X_{n}}{n}-\mu\right| \geqslant \epsilon\right) \rightarrow 0<br>$$<br><strong>中心极限定理：</strong><br>设$X _1, X _2 …$是独立同分布的随机变量序列，序列的每一项的均值为$\mu$, 方差为$\sigma^2$.<br>记<br>$$<br>Z _n=\frac{X _1+…+X _n-n \mu}{\sqrt{n} \sigma}<br>$$</p>
<p>则$Z _n$的分布函数的极限分布为标准正态分布函数：</p>
<p>$$<br>\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-z^2/2}dz<br>$$</p>
<p>即<br>$$<br>\lim _{n \rightarrow \infty} P \left(Z _n \leqslant x\right)=\Phi(x), \text { 对任意的 } x \text { 成立. }<br>$$<br><strong>强大数定律：</strong><br>设$X_1,$ $X _{2}, \cdots, X _{n}$是均值为$\mu$ 的独立同分布随机变量序列, 则样本均值$M _{n}=(X _{1}+X _{2}+…+X _{n}) / n$以概率 1 收敛于$\mu$, 即</p>
<p>$$<br>P \left(\lim _{n \rightarrow \infty} \frac{X _1+X _2+\cdots+X _n}{n}=\mu\right)=1<br>$$</p>
<p>2</p>
]]></content>
  </entry>
  <entry>
    <title>随记</title>
    <url>/2021/20210115/</url>
    <content><![CDATA[<p>20年高考各专业分数线已出</p>
<span id="more"></span>![scNfwd.md.jpg](https://s3.ax1x.com/2021/01/18/scNfwd.md.jpg)



<p>不禁一声长叹</p>
<p>走向了另一条道路</p>
<hr>
<p>选择只是几秒钟的事，然后用余下时间来还债。——《质数的孤独》</p>
<hr>
<p>若是把珠数放在第一志愿，就是另一番故事了，我和她也更近了几分，也不会处于这种有点尴尬的处境，感觉她对我有好感（也有可能是错觉(x)），但我不希望她只是爱上了一个影子亦或是只不过一个幻想，希望她能认识一个真实的我。</p>
<p>我们都需要进一步地接触，可是距离是最大的障碍，我们隔着近百公里的距离，</p>
]]></content>
  </entry>
  <entry>
    <title>一道SYSU期末高数题</title>
    <url>/2021/20210107/</url>
    <content><![CDATA[<p>证明：当$x\in(0,\frac{\pi}{2})$, $\frac{tanx}{x}&gt; \frac{x}{sinx}$.</p>
<span id="more"></span>

<p>证法一：<br>$\sqrt{ \sin x \tan x } \geq \frac{2} { \frac{1} {\sin x} + \frac{ 1}{ \tan x} } = \frac{2 \sin x} { 1 + \cos x } = 2 \tan \frac{x}{2} \geq x$</p>
<p>证法二：<br>$\frac{\sin x}{x}=\int_0^1 \cos(t x)dt$,</p>
<p>$\frac{\tan x}{x}=\int_0^1\frac{1}{\cos^2(tx)}dt(0&lt;x&lt;\frac{\pi}{2}).$</p>
<p>根据Cauchy–Schwarz不等式有：<br>$\frac{\sin x}{x}\frac{\tan x}{x} $</p>
<p>$\geq (\int_0^1\frac{1}{\sqrt{\cos(tx)}}dt)^2&gt;1(0&lt;x&lt;\frac{\pi}{2}).$</p>
<p>证法三：<br>即证$f(x)=sinxtanx-x^2&gt;0$<br>而$f(0)=f’(0)=f’’(0)=0$<br>$f’’’(x)=sinx(5sec^2x-1)+bsin^3xsec^4x&gt;0$<br>故$f(x)&gt;0$</p>
]]></content>
  </entry>
  <entry>
    <title>2020年度总结</title>
    <url>/2020/20201231/</url>
    <content><![CDATA[<span id="more"></span>早早就打算在30号就写下今年的年度总结，但是...咕咕咕  



<p>现在不得不在回广州的车上，敲下今年的年度总结。 ╮(╯▽╰)╭</p>
<p>年初也没设立什么什么今年目标，只有入学时定了几个学年小目标，就权当是今年的目标。</p>
<p>目标如下：</p>
<p>1、学习数学分析</p>
<p>2、学习PYTHON，并基于学习人工智能</p>
<p>3、背四级</p>
<p>4、学习算法</p>
<p>5、看完买的课外书</p>
<p>6、有空跑步</p>
<p><del>7、发表一篇paper</del></p>
<p><del>8、脱单</del></p>
<p>9、略</p>
<p>实现情况：</p>
<p>1、只学完了Tao的《实分析中文版》上册，即《analyse Ⅰ》。至于裴砖，……</p>
<p>2、只有前半段 （狗头</p>
<p>3、无</p>
<p>4、只学了几天</p>
<p>5、半本《鼠疫》，半本《斜阳》，半本《西西弗神话》,还有若干碎片阅读（啥，怎么都是半本?⊙﹏⊙</p>
<p>6、自从感冒后就搁置了</p>
<p>7、难度太大</p>
<p>8、<del>同上</del>  </p>
<p>总结：</p>
<p>一、没有有效利用好时间。这么多时间没有利用好，太多时间被浪费在无意义的事情上面了。</p>
<p>二、意志力不够。连最基本的每天背四级单词的目标没有达成。</p>
<p>三、好高骛远。还想发paper，就这<del>菜鸟</del>水平，计算机方面？数学方面？</p>
<p><a href="https://imgchr.com/i/rvcgO0"><img src="https://s3.ax1x.com/2020/12/31/rvcgO0.jpg" alt="1"></a></p>
<p><del>4、至于感情方面，此处略去…字</del></p>
<p>新一年目标：</p>
<p>1、学习算法、准备ACM、数模</p>
<p>2、学习相关人工智能</p>
<p>3、准备四级</p>
<p>4、每天抽空看书</p>
<p>5、踏踏实实夯实基础</p>
<p>6、树立远大目标，勇往直前（<del>啊，我的目标好像是数学机械化方向来着</del>）</p>
]]></content>
  </entry>
  <entry>
    <title>她</title>
    <url>/2020/20201221/</url>
    <content><![CDATA[<p>有人曾说，最可悲的是两条平行线<span id="more"></span>，永远没有爱情的交点。有人曾说，最可悲的是两条相交的线，终究渐行渐远。可殊不知，最可悲的是异面直线，永远不相交，在某一刻有着最近的距离，以为相遇却渐行渐远，不再相见。</p>
<p>我向来都是缘寡，喜欢的女生永远像异面直线，永远不会有交集，本以为能有一段好的开始与结束，却可悲的连一段开始也没有，就变成了昨日的回忆，成为往日的遗憾。</p>
<p>高三那年，逐渐短暂接触，她慢慢进入了我的视野，让我怦然心动。我的脑海里全是她，她的微笑，她的可爱马尾，她的活泼，她如丁香花一般。我越来越期待她能来问我问题，毕竟这可能是唯一的交流方式，我不知道也不懂得，怎么找机会和她讲话。偶尔和她对视一眼，小鹿乱撞却故作镇定。目光之余，看到她和其他异性嬉闹，就会黯然神伤，心情沮丧。每天都怀着对今天的期待，期待能有更多交集，但更多的是失落啊。</p>
<p>我们自始至终也不过只是普普通通的同学。想着高三毕业后，这段情感会慢慢消散，渐渐成为昔日的无奈。高考后，本以为她也将慢慢从我的世界里离去，她主动加我微信又让我的喜欢之情死灰复燃，更加愈发不可收拾。每个睡不着的深夜，我会打开手机翻看朋友圈，看看她有没有新的动态，分析她的心情，想象她的一举一动。有时，我会慎重地发条朋友圈，期待她能关注到，期待她能哪怕只点一个赞，最后在心里默默宽慰自己：“她肯定太忙了。”即使我看到了，她刚刚才评论了我们的共同好友。期待之后，伴随着总是失落，我也总默默自嘲：“你和她不过是普普通通的关系，她又不喜欢你，何必自作多情。”</p>
<p>”已记不起我也有权利爱人</p>
<p>谁人曾照顾过我的感受</p>
<p>待我温柔吻过我伤口“</p>
<p>——《七友》</p>
<p>有时候，我觉得自己就像一个小丑，多么可笑的小丑。可我是多么喜欢她呀，满脑子都是她。我想和她漫步在晚风中，数着星星，说着可爱的情话。有时候，做梦梦见她，手牵手漫步在大桥上，羞涩地回眸一笑让我沦陷不已。</p>
<p>“总想要透过你眼睛</p>
<p>去找寻水仙的倒影</p>
<p>没想到最后却目睹</p>
<p>一整个 宇宙的繁星</p>
<p>这一秒</p>
<p>只想在爱里沉溺”</p>
<p>——《溯》</p>
<p>我不知道她是否对我哪怕只有一丝好感，我不知道她目光之余能否注意到我，我不知道她是否能感受到和她聊天一字一句都要斟酌好久的我的情感。但是这些都不重要。我几乎无法抑制我的热烈的波涛汹涌的情感。我也曾打算在七夕向她表达我的心意。可是，我向来是一个内向的人，怯懦的人，自卑的人。而我又不善言辞，不善表达，词不达意。</p>
<p>我不敢追求我的爱，连轻轻踮起脚尖去触摸那缕月光也没有勇气。她像挂在天空的一轮弯月，可望而不可及。我和她的距离是多么遥远啊，所爱隔山海，山海不可平。我…没有勇气啊。我也不愿让她品尝异地的苦涩。精雕细琢的话语成为了过往的灰烬。</p>
<p>“她像一颗星星,总是那么遥远,照到我身上的光也总是冷的。”</p>
<p>——《三体》</p>
<p><img src="https://s3.ax1x.com/2020/12/20/ra0Wi6.jpg" alt="1"></p>
<p>我不愿像其他人一样浮躁，或为了不孤独，或随大流，或向往爱情的美好，而草率地开启一段新的恋情。大学里的表白墙，像是相亲市场，一份份让人眼花缭乱的征友资料如待价而沽的商品一般。有人不到一个月就确定了一段关系又不到一个月草草结束了这段关系。我期待能有一段细水长流的恋爱，相处间慢慢心意想通，我不希望带有谈恋爱或结婚的目的去寻找自己的爱。爱是没有理由，没有目的的，是纯洁，是不染的。</p>
<p>可是，我…终究不能也无法</p>
<p>“其实我也知道我<br>并不算成熟<br>所以更渴望能有 风雨的承受<br>无奈连光都照不够<br>也没有人来收获<br>只能自己结果 和坠地的疼痛<br>其实我也知道我 是有些懦弱<br>表里不一的坚强 难道也有错<br>我只是不愿意 一直妥协昧心到腐朽<br>成色不足的苦橙 难道注定就<br>不如那些罐头<br>讨喜地被拥有”</p>
<p>——《苦橙》</p>
<p>这一切该画上句号了</p>
<p>收起这可怜，可悲的单相思罢。</p>
<p>收起这泛滥的自作多情罢。</p>
<p><a href="https://imgchr.com/i/rHtQZF"><img src="https://s3.ax1x.com/2020/12/29/rHtQZF.md.jpg" alt="3"></a></p>
]]></content>
  </entry>
  <entry>
    <title>小镇做题家出路何在</title>
    <url>/2020/20201219/</url>
    <content><![CDATA[<p>2020年12月15日北交大大三学生跳楼，写下了一封让人唏嘘不已的遗书。部分观点让我深有同感。</p>
<p>在公众号看到一篇深有感触的文章，故摘录如下。</p>
<span id="more"></span>

<p>我是一个工人家庭出身的小镇做题家，也曾像这个群体里的大多数人一样迷茫。直到接触毛泽东思想之后，我才认识到，小镇做题家的困境主要是没有认清自己的阶级导致的，只有依靠无产阶级才能有真正的未来。</p>
<p>一、一个小镇做题家的迷茫</p>
<p>我出生于1996年，家是东南沿海的农村，后来发展成一个小镇，父母都是65后，他们文化水平不高，从事最低级的体力劳动。我的父亲是一名建筑工人，他身上的砖石水泥味伴随着我的成长，长期的重体力劳动让他落下了一身的伤病。我的母亲是一名电子厂工人，做着月休两天，每天工作11小时以上的流水线工作（有时超过11小时，超过的部分才算加班），并且工资计件结算使她几乎没有休息的工夫。在如此工作生活条件下，他们除了寄希望于我认真读书考上好大学以外，还不遗余力地贬低自己的工作，让我千万不要像他们一样。</p>
<p>在这种理念影响下，我为了高考不懈努力，终于考上了一所985大学。然而并没有人告诉我上了好大学之后应该怎么办，之前的十几年我除了做题一无所知，而且在新的环境里有人比我更会做题，加上我厌倦自己所处的这个理工科专业，更加没有学习的动力。</p>
<p>高考前高压状态的反噬，我开始放纵自己。到了毕业时期，我发现自己连过去的做题能力都失去了，两次考研失败，没有光鲜的履历，看着过去许多做题能力不如自己的人考上了研究生，我焦虑、迷茫，成了一名“985废物”、小镇做题家。或许出于是吃不到葡萄说葡萄酸的嫉妒心理，我开始怀疑自己过去的追求，开始认真思考高学历的光环究竟代表着什么。在我遇到读书生涯以来最大的低谷的时候，我想起了过去同样遇到过人生低谷的革命家，我想了解那段历史，想明白毛主席是如何走出人生低谷的。因此，我买了套毛选，一下子被其吸引住了。</p>
<p>二、毛泽东思想造成了我的心路转变</p>
<p>其实每一个中国人对毛主席的生平以及思想都不陌生，但印象里大都是教科书上那种流水账似的人物履历和冠冕堂皇的政治宣言，就像是一道数学题直接给出答案而省略了中间过程，多少令人反感。而毛选则还原了这个解题过程，毛主席用平易近人的大白话详细地向人们解释了他是如何在革命斗争中一步一步完善自己的思想。</p>
<p>我的两次考研虽然都以失败告终，但却让我对近代著名的历史事件铭记于心，我清楚地知道毛选中每篇文章写作时所对应的革命形势，因此很快就被毛主席的信念所惊讶。毛主席最突出的特点不是他的睿智或文采，而是不论革命的客观形势是否处于低谷，他都始终如一地坚信人民的力量，他对人民的信心从未有过低谷。毛选中的思想强烈地吸引着我，驱使着我深入了解更多毛主席的事迹以及马克思主义思想，并开始思考自己的人生与社会现状。</p>
<p>今年7月份，我入职了一家制造业央企，与我一同入职的还有其他几位985、211毕业的本科生和研究生。入职后不久，领导（博士学历）组织了迎新的酒局，强迫新人必须喝白酒且必须打通关（就是跟全桌每人对喝一杯），从没喝过白酒的我也只得勉强应付。酒酣之后，领导们开始围着在场唯一的女员工不停地说黄色段子，并且“教育”我们新人说公司会重点培养我们，要我们尽快适应职场，要学会在酒局上说适当的话以活跃气氛。也许过去的我会把这些“教诲”奉为真谛，向往着成为领导这样的人，然而了解了毛泽东思想之后，我意识到过去的我追求的不过是高学历所代表的高收入和高的社会地位，而毛主席33岁就官至国民党部长，却为了底层人民放弃了高官厚禄，从此我改变了过去的追求，成为一名坚定的马克思主义者。</p>
<p>随着小镇做题家的大火，我也在网上了解到，许多人与我出身类似，有着同样的烦恼。看完毛选之后我知道这一社会现象必有其深层的原因，我开始尝试从毛泽东思想来分析小镇做题家的困境。</p>
<p>三、根据毛泽东思想看小镇做题家的问题</p>
<p>在我看来，毛选5卷通篇就讲了两个问题，一是如何区分朋友和敌人；二是如何团结朋友击败敌人。这两个问题不是孤立的而是互相联系的，随着事态发展，旧的敌人消灭了之后又会产生新的敌人，而这新的敌人也可能是原来的朋友，需要重新区分。这两个问题覆盖了从个人到国家发展的全部历史进程。过去我曾以为上了名校，就可以结交更多社会上层人士，以为所有名校生都是志同道合的朋友。</p>
<p>可是当我发现自己与其他同学的差距时，我陷入了怀疑。对小镇做题家来说，考上名校就像一只青蛙费劲心力地爬到了井口，大多数都需要停下来休息一下，感叹一下世界之大，很难马上知道下一步该怎么走；而有社会资源的人像一只在井边歇息的鸟，他们早就见过井外的世界并明确知道自己将要飞向哪一个目标。小镇做题家以及他们身边的人最大的错觉，便是以为和鸟儿到达过同一高度的青蛙自然也能飞翔。我以为名校生之间容易互相理解，便跟同样名校背景的新同事一起交流我学习毛泽东思想的心得，结果他们不屑一顾，其中一人甚至说我被洗脑了。我实在想不到一位985研究生毕业的中共党员，竟然将毛泽东思想斥之为洗脑，那时我意识到青蛙和鸟儿永远成不了同类。</p>
<p>那么根据毛泽东思想，谁才是小镇做题家的朋友，谁又是真正的敌人？过去高学历代表着高素质以及承担更多的社会责任，如今高学历却与高收入挂钩，其本质上是资产阶级为了缓和阶级矛盾，刻意拿出一点资源分给高学历人群，为无产阶级打造了一个取得高学历就能获得高收入的信仰体系，我称之为高学历信仰。在高学历信仰的毒害下，广大的无产阶级寄希望于自己的下一代，放弃了自己的权益，斗争意识被不断削弱。而他们费尽心血培育出来的高学历的下一代，少数尝到了资本的甜头而迅速转过头来压迫无产阶级，而其他人正为了那点求而未得的残羹冷炙而苦恼，这些人便是小镇做题家。但是，他们普遍瞧不起体力劳动者，很少有人，能够回头关注一下培育出自己的无产阶级。</p>
<p>高学历信仰是类似印度教一样的思想糟粕，它让人们忽视了自己这辈子的利益而寄希望于来生或下一代，与印度教强调虚无的轮回不同的是，高学历信仰仍具备极小的成功的可能性，但这些成功的人无一例外会背叛无产阶级。在高学历信仰的影响下，越来越多的父母将子女送进这个唯一的通道，而世界经济形势的低迷使资产阶级不愿意再拿出原来分出的资源，通道正不断变窄，有些人把这种现象称为内卷。</p>
<p>由此得出，小镇做题家真正的朋友是含辛茹苦培养自己的无产阶级，就像那只爬到井口的青蛙，它的朋友始终是仍在井底挣扎的其他同类。毛主席说过，知识分子是毛，无产阶级是皮，皮之不存，毛将焉附？小镇做题家真正的敌人是资产阶级，是打造高学历信仰的人。我们不应该感谢这条通道，相反，这条通道是资产阶级用我们父辈的血泪铸成的，应该深刻地认识到建造这条通道的人才是真正的敌人。</p>
<p>在我看来，小镇做题家是能够理解和接受毛泽东思想的人。首先，我们具有一定的知识储备和学习能力，我们了解革命历史，能理解消化马克思主义理论知识；其次，小镇做题家获取财富和地位的难度加大，使我们不得不重新审视资产阶级打造的高学历信仰；最后，小镇做题家是能够了解工人阶级的状况，他们的原生家庭要么是无产阶级要么是其他底层群体，他们天然在无产阶级的一方。</p>
<p>我们有强烈的改变父辈处境的愿望，我们应该也必须将对父母亲的同情扩大到整个工人阶级和劳动人民，这样我们才有希望。</p>
<p>反对高学历信仰并不意味着放弃高学历，而是不该以物质利益为目的去追求高学历，相反无产阶级队伍需要高学历高素质人才，需要有人深入学习马克思主义理论知识。老一辈革命领导人基本都具有扎实的理论功底。小镇做题家在当下应该积极学习和传播马克思主义及毛泽东思想。但是，仍有很多小镇做题家将这些理论斥之为“洗脑”。</p>
<p>究其原因，大学里的马克思主义相关课程要负很大的责任。现在教科书的内容始终以应试为目的，考试题目无非是毛主席在什么时候做了什么事提出了什么思想。这样的内容除了枯燥无味、教条主义之外，还充斥着英雄史观。尽管有比如“毛主席依靠人民取得了胜利”之类的话，可是并没有解释人民发挥了什么作用，不断地重复这样的字句只会体现毛泽东一个人的作用。英雄史观会扼杀人的主观能动性，抹去人民在历史中的贡献，让人以为革命只是一两个领袖的事。毛主席一生都在致力于发动人民的力量，提高人民的觉悟。毛选的语言如此平易近人，也是因为毛主席努力地想把自己的思想传给读者，他希望人人都能继承他的事业，“春风杨柳千万条，六亿神州尽舜尧”。</p>
]]></content>
  </entry>
  <entry>
    <title>来自斯坦福轮回赛的有趣积分题目</title>
    <url>/2020/20201210/</url>
    <content><![CDATA[<span id="more"></span>

<p><img src="https://s3.ax1x.com/2020/12/22/rr5ZFI.jpg" alt="1"><img src="https://s3.ax1x.com/2020/12/22/rr53wj.jpg" alt="2"></p>
]]></content>
  </entry>
  <entry>
    <title>旅行商问题及其拓展</title>
    <url>/2020/20201207/</url>
    <content><![CDATA[<p>旅行商问题（英文：Travelling salesman problem, TSP）是图论、组合数学中一个非常经典的问题。</p>
<span id="more"></span>

<p>旅行商问题具体内容：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。其与哈密顿有着密切的联系，TSP可以约化为哈密顿回路。</p>
<p>TSP属于NPC问题，故无多项式时间算法，常用求解算法有回溯法、分支限界法、等，以上均为O（n!）的时间复杂度。</p>
<p>TSP对路线规划有着重要应用，而路线往往为非哈密顿图。可以适当地对TSP进行拓展：（正是SYSU2020年新手赛A题。）</p>
<p>给定一系列城市和每对城市之间的距离，求解访问每一座城市并回到起始城市的最短回路。（该形式貌似又被称为Euclidean TSP）</p>
<p>该问题可以约化为TSP。</p>
<p>解法：</p>
<p>获得每个城市到其他城市的最短路径，若该城市与某城市不可直接到达，那么这两站之间增加一条边，边的权为以上所求最短路径的长度。而这个图一定是哈密顿圈。该问题现在约化为了TSP。</p>
]]></content>
  </entry>
  <entry>
    <title>《我曾七次鄙视自己的灵魂》</title>
    <url>/2020/20201124/</url>
    <content><![CDATA[<p>第一次，当它本可进取时，却故作谦卑；</p>
<span id="more"></span>

<p>第二次，当它在空虚时，用爱欲来填充；</p>
<p>第三次，在困难和容易之间，它选择了容易；</p>
<p>第四次，它犯了错，却借由别人也会犯错来宽慰自己；</p>
<p>第五次，它自由软弱，却把它认为是生命的坚韧；</p>
<p>第六次，当它鄙夷一张丑恶的嘴脸时，却不知那正是自己面具中的一副；</p>
<p>第七次，它侧身于生活的污泥中，虽不甘心，却又畏首畏尾。</p>
]]></content>
  </entry>
  <entry>
    <title>《八阵图》（节选）-温瑞安</title>
    <url>/2020/20201117/</url>
    <content><![CDATA[<p>《八阵图》是武侠大家温瑞安十八岁所作的万字散文。在十八岁的年龄，少年温瑞安着实狂傲不凡，意气风发，“狂笑当歌江湖路”，肆意挥洒，纵情高歌。<br>在这个年龄，有的人却不过是些老弱不济之徒，被过去的梦想压着，从此再无力于乌托邦，只是一群倦怠未来的掘墓人。<span id="more"></span></p>
<p>《八阵图》节选如下：</p>
<hr>
<p>“青春是不中用的东西”？我们唱了，然后互相对望一眼，再唱下去。我们歪歪斜斜地乱步着，沙原上一片空漠，夜深沉;夜夜深深沉沉。我们如月下的精灵，酩酊于太白的月下，或起舞、或弄清影,但绝不止三人！想我们第一次在诗会中喝过的酒— 酒瓶狼藉高高低低，东倒西歪。一夜的哭诉,一夜的呕吐。想那次胡笳十八拍的月夜,我拔出两把莹亮的小刀，飞舞于思君令人老的月下，刀起刀落，刀去刀来，灿闪如我年轻的生命！啊白衣，我学的是国粹，练的是国术，但寥落江湖，竟无一可谈之人。我活着，是因为我的诚和真，我的劲和热，还有不能忽略的是： </p>
<p>​    我的<br>​        狂傲<br>​            啊<br>​                狂傲</p>
<p>我必须面对它了。我只怕一件事，我不怕打击，不怕失败，不怕失望，只怕死亡，因为它是我惟一克服不了的命运，改变不了的终局。或许，为了一件具有伟大的意义或真理,我不惜以生命去换取;但我憎恶死亡。可是没有人能自死亡的巨网中逃脱；既然如此，我惟有以生命的光芒去照亮死亡。我本身的光芒也许是很微弱的，但我会以我的撞击去发出我所有的星花 ，生命是悲哀的，死亡是可叹的。但我蔑视它们，既然我无法逃避，等它来吧！在它未来之前，我会尽量放发我的星花，让更多一些人能分享它片刻的温热与光芒。</p>
<p>不能征服死亡，那么，不要被它征服吧！死亡是硕大无比的，也许它的本身并不可怕，可怕的是它的灭绝。死亡造成的是对生存的遗憾与生命的留恋，但连遗憾与留恋都不能再容纳了，因为死了便等于什么都没有了。死是痛苦的，他，或她，只能静静静静地蜷伏在冷冷湿湿的黄土中，每一昼每一夜昼昼夜夜地躺着，不能移动也不能说话，没有思想也没有感觉,直至有一天他们变成了一堆白骨，由白骨再化成泥尘，永远，永远永远永远氷远地消失在世上;尽管他们在生前或许是圣贤豪杰，或是绝世红颜，但那都是些过去的事了；如果他们曾在世上擦亮起一片灿烂的星火，那么，或许有人会追念他们在星月下。但追念又能弥补些什么呢？仍生存着的，只平添一种淡淡的轻愁；已死去了的，已完全没有感觉了。我们活着的一天，有多少时候是去为那些逝者而缅怀而追念呢？多少名将会忆起岳武穆的雄风？多少名士会追念苏东坡的豪情？就像我们这群专研文学的，有多少时候，会默默地为那给世界思潮巨大影响的学人柏拉图、亚里士多德等低首追回过？更休说那些成就不及这些伟人的冤魂了。—将功成.枯朽的岂仅是万骨而已？ 一次改革，熬白了多少人的青发！但那些人呢？枯了，朽了，随风而逝了，他们曾经活过，曾为自己想过，也为别人想过，但而今呢？偶尔想起他们的，又有谁为他们呢？甚至他们已被淡忘了，他们的名宇已随历史的蹄尘而湮远了— 最残忍的是：他们已化为泥尘，不管被忆或被忘，都与他们无关了，死去便是什么都没有，包括追念和何忆。或许如今我们正踩在他们的头顶上，在他们在冷湿的黄土中，亘古以来所发生的事，已与他们无涉:他们已什么都不知道！他们连什么都不是了！覆盖在他们顶上的，是如此美丽而古典的星<br>空，但他们知道吗？他们知道吗？</p>
<p>我们仍疾步走着，被蛊惑似的走着，被赶尸似的走着。如果前面忽然出现的是一具无瞳无目长舌滴血的慑青鬼，对我们来说，或许还是一种存在的证实— 至少我们知道，死后还有再生，虽然这种“再生”是等于“死活”，或许是活在一个更惨诡的世界里，但这毕竟是存在啊存在！只要有存在、有意识，便算是有价值的活了！只要死去不是什么都没有:没有回忆，没有过去的痛苦和欢乐，也没有现在的痛苦和欢乐，更没有将来的痛苦和欢乐！就连我们现在正想着生存和死亡，但有一天，忽然连这一点痛苦的思索都无存了，那该是多残忍啊残忍<br>        残<br>        忍。<br>人渺小，人太渺小了，很容易被死亡所击败。夭折，寿终，都逃离不了宇宙的冷视。宇宙究竟是什么呢？如果我们抬目，只见一片黑压压的枝叶，就这样扼杀了人的视力。就算我们能仰望星空吧！我们所能见到的，是浩淼的星海。哪一颗星离你最近?设若你神游到那星上去，在那儿望见的地球，是不是也是星海中一颗随时可以幻灭的小星？而你只不过是这小星星上的一丁点儿些微的小东西罢了！你可以干出一些什么伟大的事业来吗？也许吧！在这大宇宙里，我们能了解的是多少颗星星？唔，从这里望去，是一片无尽的云海，那么无尽的云海外又是什么？无尽的云海之外的无尽云海之外的无尽的云海之外之外之外又是什么？是边际吗？边际之外又是些什么？要永垂千古，要永恒，要不朽，在我们的星球上已难做到，每一个星球都有它们自己的经典，我们能做到的又是什么？星星之外的星星之外—我是说在最强的暸望镜中，能看到的是多少颗星星？星星之外呢？这浩淼的宇宙啊— 宇宙真的浩淼吗？这整个“无限”的大宇宙，是不是一个“无限”的神它指下的棋盘，棋盘上放满小星星，所谓时间，便是它们的对弈的黑子白子呢？我不知道，没有人能知道，如果人类以后还有千千万万的历史，等有一天他们“征服”整个宇宙后，才惊觉他们只是从一个太极八卦图里跳出来而已，那是何等荒唐啊！八卦两仪以外的呢？所以当我们望星，当我们整个融人大自然时，我们早已被那蓝得深永的云海所溺毙了！</p>
<p>一霎那间，时间和命运的洪流淹盖了一切，我，人类，以及一切一切。未知的无限,对悠悠之天地的无奈与哀恨，都浮现在我所有的感官里。前人的遗恨，今人的虚寂，如戏剧而且是悲剧地在空漠的时间之流里鱼贯走过，多么ironical!但是我能做些什么?我们能做些什么呢？我们仍然年少，仍然狂热，仍然渴切着把自<br>己的辉煌映照在别人的身上！怎么能因为时间、空间与命运的汪洋便丧失了渡航的勇气呢？如果有命运，如果真的有命运的话，命定了我现在要因恐惧而停顿我的步伐，我偏走偏要走要走要走要走—— 如果有命运，命运那厮要我现在不能开口，我偏要开口，开口笑:哈哈哈哈哈哈。这算是给命运的一种反击？究竟是我败了它？还是它败了我？是它本来要我没来由地笑起来？还是我没来由的笑已惊破它的掌握？我不知道，我知道我的伙伴因为我笑声而放缓了步伐。我不能知道得那么多了！我还年轻，我仍豪放，我的刀尖而利，我的箫并不凄凉！我是龙啊龙是我我是龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙……周遭还是无天无地无边无际无岸无涯无远无近无生命的黑暗。、</p>
]]></content>
  </entry>
  <entry>
    <title>一道三角函数极限计算的一题多解</title>
    <url>/2020/20201115/</url>
    <content><![CDATA[<p><img src="https://s3.ax1x.com/2020/11/18/DnIfQe.jpg" alt="1"><span id="more"></span></p>
<p><img src="https://s3.ax1x.com/2020/11/18/DnIhsH.jpg" alt="2"></p>
<p><img src="https://s3.ax1x.com/2020/11/18/DnIRzD.jpg" alt="3"></p>
]]></content>
  </entry>
  <entry>
    <title>读《西西弗神话》有感</title>
    <url>/2020/20201026/</url>
    <content><![CDATA[<p>我们都是生活中的西西弗，重复着日复一日地习惯和循环的链条。神话故事中的西西弗亦是如此，被迫接受了无法抗争的命运，重复着滚石头的动作，荒谬且痛苦。<span id="more"></span><br>初读《西西弗神话》，本以为加缪同齐奥朗一般的虚无主义者，将以一种彻底虚无的文字敲击我们的心灵。然而并非如此，在加缪的文字中，我看到更多的是积极反抗，不消极避世或是像犬儒主义者般消极入世。<br>在《西西弗神话》中，加缪首先探讨了荒谬与自杀的关联。我们被时间裹挟着向前，未来的确定与不确定使我们染上了陌生和诡异感，进而感受到非常痛苦的抵抗，荒谬感。<br>加缪将荒谬人分为三种。一是生理自杀者，无法忍受生活的荒缪与无意义，选择了结自己的生命。二是哲学自杀者，通过神（上帝）和宗教屏蔽自己的感知，不去感受荒谬，像把头埋在沙里的鸵鸟，掩耳盗铃。三是被加缪称赞的反抗者，荒谬英雄。重复着每天痛苦而不终止的悲剧。无法拒绝命运，那我就为现在而活，欣然坦然接受自己的命运，将巨石徒劳无功地一次次推上山顶，不作荒谬的逃避者，清醒地体验生活的荒谬感，以激情的态度向前、向前、向前，在无意义、荒谬的工作中，找到属于自己的意义，找到自己的存在。<br>罗曼罗兰曾言：世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活。西西弗正是这样的英雄，这样的“荒谬英雄”。生命本身是荒谬的，是毫无意义的，但我们要觅寻到属于我们的命运，属于我们的某种意义，在心中坚守一生。也正如《德米安》中所言：其他的路都是不完整的，是人的逃避方式，是随波逐流，是对内心的恐惧。</p>
<p>我也曾切身体会生活中的荒谬感，我也不争气地成为过哲学自杀者，也曾主动“拥抱“上帝，也略有涉猎等宗教书籍，但发现这并不是我的道路。偏激的我甚至转向了神的对立面，阅读了一些渎神之作。但这无济于事，于事无补，我变得更加虚无，犬儒，消极，一词以蔽之就是——丧。我很感谢我能遇上这本书还有许许多多给予我希望，”拯救“了我的书。</p>
]]></content>
  </entry>
  <entry>
    <title>《黄河》——《山河录》</title>
    <url>/2020/20201025/</url>
    <content><![CDATA[<p>是曰：</p>
<p>我的歌</p>
<p>是一道静静的水流穿出幽谷</p>
<p>本是悠闲，而后激越。越是荒漠，越是悲壮。</p>
<span id="more"></span>

<p>转转折折，许许多多汇合后，</p>
<p>化成一条万古云霄万古愁的身姿，浩浩荡荡地唱：</p>
<p>我是黄河我是黄河</p>
<p>我的悲伤是千万人的悲伤</p>
<p>我的歌是千万人的歌</p>
<p>我是黄河我是黄河我</p>
<p>是黄河我是黄河我是黄河</p>
<p>我是黄河……</p>
<p>流动是可喜的</p>
<p>成为一池碧潭却是……</p>
<p>在所有的东树里</p>
<p>我是风，自湖水的衣襟褶过</p>
<p>在一棵枯之间停留</p>
<p>惊见两掌红红而纤小的叶。</p>
<p>我是幽静的水流</p>
<p>上可以几千万里</p>
<p>成千军万马的降临</p>
<p>下可以成瀑布</p>
<p>把岩石冲激成冲激的岩石</p>
<p>那我就化身成人吧</p>
<p>杀身成仁，风涌云动</p>
<p>在断崖上，断日下</p>
<p>一件白衣荡荡而飘</p>
<p>轻愁是美好的</p>
<p>可是执着呢？……</p>
<p>在大梦中，我是那寻寻觅觅叩访惊喜的人。</p>
<p>究竟谁是侠骨的真？</p>
<p>今天我写诗</p>
<p>明天我的路更远</p>
<p>从等待惊喜到迷惘得在暮色里摘花</p>
<p>在苍茫中回首</p>
<p>看月窗前的自己和她不甚清楚</p>
<p>我今天要走</p>
<p>明天雪鱼冻林</p>
<p>在迟了千百年后的今宵</p>
<p>我们于风尘中相见</p>
<p>仅仅让君子知道</p>
<p>许多感动因年龄而不再</p>
<p>我难以再作悲伤的流露……</p>
<p>而今大江一重，搁在身前</p>
<p>兄弟，读你的诗才几行</p>
<p>大江已寒……</p>
<p>今天我送你</p>
<p>明天路可以远至逍遥千里</p>
<p>冷漠是可喜的</p>
<p>真挚的一惊呢？</p>
<p>在全然的黑暗中</p>
<p>风和风在呼啸叶子和叶子在回应</p>
<p>我感觉到你就是和我走哪不了解长路的人。</p>
<p>没有关怀，不说一句话</p>
<p>怕更受伤。怕没有风。</p>
<p>怕没有温暖的黑暗。</p>
<p>怕一朵花谢和她的开……</p>
<p>灯乍亮，</p>
<p>你还是端坐在千万人中</p>
<p>那么脆弱而易受伤</p>
<p>或作嗔喜，或作自卫而笑……</p>
<p>而千万人中，</p>
<p>我就渴望那么一眼</p>
<p>千万年中，我生来就为等着</p>
<p>千次万次中，就白衣那么一次</p>
<p>当杏花 烟雨 绿水江南岸。</p>
<p>当我诗篇背后</p>
<p>透出银色的字</p>
<p>你喜悦不喜悦？</p>
<p>感动是可忧的</p>
<p>而我年岁悠悠……</p>
<p>就化身为枯藤松柏吧</p>
<p>我有更长而倦的守望</p>
<p>在许多敬佩与不敬佩的目光中</p>
<p>你的了解更是抹不去的一笔。</p>
<p>容颜可以秀动峨眉</p>
<p>我是多么向往那绿水的情怀</p>
<p>你纵化为悄悄的女魂</p>
<p>小心我便是那珍藏古镜的书生</p>
<p>把你摄入镜中，是时候</p>
<p>便轻声一声二声三声呵暖你</p>
<p>要年出来伴我长夜枯灯</p>
<p>我一剑西来</p>
<p>你衣裙袅动</p>
<p>那么小小的可爱</p>
<p>流过庭院</p>
<p>我在寺中抄经</p>
<p>而明天要练拳易筋……</p>
<p>春山爱笑</p>
<p>明天我的路更远</p>
<p>马蹄成了蝴蝶</p>
<p>弯弓射箭，走过绿林</p>
<p>我是那上京应考而不读书的书生</p>
<p>来洛阳是为求看你的倒影</p>
<p>水里的绝笔，天光里的遗言</p>
<p>挽绝你小小的清瘦</p>
<p>一瓢饮你小小的丰满</p>
<p>就是爱情和失恋</p>
<p>是我一首诗又一首诗</p>
<p>活得像泰山刻石惊涛裂岸的第一笔……</p>
<p>我的笔又苦又尖</p>
<p>梦是可喜</p>
<p>爱是可忧</p>
<p>我还有静静的玄关要迎送</p>
<p>你听我步履远去</p>
<p>我送你迎风</p>
<p>浩浩荡荡，长洲巨滩</p>
<p>就洞庭，就太华</p>
<p>括苍到点苍，</p>
<p>我的金刚经</p>
<p>比出匣时更势若沧浪</p>
<p>我是那自出阳关的第一水</p>
<p>从柔情传达给我的激情</p>
<p>剪刀峰，大小龙秋飞瀑</p>
<p>一气呵成而泻千里</p>
<p>我的歌不尽</p>
<p>上可以九万里而不止</p>
<p>下可以……</p>
<p>我还是那不应考而为骑骏马上京的一介寒生</p>
<p>秋水成剑，生平最乐</p>
<p>无数知音可刎颈</p>
<p>红颜能为长剑而琴断，宝刀能为知己而轻用</p>
<p>有女拂袖。有女明灯。有女答客。</p>
<p>沏茶还是茗酒</p>
<p>为剑可以白衣</p>
<p>可以飘行千里</p>
<p>而我正有远远的路要走……</p>
<p>越来越接近那吼声了</p>
<p>那是没有终止的冲决</p>
<p>崩却原是苍茫滩上的</p>
<p>一夫当关，狠命一击</p>
<p>气势自出，岁月俞久</p>
<p>我的京试愈垂青史……</p>
<p>这首诗我不停而写</p>
<p>才气你究竟什么时候才断绝？</p>
<p>水声更近，天涯无尽</p>
<p>在此诀别，红颜知音</p>
<p>那在雁荡飞跃之君子</p>
<p>那烛光中仍独悒清芬之秀颜</p>
<p>几时才在明月天山间</p>
<p>我化成大海</p>
<p>你化成清风</p>
<p>我们再守一守</p>
<p>那锦绣的神州……</p>
]]></content>
  </entry>
  <entry>
    <title>《德米安》</title>
    <url>/2020/20201022/</url>
    <content><![CDATA[<p>我常幻想未来，想象着自己可能会成为一名诗人、先知或画家。</p>
<span id="more"></span>但这些都徒劳无益。我人生的意义并不是写诗、讲道或绘画，其他人的也不应如此，所有这些应仅被视为爱好。每个人都只有一个使命，那就是寻找自我，无论最终成为诗人还是疯子、先知还是罪犯，都不关紧要。一个人的主要任务就是找到属于自己的命运，并全心全意地沿着命运之路前行。其他一切都是逃避的借口，是泯然众人的退缩、随波逐流，也是内心的恐惧。我心中的不断出现新意象，它们可能曾在我面前出现过，但直到现在我才第一次经历。这是一次本性的实验，也是一场结果未知的博弈，它可能会找到新的出路，也可能什么也找不到。我唯一的使命就是任其自由发展，感受它的意志，并完全掌握住。除此以外，别无其他。我早已感受过孤独，现在我将要忍受更刻骨铭心的孤独，一切都无法避免。]]></content>
  </entry>
  <entry>
    <title>一个关于e的证明的一题多解</title>
    <url>/2020/20201009/</url>
    <content><![CDATA[<p>求证：$e&gt;1+1+\frac{1}{2!}+$···$+\frac{1}{k!}$</p>
<span id="more"></span>

<p><img src="https://s3.ax1x.com/2020/11/18/DnW1SJ.jpg" alt="1.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>《龙族》</title>
    <url>/2020/20200926/</url>
    <content><![CDATA[<p>路明非沉默了好一会儿:“师兄你看过《圣斗士星矢》没有?”</p>
<p>楚子航一愣:“听说过。”<br>“我看的时候超感动的,连台词都能背下来。”路明非嘟嘟囔囔,<span id="more"></span>“有一次星矢给人打倒了,爬都爬不动了,就跟雅典娜说,我一点力气都不剩了,我再也前进不了了。雅典娜说可是你还有希望啊。星矢想对啊,我还有希望啊,有希望我最大啊,就又站起来把敌人打倒了。”他呆呆地望着窗外,“那时候我心想,说得真好!我也有希望啊,有希望我总会牛逼的。”<br>“后来看到冥界篇,星矢又给打倒了,这次是给神打倒的,人是打不过神的,这次连希望都没有,”他又说,“星矢又跟雅典娜说,我把一切都用上了女神,我输了,雅典娜又说,可是你还有生命啊,你不是一无所有。星矢心想对啊,我还有生命啊!我燃烧生命我最大啊!于是又站起来把神也打倒了。我又很感动,心里暗暗地发狠,恨不得有件什么事让我也把命赌上去做。”<br>“可后来我想明白了。雅典娜是星矢的老板，还是个无良老板，老板跟苦逼员工说，要怀着美好的希望啊，要拿生命出来作战啊!希望啊生命啊，其实都是借口，哄小屁孩的，让你觉得将来有盼头。”路明非轻声说, “有些事你发狠你就能牛逼，大部分事你怀着希望赌上命都没用。”<br>两个人都沉默着,但空气中有股火药般的味道,楚子航的瞳孔中闪动着仿佛实质的怒火。<br>“我知道师兄你怎么想，我就是很懦弱啊。”路明非低下头去, “你又不是第一天认识我。”<br>楚子航深呼吸,强压下莫名的愤怒,对他而言这种愤怒实在是莫名其妙,按照他的性格不该对别人的事那么在意。<br>“我小学的时候在班里被人看不起，”他轻声说,“因为那时候我妈妈带着我改嫁了，班里的人都知道我爸爸不是亲爸爸。那时候我上的是一个国际小学，班里同学的家境都很好，好多人的父母跟我继父有来往。他们嘲笑我的一个理由是因为我妈妈长得漂亮，所以我才有机会上那个小学，我其实是个司机的儿子。” 他的声音有些颤抖:“他们说，楚子航的爸爸是为了睡他妈妈所以才对他好的!”<br>路明非愣住了,有些手足无措。该死,这些私密往事可不是他该知道的。他作为学生会主席恺撒·加图索旗下的小走狗,跟狮心会会长楚子航过从甚密,夜深人静交换心事,这要被狗仔队拍照留念简直是通敌大罪。<br>“那个带头这么说的家伙是个空手道黑带，中国最年轻的黑带。”楚子航说,“我的血统没有觉醒，我打不过他。”<br>“你后爹不是对你挺好的?跟你后爹说，让你后爹找他老爹，拼爹师兄你绝不输的，你两个爹，个个威武，人家就一个。”路明非忍不住嘴欠。<br>“不，这件事我没跟他说过，因为跟他没有关系，这是我的事。”楚子航低声说,“我只是要他送我去学剑道。我用了三年的时间，拿到了黑带，在那之前没人相信一个小学生能做到。但我必须在三年内拿到，因为如果超过了三年我就毕业了，我不知道会去哪个中学，我就不能揍他了。”<br>“喔!”路明非赞叹。<br>“我在毕业典礼之前约他打架，他每次冲我飞腿的时候我就用竹剑打在他膝盖上，三年里我每次练习都对着空气练习这种击打。我想他的腿怎么踢来，我怎么击打。他每次爬起来都不敢相信，说你怎么可能老打中?”楚子航的声音有些嘶哑,“我不回答，我当然可以每次打中!因为我练了一万次!”<br>他按在路明非的肩上:“每个人都可以把自己的命握在自己手里，只要你相信你能做到!”<br>路明非呆呆地看着他,楚子航的瞳孔中如打铁那样跳动着火星。<br>“师兄你真是励志帝。”过了好一会儿,路明非嘟囔。<br>“我希望你懂我的意思,诺诺的事你放不放弃,我不关心,”楚子航说,“但更多的事,希望你别放弃!”<br>“师兄，你把人家打那么惨，后来怎么跟家里交代的?”路明非忽然问。<br>“他妈妈找到学校，我只能回去找家长，我找了我妈妈，”楚子航挠了挠额角,“你知道我妈妈那个人…其实跟靠不住的…听我说了打人的原因之后，她笑得前仰后合。”<br>“前仰后合?”<br>“反正是…很欢乐的样子。然后她就穿上最好的衣服，戴上她百达翡丽的手表和卡地亚的钻戒，带着司机和我家保安，开着我爹最贵那辆奔驰去学校跟他妈妈见面，有钱的女人总会在这种时候炫耀，我看他妈妈来的时候也是一身金闪闪的。”<br>“拼爹又拼妈。”路明非说。<br>“我忽然明白了妈妈的用意。他妈看我妈一身打扮，心理上先输了，气势就低落了。”楚子航摇摇头,“但毕竟是我打人的，他妈妈还是嚷嚷，话里还是讽刺我妈妈带着我改嫁。我想其实那些话都是那个男生在家里听自己爸妈说的，他不过来学校里鹦鹉学舌。”<br>“你妈怒了?”<br>“没有，我妈妈很镇静。我妈妈说这件事呢，是你家儿子说我家儿子不是他爸爸亲生开始的，这是事实。但是呢，要是我家儿子跟你家儿子比花钱，那就是拿你老公跟我儿子的继父比，谁输谁赢，各安天命。但我家儿子是打架赢的你家儿子，这就说明我儿子基因好，身体好，基因身体可都是他亲爸爸给他的哟!你儿子那么弱，凭什么嘲笑我儿子?哦对了，你老公是不是身体不好?要么怎么生出的儿子那么弱?不是空手道黑带么?我儿子练了三年就打赢他了，这不可能吧?你不带你家儿子去医院查查?”楚子航苦笑,“她就扔下医药费带我回家了，我妈妈那个人，说刻薄话也很厉害的。”<br>“你娘好上等!”路明非竖起大拇指。<br>可他忽然又不笑了:“师兄你知道么?我也跟人打过架，原因跟你差不多。我初中同学说我爸爸妈妈应该是在国外离婚了，谁都不要我，就把我仍在叔叔婶婶家。后来学校让我找家长，我就跟婶婶说了…”他舔了舔嘴唇,“婶婶把我劈头盖脸地骂了一顿，拉着我去跟人家道歉，让我帮人家做值日，这样可以少给点医药费…回到家之后，我听见夜里她和叔叔商量，说是不是我爹娘真的在国外离婚了没告诉他们，以后还有没有人给我付生活费…”<br>楚子航愣住了。<br>“后来整个星期我都在帮那个家伙做值日，晚上回到叔叔家要给家里每个人盛好饭再吃饭，要洗碗，听婶婶说‘这个月你的生活费可要用完啦，我把你的生活费单存了一个折子可没有乱用’的话，我表弟跟我说要是我的生活费下个月不寄来我可能就得搬出去了，这样他就能自己一个人一间屋了…”路明非又笑了,笑得很难过,“所以师兄，你牛逼是因为有人给你兜着啊，你有靠谱后爹，还有漂亮老娘，他们其实都是…爱你的啊，你不管做了什么坏事都有地方去的…可我没有，你要我怎么勇敢呢?”</p>
]]></content>
  </entry>
  <entry>
    <title>一个关于不定方程的猜想</title>
    <url>/2020/20200905/</url>
    <content><![CDATA[<p>不定方程：$a^x+(a+1)^y=(2a+1)^z$.<br>$a,x,y,z$均为自然数，且$a&gt;1$<br>证明：若$x,y,z$不全为偶数，则$x=y=z=1$</p>
<p>该猜想由本人约一年半前提出，曾发在StackExchange、数学研发论坛、AOPS论坛，可惜未有人给出完整证明。</p>
]]></content>
  </entry>
  <entry>
    <title>不切实际的梦</title>
    <url>/2020/20200426/</url>
    <content><![CDATA[<p>回想起来不过是看了几篇鸡汤的无谓的冲动 -2021/7月</p>
<hr>
<p>终究是一场梦  -2020/8月</p>
<hr>
<p>记于庚子年庚辰月己亥日。</p>
<span id="more"></span>

<p><img src="https://pic2.zhimg.com/80/v2-c043f649b079cead85292a4c9ad7eefd_720w.jpg" alt="1"></p>
<hr>
]]></content>
  </entry>
  <entry>
    <title>《相忘于江湖》-简祯</title>
    <url>/2020/20200111/</url>
    <content><![CDATA[<p>​        回忆若能下酒，往事便可来一场宿醉，醒来时天依旧分明，风依旧清亮，敛裳立于光阴的两岸目送漂泊者的远去，便可知过去、现在、将来都是匆匆。<span id="more"></span>我们必定是在被选择的命运中挣扎，因此那壶酒并不总是香醇的，还有数不尽的苦涩。生活中穿插着大大小小的不情不愿，有时悲从中来，也会鞠一把泪，更多的时候是迷茫，是惘然，是不知所措。这是必然明白生命本身的残忍，正如果实的故事——它不容我们不献出积累的馨芳，交出受过光热的每一层颜色，点点沥尽最难看的酸怆。不过也是因为这样，活着才有更多的乐趣，终究我们是可以不断抗争的八九点种的太阳，可以从超越了晨昏的日界线后重新出发，让所有流动的血和热情来坚守灵魂的高贵，甚至去扼住命运的咽喉。</p>
]]></content>
  </entry>
  <entry>
    <title>对数平均的一个上界</title>
    <url>/2019/20190831/</url>
    <content><![CDATA[<p>$\frac{x+y}{2}+\sqrt{xy}-\frac{x+\sqrt{xy}+y}{3}&gt;\frac{x-y}{lnx-lny}$</p>
<p>巧妙的将算术平均、几何平均、海伦平均、对数平均结合在了一起。</p>
]]></content>
  </entry>
  <entry>
    <title>条条大路通罗马，巧比大小多方法</title>
    <url>/2019/20190815/</url>
    <content><![CDATA[<p>（2014·湖北·22）<br>$\pi$为圆周率，e=2.71828…为自然对数的底数。<br>（1）求函数$f(x)=\frac{lnx}{x}$的单调区间<br>（2）将$e^3,3^e,3^{\pi},{\pi}^3,e^{\pi},{\pi}^e$这六个数按从小到大的顺序排列，并证明你的结论。</p>
<span id="more"></span>

<p>解：（3）本题的关键是比较$e^3$与${\pi}^e$、$e^{\pi}$与${\pi}^3$</p>
<p>$\because  $9&gt;${\pi}e$</p>
<p>$\therefore$ 只需证 ${\pi}^e$&gt;$e^3$，剩下的不言自明。</p>
<p>这道题看似是“空中楼阁”，但好心的命题者设计了梯子（第一问） 。</p>
<p>解法一：由（1）知，</p>
<p>$f(x)&lt;f(e)=\frac{1}{e}$</p>
<p>$f(\frac{e^2}{\pi})=\frac{ln{\frac{e^2}{\pi}}}{\frac{e^2}{\pi}}&lt;\frac{1}{e}$</p>
<p>即$ln{\pi}&gt;2-\frac{e}{\pi}$</p>
<p>$\therefore eln\pi&gt;e(2-\frac{e}{\pi})&gt;2.7(2-\frac{2.7}{3.14}) &gt;3$</p>
<p>$ln{\pi^e}&gt;3=ln{e^3} $</p>
<p>$\Rightarrow e^3 &lt;\pi ^e$</p>
<p>事实上，这梯子不太容易攀爬，我们不妨舍弃该梯子，换一条道路直捣黄龙。<br>解法二：$e^3$&lt;$\pi^e$<br>$\Leftrightarrow {\pi}^{\frac{e}{3}}&gt;e$<br>而$\frac{e}{3}&gt;\frac{9}{10}$<br>$\therefore $只需证$ \pi^{\frac{9}{10}}&gt;e$<br>而 ${1+\frac{1}{x}}^{x+1}\geq e$<br>$\therefore $只需证 $\frac{\pi}{e}&gt;1+\frac{1}{8}$<br>即证$8\pi&gt;9e$</p>
<p>而8x3.14&gt;9x2.72</p>
<p>证毕。</p>
]]></content>
  </entry>
  <entry>
    <title>巧妙曲线系，减少计算量</title>
    <url>/2019/20190810/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2019/12/21/U5cwaqiAjxlTbYm.jpg" alt="360截图20191221184837061.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>洞察本质，避免分类</title>
    <url>/2019/20190809/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2019/12/21/6UwHYWbOETFJyVB.jpg" alt="360截图20191221184239890.jpg"></p>
]]></content>
  </entry>
  <entry>
    <title>一个几何不等式</title>
    <url>/2019/20190729/</url>
    <content><![CDATA[<p>$R-2r{\ge}\frac{1}{8R}\sum(a-b)^2$</p>
<span id="more"></span>证明：
<p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}\frac{1}{8R^2}\sum(a-b)^2$</p>
<p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}\sum sin^2{A} -\sum sin{A}sin{B}$</p>
<p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}2+2cos{A}cos{B}cos{C} -\sum sin{A}sin{B}$</p>
<p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}2+2*\frac{s^2-(2R+r)^2}{4R^2} -\frac{s^2+4Rr+r^2}{4R^2}$</p>
<p>$\Leftrightarrow 4R^2+4Rr+3r^2{\ge}s^2 $  (此即Gerretsen’s Inequality)</p>
<p>这条不等式强化了欧拉不等式$(R\geq 2r)$，还指明了等号成立条件。唯一不足是不太具有对称性，还可以变形为:$\frac{R}{2r}\geq \frac{abc+a^3+b^3+c^3}{4abc}$</p>
]]></content>
  </entry>
  <entry>
    <title>LTE引理[中文版]</title>
    <url>/2019/20190727/</url>
    <content><![CDATA[<p>Lifting The Exponent Lemma是求解指数丢番图方程(不定方程)的有效<br>方法。它在奥林匹克民间传说中非常有名（例如，参见[3]），尽管其起源<br>很难追溯。在数学上，它是数论中经典Hensel引理（见[2]）的近亲（在证<br>明的陈述和观点中）。在本文中，我们分析了这种方法并介绍了它的一些<br>应用。<span id="more"></span><br>在涉及指数方程的许多问题中，我们可以使用Lifting The Exponent<br>Lemma（这是一个长名称，我们称之为LTE！）,特别是我们可以找到某些<br>质因子的时候。有时LTE引理甚至能秒杀一道题。这个引理显示了如何找<br>到素数p的最大幂 —— 通常≥3 —— a^n ±b^n型——本文中定理和引理<br>的证明没有任何复杂难理解之处，所有这些都使用了初等数学。理解定理<br>的用法及其含义对于你来说比记住它的详细证明更重要。 </p>
<p>….</p>
<p>本文来自AOPS社区  <a href="https://artofproblemsolving.com/community/c6h393335p2186092">https://artofproblemsolving.com/community/c6h393335p2186092</a> </p>
<p>本人精心翻译</p>
<p>鉴于篇幅过长，剩余部分放在<a href="https://www.bilibili.com/read/cv6779585">https://www.bilibili.com/read/cv6779585</a></p>
]]></content>
  </entry>
  <entry>
    <title>2019IMO P4</title>
    <url>/2019/20190718/</url>
    <content><![CDATA[<p><img src="https://s2.ax1x.com/2019/07/19/Zj2TW4.jpg" alt="1"><span id="more"></span></p>
<p><img src="https://s2.ax1x.com/2019/07/19/Zj2LO1.jpg" alt="2"></p>
<p><img src="https://s2.ax1x.com/2019/07/19/Zj2bl9.jpg" alt="3"></p>
<p><img src="https://s2.ax1x.com/2019/07/19/Zj2jw6.jpg" alt="4"></p>
<p><img src="https://s2.ax1x.com/2019/07/19/Zj2Xex.jpg" alt="5"></p>
<p><img src="https://s2.ax1x.com/2019/07/19/Zj2vTK.jpg" alt="6"></p>
]]></content>
  </entry>
  <entry>
    <title>高中三年校内图书馆借阅记录</title>
    <url>/2019/20190624/</url>
    <content><![CDATA[<p>共借阅121本，图书馆记录了87本（图书馆因饭卡原因部分借阅记录缺失），含重复借阅书籍。</p>
<span id="more"></span>

<p><img src="https://s3.ax1x.com/2021/01/28/ySqi1f.md.png" alt="ySqi1f.md.png"></p>
<p>具体书目：</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>索书号</th>
<th>题名</th>
<th>借出日期</th>
<th>还书日期</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>C53/40</td>
<td>叩响命运的门：人生必读的一百零二篇人文素养经典</td>
<td>2020-1-16</td>
<td>2020-5-14</td>
</tr>
<tr>
<td>2</td>
<td>F552/2</td>
<td>明代的漕运</td>
<td>2020-1-15</td>
<td>2020-5-14</td>
</tr>
<tr>
<td>3</td>
<td>F06/13</td>
<td>地球人不靠谱</td>
<td>2020-1-15</td>
<td>2020-5-14</td>
</tr>
<tr>
<td>4</td>
<td>I267/1649</td>
<td>下午茶</td>
<td>2020-1-14</td>
<td>2020-5-29</td>
</tr>
<tr>
<td>5</td>
<td>I267/1642</td>
<td>好一座浮岛</td>
<td>2020-1-9</td>
<td>2020-1-13</td>
</tr>
<tr>
<td>6</td>
<td>I267/1924</td>
<td>水问</td>
<td>2020-1-5</td>
<td>2020-1-14</td>
</tr>
<tr>
<td>7</td>
<td>01/82</td>
<td>怎样解题：数学思维的新方法</td>
<td>2019-12-30</td>
<td>2020-1-5</td>
</tr>
<tr>
<td>8</td>
<td>I247.5/1989</td>
<td>丑行或浪漫</td>
<td>2019-12-24</td>
<td>2019-12-30</td>
</tr>
<tr>
<td>9</td>
<td>K207/32</td>
<td>中国大历史</td>
<td>2019-12-24</td>
<td>2020-1-14</td>
</tr>
<tr>
<td>10</td>
<td>K825.6/219</td>
<td>三毛：我需要最狂的风，和最静的海</td>
<td>2019-12-16</td>
<td>2019-12-24</td>
</tr>
<tr>
<td>11</td>
<td>B221/44</td>
<td>卜辞看人生：易经</td>
<td>2019-12-10</td>
<td>2019-12-24</td>
</tr>
<tr>
<td>12</td>
<td>I313.4/227</td>
<td>黎明之街</td>
<td>2019-12-3</td>
<td>2019-12-4</td>
</tr>
<tr>
<td>13</td>
<td>K826.1/43</td>
<td>吴文俊与中国数学</td>
<td>2019-12-3</td>
<td>2019-12-10</td>
</tr>
<tr>
<td>14</td>
<td>F815/1</td>
<td>另一片海：阿拉伯之春、欧债风暴与新自由主义之殇</td>
<td>2019-11-25</td>
<td>2019-12-16</td>
</tr>
<tr>
<td>15</td>
<td>F815/1</td>
<td>另一片海：阿拉伯之春、欧债风暴与新自由主义之殇</td>
<td>2019-11-25</td>
<td>2019-11-25</td>
</tr>
<tr>
<td>16</td>
<td>C912/88</td>
<td>明亮的对话：公共说理十八讲</td>
<td>2019-11-21</td>
<td>2019-12-16</td>
</tr>
<tr>
<td>17</td>
<td>0178/2</td>
<td>初等不等式的证明方法</td>
<td>2019-11-18</td>
<td>2019-11-25</td>
</tr>
<tr>
<td>18</td>
<td>B821/193</td>
<td>段子：人生何求：一个江湖老总的人生私藏</td>
<td>2019-11-18</td>
<td>2019-11-20</td>
</tr>
<tr>
<td>19</td>
<td>B97/19</td>
<td>眼泪与圣徒</td>
<td>2019-11-5</td>
<td>2019-11-11</td>
</tr>
<tr>
<td>20</td>
<td>D931.3/1</td>
<td>与手枪的不幸相遇：日本司法物语</td>
<td>2019-11-4</td>
<td>2019-11-11</td>
</tr>
<tr>
<td>21</td>
<td>D669/55</td>
<td>从有意义到有意思：《新周刊》生活观</td>
<td>2019-11-4</td>
<td>2019-11-20</td>
</tr>
<tr>
<td>22</td>
<td>F014.3/5</td>
<td>合适：从升学择校、相亲配对、牌照拍卖了解新兴实用经济学</td>
<td>2019-10-23</td>
<td>2019-11-4</td>
</tr>
<tr>
<td>23</td>
<td>I313.45/73</td>
<td>一个人的好天气</td>
<td>2019-10-23</td>
<td>2019-11-4</td>
</tr>
<tr>
<td>24</td>
<td>B97/15</td>
<td>在世界的爱心之中：德兰修女的感想、故事与祷辞</td>
<td>2019-10-10</td>
<td>2019-10-15</td>
</tr>
<tr>
<td>25</td>
<td>B94/11</td>
<td>佛教三经·圆觉经：最新图文版</td>
<td>2019-10-9</td>
<td>2019-10-21</td>
</tr>
<tr>
<td>26</td>
<td>J228/55</td>
<td>有兽焉</td>
<td>2019-9-28</td>
<td>2019-10-8</td>
</tr>
<tr>
<td>27</td>
<td>I267/1682</td>
<td>历史，从未这样</td>
<td>2019-9-28</td>
<td>2019-10-7</td>
</tr>
<tr>
<td>28</td>
<td>I253/98/3</td>
<td>抗日战争·第三卷：1942年6月-1945年9月</td>
<td>2019-9-26</td>
<td>2019-9-28</td>
</tr>
<tr>
<td>29</td>
<td>I267/1283</td>
<td>在自己心中盖一座花园</td>
<td>2019-9-25</td>
<td>2019-10-7</td>
</tr>
<tr>
<td>30</td>
<td>B21/3</td>
<td>风流去</td>
<td>2019-9-25</td>
<td>2019-10-7</td>
</tr>
<tr>
<td>31</td>
<td>I313.4/71/[2]</td>
<td>嫌疑人X的献身</td>
<td>2019-9-24</td>
<td>2019-9-25</td>
</tr>
<tr>
<td>32</td>
<td>J228.2/139</td>
<td>流学的一年</td>
<td>2019-9-24</td>
<td>2019-9-25</td>
</tr>
<tr>
<td>33</td>
<td>I253/65</td>
<td>朝鲜战争</td>
<td>2019-9-19</td>
<td>2019-9-24</td>
</tr>
<tr>
<td>34</td>
<td>D50/3</td>
<td>世界趋势2050</td>
<td>2019-9-19</td>
<td>2019-9-28</td>
</tr>
<tr>
<td>35</td>
<td>D097.12/2</td>
<td>自由的基因：美国自由主义的历史变迁</td>
<td>2019-9-19</td>
<td>2019-9-24</td>
</tr>
<tr>
<td>36</td>
<td>01/41</td>
<td>500个世界著名数学征解问题</td>
<td>2019-9-19</td>
<td>2019-10-7</td>
</tr>
<tr>
<td>37</td>
<td>I253/65</td>
<td>朝鲜战争</td>
<td>2019-9-19</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>38</td>
<td>01/41</td>
<td>500个世界著名数学征解问题</td>
<td>2019-9-17</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>39</td>
<td>K825.2/39</td>
<td>历史可以很精彩之战将传</td>
<td>2019-9-17</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>40</td>
<td>TP18/9</td>
<td>AI：人工智能的本质与未来</td>
<td>2019-9-16</td>
<td>2019-9-17</td>
</tr>
<tr>
<td>41</td>
<td>I565.4/60</td>
<td>卡门</td>
<td>2019-9-16</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>42</td>
<td>D60/9</td>
<td>中国人，你要自信</td>
<td>2019-9-11</td>
<td>2019-9-16</td>
</tr>
<tr>
<td>43</td>
<td>D50/3</td>
<td>世界趋势2050</td>
<td>2019-9-11</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>44</td>
<td>D097.12/2</td>
<td>自由的基因：美国自由主义的历史变迁</td>
<td>2019-9-11</td>
<td>2019-9-19</td>
</tr>
<tr>
<td>45</td>
<td>TS971/70</td>
<td>茶与美</td>
<td>2019-9-11</td>
<td>2019-9-11</td>
</tr>
<tr>
<td>46</td>
<td>TS976/36</td>
<td>万物皆有理：你很熟悉但未必明白的那些事儿</td>
<td>2019-9-11</td>
<td>2019-9-16</td>
</tr>
<tr>
<td>47</td>
<td>I313.4/160</td>
<td>大雪中的山庄</td>
<td>2019-9-10</td>
<td>2019-9-11</td>
</tr>
<tr>
<td>48</td>
<td>0189/2</td>
<td>庞加莱猜想：追寻宇宙的形状</td>
<td>2019-9-10</td>
<td>2019-9-11</td>
</tr>
<tr>
<td>49</td>
<td>G236/23</td>
<td>国家与市场</td>
<td>2019-9-9</td>
<td>2019-9-11</td>
</tr>
<tr>
<td>50</td>
<td>B97/19</td>
<td>眼泪与圣徒</td>
<td>2019-9-9</td>
<td>2019-9-10</td>
</tr>
<tr>
<td>51</td>
<td>J228/56</td>
<td>破耳兔：你不完美的样子也很美</td>
<td>2019-9-5</td>
<td>2019-9-9</td>
</tr>
<tr>
<td>52</td>
<td>C913/116</td>
<td>爱情数学：如何用数学找到真爱?</td>
<td>2019-9-5</td>
<td>2019-9-9</td>
</tr>
<tr>
<td>53</td>
<td>B0/6</td>
<td>大问题：简明哲学导论</td>
<td>2019-9-5</td>
<td>2019-9-10</td>
</tr>
<tr>
<td>54</td>
<td>TP311.5/2</td>
<td>Python密码学编程：Hacking Secret Ciphers with Python</td>
<td>2018-11-28</td>
<td>2018-12-26</td>
</tr>
<tr>
<td>55</td>
<td>I561.4/122</td>
<td>脉搏</td>
<td>2018-11-28</td>
<td>2018-11-29</td>
</tr>
<tr>
<td>56</td>
<td>TP311.5/2</td>
<td>Python密码学编程：Hacking Secret Ciphers with Python</td>
<td>2018-11-28</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>57</td>
<td>I561.4/122</td>
<td>脉搏</td>
<td>2018-11-28</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>58</td>
<td>TP311.5/2</td>
<td>Python密码学编程：Hacking Secret Ciphers with Python</td>
<td>2018-11-28</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>59</td>
<td>I561.4/119</td>
<td>蝴蝶梦</td>
<td>2018-11-22</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>60</td>
<td>I242.43/5</td>
<td>杨家将演义</td>
<td>2018-11-22</td>
<td>2018-11-22</td>
</tr>
<tr>
<td>61</td>
<td>01/28</td>
<td>二战时期密码决战中的数学故事</td>
<td>2018-11-22</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>62</td>
<td>I313.4/30</td>
<td>脑人</td>
<td>2018-11-15</td>
<td>2018-11-22</td>
</tr>
<tr>
<td>63</td>
<td>TP311.5/2</td>
<td>Python密码学编程：Hacking Secret Ciphers with Python</td>
<td>2018-11-15</td>
<td>2018-11-28</td>
</tr>
<tr>
<td>64</td>
<td>I313.4/30</td>
<td>脑人</td>
<td>2018-11-15</td>
<td>2018-11-15</td>
</tr>
<tr>
<td>65</td>
<td>TP311.5/2</td>
<td>Python密码学编程：Hacking Secret Ciphers with Python</td>
<td>2018-11-15</td>
<td>2018-11-15</td>
</tr>
<tr>
<td>66</td>
<td>I247.5/1894</td>
<td>悬案组</td>
<td>2018-11-1</td>
<td>2018-11-1</td>
</tr>
<tr>
<td>67</td>
<td>01/2</td>
<td>新编中学数学手册</td>
<td>2018-11-1</td>
<td>2018-11-1</td>
</tr>
<tr>
<td>68</td>
<td>I247.5/1894</td>
<td>悬案组</td>
<td>2018-10-31</td>
<td>2018-11-1</td>
</tr>
<tr>
<td>69</td>
<td>01/2</td>
<td>新编中学数学手册</td>
<td>2018-10-31</td>
<td>2018-11-1</td>
</tr>
<tr>
<td>70</td>
<td>01/2</td>
<td>新编中学数学手册</td>
<td>2018-10-25</td>
<td>2018-10-31</td>
</tr>
<tr>
<td>71</td>
<td>0121/1</td>
<td>速算速成</td>
<td>2018-10-25</td>
<td>2018-10-31</td>
</tr>
<tr>
<td>72</td>
<td>I247.7/357</td>
<td>失重</td>
<td>2018-10-24</td>
<td>2018-10-25</td>
</tr>
<tr>
<td>73</td>
<td>I247.5/1846</td>
<td>孽子</td>
<td>2018-10-18</td>
<td>2018-10-24</td>
</tr>
<tr>
<td>74</td>
<td>I534.4/1</td>
<td>战友同志</td>
<td>2018-10-11</td>
<td>2018-10-18</td>
</tr>
<tr>
<td>75</td>
<td>I562.4/7</td>
<td>尤利西斯</td>
<td>2018-10-11</td>
<td>2018-10-18</td>
</tr>
<tr>
<td>76</td>
<td>I516.4/14</td>
<td>第十三天</td>
<td>2018-10-8</td>
<td>2018-10-11</td>
</tr>
<tr>
<td>77</td>
<td>I247.5/1788</td>
<td>曲终人在</td>
<td>2018-9-27</td>
<td>2018-10-8</td>
</tr>
<tr>
<td>78</td>
<td>I247.5/1694</td>
<td>英雄时代</td>
<td>2018-9-27</td>
<td>2018-10-8</td>
</tr>
<tr>
<td>79</td>
<td>I247.5/1788</td>
<td>曲终人在</td>
<td>2018-9-27</td>
<td>2018-9-27</td>
</tr>
<tr>
<td>80</td>
<td>I247.5/1694</td>
<td>英雄时代</td>
<td>2018-9-27</td>
<td>2018-9-27</td>
</tr>
<tr>
<td>81</td>
<td>011-53/1</td>
<td>数学珍宝：历史文献精选</td>
<td>2018-9-26</td>
<td>2018-9-27</td>
</tr>
<tr>
<td>82</td>
<td>I247.4/16</td>
<td>金瓶梅·西门公子：故事</td>
<td>2018-9-26</td>
<td>2018-9-27</td>
</tr>
<tr>
<td>83</td>
<td>I712.4/243</td>
<td>沙丘</td>
<td>2018-9-20</td>
<td>2018-9-26</td>
</tr>
<tr>
<td>84</td>
<td>01/2</td>
<td>新编中学数学手册</td>
<td>2018-6-21</td>
<td>2018-9-3</td>
</tr>
<tr>
<td>85</td>
<td>015/5</td>
<td>高等代数</td>
<td>2018-4-25</td>
<td>2018-5-10</td>
</tr>
<tr>
<td>86</td>
<td>I247.5/1850</td>
<td>出家</td>
<td>2018-4-24</td>
<td>2018-4-25</td>
</tr>
<tr>
<td>87</td>
<td>I247.5/1442</td>
<td>纪委书记</td>
<td>2018-4-19</td>
<td>2018-4-25</td>
</tr>
</tbody></table>
]]></content>
  </entry>
  <entry>
    <title>丑奴儿·书博山道中壁</title>
    <url>/2019/20190430/</url>
    <content><![CDATA[<p>In my younger days, I had tasted only  gladness,</p>
<p>But loved to mount the top floor,</p>
<p>But loved to mount the top floor,</p>
<p>To write a song pretending sadness,</p>
<span id="more"></span>

<p>And now I’ve tasted Sorrow’s flavors, bitter  and sour,</p>
<p>And can’t find a word,</p>
<p>And can’t find a word,</p>
<p>But merely say，“what a golden autumn hour！”</p>
<p>——翻译来自林语堂</p>
<p>少年不识愁滋味，爱上层楼。爱上层楼。为赋新词强说愁。<br>而今识尽愁滋味，欲说还休。欲说还休。却道天凉好个秋。</p>
<p><img src="https://images.unsplash.com/photo-1478993161925-a849b2c448d1?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=750&q=80" alt="prove1"></p>
]]></content>
  </entry>
  <entry>
    <title>一个美妙的不等式</title>
    <url>/2019/20190323/</url>
    <content><![CDATA[<p>最近在Aops论坛上发现了这个美妙的不等式，由Pirkuliyev Rovsen提出并被我加强。<br><img src="https://s2.ax1x.com/2019/03/23/AJuZo6.jpg" alt="1"></p>
<p>证明：</p>
<span id="more"></span>

<p>不等式右边：</p>
<p><img src="https://s2.ax1x.com/2019/03/23/AGcYZR.jpg" alt="prove1"></p>
<p>这个等价于对数平均不等式。</p>
<p>不等式左边(来自Aniv的证明)：</p>
<p><img src="https://s2.ax1x.com/2019/03/23/AGc4SS.jpg" alt="prove2"></p>
<p>这个不等式精度很高，ln2的值只相差了大约千分之一。</p>
]]></content>
  </entry>
  <entry>
    <title>About Me</title>
    <url>/2019/about/</url>
    <content><![CDATA[<p>欢迎来到我的个人博客!<br>（纯粹是自娱自乐）</p>
<span id="more"></span>

<p>更新日志：</p>
<p>2021/07/11 增加网站运行时间显示</p>
<p>2021/07/10 恢复本地博客环境</p>
<p>2021/07/06 电脑系统损坏，重装系统，本地博客环境丢失，博客暂停更新</p>
<p>2021/05/05 增加置顶功能，博客时间增加更新时间（hexo-index-generator-plus）</p>
<p>2021/04/15 写下第四十篇博客</p>
<p>2021/04/12 增加说说页面（使用<a href="https://github.com/ArtitalkJS/Artitalk">Artitalk</a>）</p>
<p>2021/02/17 增加“欧拉计划（中文）”栏目（来自PE-CN.github.io）</p>
<p>2021/01/22 写下第三十篇博客</p>
<p>2021/01/16 引入相册功能，增加“人间烟火”栏目</p>
<p>2020/12/28 启用文章加密，申请SSL证书</p>
<p>2020/12/22 更新ayer，增加夜间模式，字数统计，百度访客统计等功能</p>
<p>2020/11/18 回归github pages</p>
<p>2020/11/15 写下第二十篇博客</p>
<p>2020/09/27 github pages DNS解析出错，转用gitee pages</p>
<p>2020/01/11 写下第十篇博客</p>
<p>2020/01/04 切换博客主题为ayer，引入mathjax，引入访问统计</p>
<p>2019/03/23 切换博客主题为diaspora，写下第一篇博客</p>
<p>2019/02/06 建立博客</p>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/hello-world/</url>
    <content><![CDATA[<p>测试mathjax:$x^2$ </p>
<p>$\frac{sinx}{x}$</p>
]]></content>
  </entry>
</search>
